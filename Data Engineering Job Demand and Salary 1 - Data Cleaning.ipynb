{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0113d319",
   "metadata": {},
   "source": [
    "# Data Background\n",
    "\n",
    "Dataset is data engineering job postings from Glassdoor in the USA in March 2023. It includes company and job details such as company size, industry, location, title, salary, etc. \n",
    "\n",
    "Data source: https://github.com/Hamagistral/DataEngineers-Glassdoor/tree/master/data/kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc84170a",
   "metadata": {},
   "source": [
    "# Load and Examine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daa9f819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "189173cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of rows =  900\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>location</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_description</th>\n",
       "      <th>salary_estimate</th>\n",
       "      <th>company_size</th>\n",
       "      <th>company_type</th>\n",
       "      <th>company_sector</th>\n",
       "      <th>company_industry</th>\n",
       "      <th>company_founded</th>\n",
       "      <th>company_revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PCS Global Tech\\n4.7</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Riverside, CA</td>\n",
       "      <td>Data Engineer | PAID BOOTCAMP</td>\n",
       "      <td>Responsibilities\\n· Analyze and organize raw d...</td>\n",
       "      <td>$70,000 /yr (est.)</td>\n",
       "      <td>501 to 1000 Employees</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Information Technology Support Services</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Futuretech Consultants LLC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Newton, MS</td>\n",
       "      <td>Snowflake Data Engineer</td>\n",
       "      <td>My name is Dileep and I am a recruiter at Futu...</td>\n",
       "      <td>$42.50 /hr (est.)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clairvoyant\\n4.4</td>\n",
       "      <td>4.4</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Data Engineer (MDM)</td>\n",
       "      <td>Required Skills:\\nMust have 5-8+ Years of expe...</td>\n",
       "      <td>$67.50 /hr (est.)</td>\n",
       "      <td>51 to 200 Employees</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Pharmaceutical &amp; Biotechnology</td>\n",
       "      <td>Biotech &amp; Pharmaceuticals</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple\\n4.2</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Cupertino, CA</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Summary\\nPosted: Dec 22, 2021\\nWeekly Hours: 4...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000+ Employees</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Computer Hardware Development</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>$10+ billion (USD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skytech Consultancy Services\\n5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Baltimore, MD</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Description of Work:\\nTechnical experience in ...</td>\n",
       "      <td>$65.00 /hr (est.)</td>\n",
       "      <td>1 to 50 Employees</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             company  company_rating       location  \\\n",
       "0               PCS Global Tech\\n4.7             4.7  Riverside, CA   \n",
       "1         Futuretech Consultants LLC             NaN     Newton, MS   \n",
       "2                   Clairvoyant\\n4.4             4.4         Remote   \n",
       "3                         Apple\\n4.2             4.2  Cupertino, CA   \n",
       "4  Skytech Consultancy Services\\n5.0             5.0  Baltimore, MD   \n",
       "\n",
       "                       job_title  \\\n",
       "0  Data Engineer | PAID BOOTCAMP   \n",
       "1        Snowflake Data Engineer   \n",
       "2            Data Engineer (MDM)   \n",
       "3                  Data Engineer   \n",
       "4                  Data Engineer   \n",
       "\n",
       "                                     job_description     salary_estimate  \\\n",
       "0  Responsibilities\\n· Analyze and organize raw d...  $70,000 /yr (est.)   \n",
       "1  My name is Dileep and I am a recruiter at Futu...   $42.50 /hr (est.)   \n",
       "2  Required Skills:\\nMust have 5-8+ Years of expe...   $67.50 /hr (est.)   \n",
       "3  Summary\\nPosted: Dec 22, 2021\\nWeekly Hours: 4...                 NaN   \n",
       "4  Description of Work:\\nTechnical experience in ...   $65.00 /hr (est.)   \n",
       "\n",
       "            company_size       company_type                  company_sector  \\\n",
       "0  501 to 1000 Employees  Company - Private          Information Technology   \n",
       "1                    NaN                NaN                             NaN   \n",
       "2    51 to 200 Employees  Company - Private  Pharmaceutical & Biotechnology   \n",
       "3       10000+ Employees   Company - Public          Information Technology   \n",
       "4      1 to 50 Employees   Company - Public                             NaN   \n",
       "\n",
       "                          company_industry  company_founded  \\\n",
       "0  Information Technology Support Services              NaN   \n",
       "1                                      NaN              NaN   \n",
       "2                Biotech & Pharmaceuticals              NaN   \n",
       "3            Computer Hardware Development           1976.0   \n",
       "4                                      NaN              NaN   \n",
       "\n",
       "            company_revenue  \n",
       "0  Unknown / Non-Applicable  \n",
       "1                       NaN  \n",
       "2  Unknown / Non-Applicable  \n",
       "3        $10+ billion (USD)  \n",
       "4  Unknown / Non-Applicable  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SalaryData = pd.read_csv(\"glassdoor-data-engineer-kaggle.csv\")\n",
    "print('# of rows = ',SalaryData.shape[0])\n",
    "SalaryData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d55783c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company              object\n",
      "company_rating      float64\n",
      "location             object\n",
      "job_title            object\n",
      "job_description      object\n",
      "salary_estimate      object\n",
      "company_size         object\n",
      "company_type         object\n",
      "company_sector       object\n",
      "company_industry     object\n",
      "company_founded     float64\n",
      "company_revenue      object\n",
      "dtype: object\n",
      "Keys of SalaryData dataset:\n",
      " Index(['company', 'company_rating', 'location', 'job_title', 'job_description',\n",
      "       'salary_estimate', 'company_size', 'company_type', 'company_sector',\n",
      "       'company_industry', 'company_founded', 'company_revenue'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "types = SalaryData.dtypes\n",
    "print(types)\n",
    "print(\"Keys of SalaryData dataset:\\n\", SalaryData.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80bc5ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company               0\n",
       "company_rating      185\n",
       "location              0\n",
       "job_title             0\n",
       "job_description       0\n",
       "salary_estimate      64\n",
       "company_size        135\n",
       "company_type        135\n",
       "company_sector      391\n",
       "company_industry    391\n",
       "company_founded     495\n",
       "company_revenue     135\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SalaryData.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d84942",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be467ab",
   "metadata": {},
   "source": [
    "## 1. Covert All Salary to Annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11fa96c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    $70,000 /yr (est.)\n",
       "1     $42.50 /hr (est.)\n",
       "2     $67.50 /hr (est.)\n",
       "3                   NaN\n",
       "4     $65.00 /hr (est.)\n",
       "Name: salary_estimate, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SalaryData['salary_estimate'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbc77d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_salary(salary_string):\n",
    "    if pd.isnull(salary_string):\n",
    "        return np.nan\n",
    "    else:\n",
    "        match_year = re.search(r'\\$(\\d{1,3},?\\d{0,3},?\\d{0,3}) \\/yr \\(est.\\)', salary_string)\n",
    "        match_hour = re.search(r'\\$(\\d+(\\.\\d+)?) \\/hr \\(est.\\)', salary_string)\n",
    "\n",
    "        if match_year:\n",
    "            salary_amount = float(match_year.group(1).replace(',',''))\n",
    "        elif match_hour:\n",
    "            hourly_salary = float(match_hour.group(1))\n",
    "            salary_amount = hourly_salary *1800\n",
    "        else:\n",
    "            salary_amount = np.nan\n",
    "        return salary_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5eff075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     70000.0\n",
       "1     76500.0\n",
       "2    121500.0\n",
       "3         NaN\n",
       "4    117000.0\n",
       "Name: salary_estimate, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SalaryData['salary_estimate'] = SalaryData['salary_estimate'].apply(clean_salary)\n",
    "SalaryData['salary_estimate'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779e3da6",
   "metadata": {},
   "source": [
    "## 2. Clean Location And Extract States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9edb0ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Remote           149\n",
       "GA                96\n",
       "TX                94\n",
       "CA                87\n",
       "NJ                80\n",
       "MN                49\n",
       "DC                46\n",
       "VA                44\n",
       "WI                36\n",
       "MD                34\n",
       "IL                34\n",
       "MS                24\n",
       "NY                21\n",
       "MA                19\n",
       "CT                18\n",
       "OR                17\n",
       "United States     14\n",
       "PA                12\n",
       "UT                 8\n",
       "TN                 6\n",
       "FL                 4\n",
       "OH                 3\n",
       "DE                 1\n",
       "SC                 1\n",
       "OK                 1\n",
       "CO                 1\n",
       "NC                 1\n",
       "Name: job_state, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SalaryData['location'] = SalaryData['location'].astype(str)\n",
    "SalaryData['job_state'] = np.where(SalaryData['location'].str.lower() == 'remote', SalaryData['location'], SalaryData['location'].str.split(', ').str[-1])\n",
    "SalaryData.job_state.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec880c",
   "metadata": {},
   "source": [
    "## 3. Clean Job Title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f2a04",
   "metadata": {},
   "source": [
    "### How do titles look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7d67549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AWS Data Engineer',\n",
       " 'AWS Senior Data Engineer',\n",
       " 'AWS python data engineer',\n",
       " 'Anaplan Data Engineer',\n",
       " 'Azure Data Engineer',\n",
       " 'Azure Data Engineer (Azure Data Factory, Databricks)',\n",
       " 'Azure Tech Lead/ Sr Data Engineer',\n",
       " 'Big Data Engineer',\n",
       " 'Big Data SDET Engineer',\n",
       " 'Big Data Sytems Engineer(Hadoop)',\n",
       " 'Cloud Data Engineer (Azure)',\n",
       " 'Data Analytics Engineer',\n",
       " 'Data Engineer',\n",
       " 'Data Engineer (ETL & Data Catalogue Support)',\n",
       " 'Data Engineer (ETL)',\n",
       " 'Data Engineer (MDM)',\n",
       " 'Data Engineer (Remote)',\n",
       " 'Data Engineer - Hybrid Onsite - King of Prussia, PA',\n",
       " 'Data Engineer - Onsite',\n",
       " 'Data Engineer - Remote',\n",
       " 'Data Engineer 925',\n",
       " 'Data Engineer ETL',\n",
       " 'Data Engineer | PAID BOOTCAMP',\n",
       " 'Data Engineer- Remote',\n",
       " 'Data Engineer/Data Scientist',\n",
       " 'Data Integration Engineer',\n",
       " 'Data Logging Engineer',\n",
       " 'Data Software Engineer',\n",
       " 'Data Warehouse Engineer',\n",
       " 'Data engineer lead',\n",
       " 'ETL Data Engineer',\n",
       " 'Full Stack Data Engineer',\n",
       " 'GCP Data Engineer',\n",
       " 'Hadoop Big Data Engineer',\n",
       " 'Jr. Data Engineer',\n",
       " 'Junior Data Engineer',\n",
       " 'Junior Data Scientist or Data Engineer -ONSITE',\n",
       " 'Lead Data Engineer',\n",
       " 'Lead Informatica / Data Engineer',\n",
       " 'Manufacturing Data Engineer',\n",
       " 'Python Data Engineer',\n",
       " 'Senior Data Engineer',\n",
       " 'Senior Data Engineer (Must be local to Atlanta, GA)',\n",
       " 'Senior Data Engineer - Remote',\n",
       " 'Senior Data Engineer, Analytics',\n",
       " 'Senior/Lead Data Engineer',\n",
       " 'Snowflake Data Engineer',\n",
       " 'Software Engineer, Senior – Data and Analytics',\n",
       " 'Sr. Data Engineer',\n",
       " 'Wells Data Engineer']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(SalaryData['job_title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef964535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contains \"data engineer\" =  845\n",
      "Not contains \"data engineer\" =  55\n"
     ]
    }
   ],
   "source": [
    "print('Contains \"data engineer\" = ', SalaryData['job_title'].str.contains('data engineer', case=False).sum())\n",
    "print('Not contains \"data engineer\" = ', (~SalaryData['job_title'].str.contains('data engineer', case=False)).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639b0527",
   "metadata": {},
   "source": [
    "### Remove non-data engineer job postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d55e2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of rows =  845\n"
     ]
    }
   ],
   "source": [
    "SalaryData = SalaryData[SalaryData['job_title'].str.contains('data engineer', case=False)]\n",
    "print('# of rows = ',SalaryData.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562268e8",
   "metadata": {},
   "source": [
    "### Categorize by seniority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dd9556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seniority (title):\n",
    "    \n",
    "    # considered senior if title has sr., senior, lead or principal\n",
    "    if'sr' in title.lower() or 'senior' in title.lower() or 'lead' in title.lower() or 'principal' in title.lower():\n",
    "        return 'senior'\n",
    "    elif 'jr' in title.lower() or 'junior' in title.lower():\n",
    "        return 'junior'\n",
    "    else:\n",
    "        return 'intermediate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3919eb14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intermediate    626\n",
       "senior          210\n",
       "junior            9\n",
       "Name: seniority, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SalaryData['seniority'] = SalaryData['job_title'].apply(seniority)\n",
    "SalaryData.seniority.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b263da",
   "metadata": {},
   "source": [
    "## 4. Extract Skills From Job Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcde9e70",
   "metadata": {},
   "source": [
    "### How do job descriptions look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5346666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(Submit Resume and All Inclusive hourly rate or Salary).\\nThe selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.\\no Preferred skill set:\\n§ Strong background in Microsoft SQL Server, both installing/configuring and administering.\\n§ Experience with Microsoft Azure.\\n§ Experience designing and implementing ETL processes.\\n§ Experience with event-driven automated systems preferred.\\n§ Some familiarity with Tableau Server and Office 365 preferred.\\n§ Strong communication skills\\nJob Types: Full-time, Contract\\nSchedule:\\nMonday to Friday\\nWork Location: Hybrid remote in DeKalb, IL 60115',\n",
       " \"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)\\n\\nFeeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?\\n\\nIf so, we're looking for a motivated and driven person like you who has:\\nStrength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems\\nExperience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data\\nA passion for continuous improvement in refining the approach and doing it better and faster the next time\\n\\nBonus points if you're bringing knowledge of or really want to learn the following:\\nConsultancy experience with a focus on Agile practices\\nAWS and Azure Cloud\\nPython or similar scripting languages\\nAWS Quicksight, Tableau, Power BI, or other visualization tools\\n\\nIn return, we offer:\\nA mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields\\nA home where your voice matters and you can affect real change\\nAn employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture\\n\\nWe do not want you to make the leap without knowing what we need, so here is how we define success for this position:\\nSoak up knowledge from the existing team of experts in the first 30 days\\nBring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months\\nMentor a new data engineering hire in your first 90 days\\n\\nWe need to know - can you make this happen? If so, we definitely need to talk to you.\\n\\nWe value diversity at Double Line. We hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.\\n\\nDouble Line does not sponsor applicants for work visas at this time.\\n\\nDouble Line does not currently offer relocation assistance.\\n\\niS4dbmGU5w\",\n",
       " '12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.\\nThe DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.',\n",
       " '12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.\\nThe DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.\\nJob Types: Full-time, Contract\\nPay: $80.00 - $90.00 per hour\\nSchedule:\\n8 hour shift\\nExperience:\\nLinux-based Operating Systems: 8 years (Preferred)\\nETL: 10 years (Preferred)\\nWork Location: Hybrid remote in Washington, DC 20001',\n",
       " '12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.\\nThe DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and system administrator for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Support the System Administration of associated tools and software of data and analytics landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, and analytic products. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.\\nJob Types: Full-time, Contract\\nPay: $80.00 - $90.00 per hour\\nSchedule:\\n8 hour shift\\nExperience:\\nETL processes development: 10 years (Preferred)\\nWork Location: Remote',\n",
       " \"3+ years of experience in data engineering, software engineering, or other related roles. 3+ years in relational database concepts with a solid knowledge of star schema, SQL, SQL Tuning, OLAP, Big Data technologies 3+ years of experience in generating and maintaining data pipelines from various data sources, in collaboration with diverse stakeholders. 3+ years of experience working with Amazon Webservices, S3, EMR, Redshift etc. Experience with best practices for development including query optimization, version control, code reviews, and documentation. Experience with coding languages like Python/Java/Scala\\nAbout Us: Twitch is the world's biggest live streaming service, with global communities built around gaming, entertainment, music, sports, cooking, and more. It's where millions of people come together to chat, interact, and make their own entertainment. We're about community, inside and out. You'll find coworkers who are eager to team up, collaborate, and smash (or elegantly solve) problems together. We're on a quest to empower live communities, so if this sounds good to you, see what we're up to on LinkedIn and Twitter, get interviewing tips on Instagram, and discover projects we're solving on our Blog. About the Role: Data is central to Twitch's decision-making process, and data engineers operate at the forefront of this by creating authoritative datasets that drives analysis and decision-making across all of Twitch. In this role you will be shaping the way that business performance is measured, defining how we transform our data, and scaling analytics methods and tools to support our growing business, leading the way for high quality, high velocity decisions. For this role, we're looking for an experienced data engineer to join our Content Data Science team, which is focused on empowering staff throughout Twitch to use and trust our business data. Your responsibilities may range from developing and enhancing our data warehouse which act as authoritative sources of truth across the company, driving data quality and trustworthiness across product verticals and business areas, building self-service business intelligence infrastructure for analysts, as well as connecting into data interfaces that enable everyone in Twitch to discover and analyze the data. In the process, you will have the opportunity to interact with technical and non-technical staff members throughout the company, and will report to the Director of Content Data Science. This position can be located in San Francisco, CA; Irvine, CA; Seattle, WA; New York, NY; and Salt Lake City, UT. You Will: Define and own team level data architecture for trusted, governed, dimensionally-modeled repository of data that enables Twitch staff to quickly and reliably answer their business questions. Keep existing data sources fresh against data quality issues, design, develop and maintain data quality assurance framework and continuously improve the processes for developing new ones raising the level of quality expected from our work. Conduct unit, integration, and system tests on our data sources in order to validate data against source systems, and continuously optimize performance in order to improve query speed and reduce cost. Improve search, discovery and literacy: Create exploration and visualization interfaces in our BI tools and evangelize the adoption of these sources across the company through education and training programs. Improve business and engineering team processes via data architecture, engineering, test, and operational excellence best practices. Make enhancements that improve data processes.\\nBonus Points\\nA passion for data science and interest in growing / learning data science, machine learning at scale.\\nA passion for games and the gaming industry\\nPerks\\nMedical, Dental, Vision & Disability Insurance\\n401(k)\\nMaternity & Parental Leave\\nFlexible PTO\\nAmazon Employee Discount\\nMonthly Contribution & Discounts for Wellness Related Activities & Programs (e.g., gym memberships, off-site massages, etc.)\\nWe are an equal opportunity employer and value diversity at Twitch. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, or disability status, or other legally protected status.\\nPursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.\\nPursuant to the Los Angeles Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.\\n\\nWe are an equal opportunity employer and value diversity at Twitch. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, or disability status, or other legally protected status.\\n\\nPursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.\\n\\nOur compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $105,700/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. Applicants should apply via our internal or external career site.\",\n",
       " \"APLA is building capabilities around the company's data foundation to build data sources that are needed for reporting and analytics\\nThe type of engineer were looking for is a Full Stack Data Engineer\\nKnowledge of data visualization engineering as well as consumption and view build engineering\",\n",
       " 'AR# 226054\\nRole: Lead Data Engineer /Databricks (On-site)\\nLocation: Mt. Laurel, NJ\\nFull-time\\nVisa status: GC/USC\\nMust have skills:\\nDatabricks, Python, RDBMS, PowerShell scripting, data warehouse\\nDetailed JD:\\nExperience in ETL/Pipeline Development using tools such as Azure Databricks/Apache Spark and Azure\\nData Factory with development expertise on batch and real-time data integration\\nExperience in programming using Python\\nRDBMS knowledge and experience in writing the Store Procedures\\nExperience in writing bash and Power shell scripting.\\nExperience in data ingestion, preparation, integration, and operationalization techniques in optimally addressing the data requirements\\nExperience in Cloud data warehouse like Azure Synapse, Snowflake analytical warehouse\\nExperience with Orchestration tools, Azure DevOps, and GitHub\\nExperience in building end to end architecture for Data Lakes, Data Warehouses and Data Marts\\nExperience in relational data processing technology like MS SQL, Delta Lake, Spark SQL, SQL Server\\nExperience to own end-to-end development, including coding, testing, debugging and deployment\\nExtensive knowledge of ETL and Data Warehousing concepts, strategies, methodologies\\nExperience working with structured and unstructured data\\nFamiliarity with Azure services like Azure functions, Azure Data Lake Store, Azure Cosmos\\nAbility to provide solutions that are forward-thinking in data and analytics\\nJob Type: Full-time\\nSalary: $120.00 - $130.00 per year\\nSchedule:\\n8 hour shift\\nExperience:\\nData Warehouse: 10 years (Required)\\nPython: 10 years (Required)\\nPowerShell: 10 years (Required)\\nData Bricks: 10 years (Required)\\nRDBMS: 10 years (Required)\\nWork Location: On the road\\nSpeak with the employer\\n+91 609-917-9952',\n",
       " 'AWS Python Data Engineer\\nMust Have: AWS Databricks Python, Spark, PySpark\\nLocation Hartford, CT or St. Paul, MN or Hunt Valley, MD\\nRoles & responsibilities:\\nActs as a single point of contact for data migration to AWS projects for customer\\nProvides innovative and cost-effective solution using AWS, Spark, python & customer suggested toolset\\nOptimizes the use of all available resources\\nDevelops solutions to meet business needs that reflect a clear understanding of the objectives, practices and procedures of the corporation, department and business unit\\nAs a leader in the Cloud Engineering you will be responsible for the overseeing development\\nLearn/adapt quickly to new Technologies as per the business need\\nDevelop a team of Operations Excellence, building tools and capabilities that the Development teams leverage to maintain high levels of performance, scalability, security and availability\\nSkills:\\nThe Candidate must have 3-5 yrs of experience in PySpark & Python\\nHands on experience on AWS Cloud platform especially S3, lamda, EC2, EMR\\nExperience on spark scripting\\nHas working knowledge on migrating relational and dimensional databases on AWS Cloud platform\\nRelevant experience with ETL methods and with retrieving data from dimensional data models and data warehouses.\\nStrong experience with relational databases and data access methods, especially SQL.\\nKnowledge of Amazon AWS architecture and design\\nJob Type: Full-time\\nSchedule:\\n8 hour shift\\nExperience:\\nPython: 3 years (Preferred)\\nWork Location: On the road',\n",
       " 'About the Position\\nWe are looking for a Data Engineer who can help us understand, clean, manage, and share the data that guides our trading. At Jane Street, having a thorough and accurate understanding of data is at the core of the work we do.\\nUsing our mix of in-house and open-source software, you will analyze datasets gathered from a variety of sources, checking for anomalies, matching formats and symbologies, automating ETL processes, and generally making it easier for our traders to generate valuable insights.\\nYou should be excited about digging deep into datasets and explaining your findings to different types of colleagues, working collaboratively with traders and software engineers.\\nWhile prior experience with financial data would be nice, we don’t expect you to have a finance background. We’re happy to hire talented engineers and teach them what they need to know.\\nAbout You\\nTop-notch programming skills in any language (Python a plus)\\nExperience with using SQL and relational databases\\nExperience with generating data visualizations\\nKnowledge of statistical techniques, including multivariate regression and time series analysis\\nClear and concise communication skills; able to efficiently analyze and deconstruct technical problems\\nFluency in English required\\nBase salary is $175,000 - $300,000. Base salary is only one part of Jane Street total compensation, which includes an annual discretionary bonus.\\nJane Street is an Equal Opportunity Employer',\n",
       " 'About the Role\\nRocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.\\nThe Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.\\nRocket Travel is a place where you:\\nWork with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.\\nEmbody curiosity, community, and accountability. We live and build products by these values every day.',\n",
       " \"About the Role\\nRocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.\\nThe Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.\\nRocket Travel is a place where you:\\nWork with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.\\nEmbody curiosity, community, and accountability. We live and build products by these values every day.\\nOwn decisions and take action that can be implemented in a matter of days (or hours).\\nGet inspired and encouraged to vacation faster, with an annual vacation stipend.\\nReceive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.\\nTotal Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%\\nCan have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.\\nShare your passion for travel with equally adventurous teammates.\\nWork within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels\\nAs a Senior Data Engineer at Rocket Travel, you will:\\nBuild SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics\\nOwn projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data\\nEvolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins\\nWork closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance\\nImprove and expand our suite of automated tests using pytest and great expectations\\nContinuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt\\nCollaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models\\nMaintain a sense of empathy for our customers and move quickly where users are most acutely affected\\nAbout you:\\nMust Haves:\\nYou have worked with Python/SQL for at least 3 years in a professional setting\\nYou have worked with Python common data related libraries (pandas, numpy)\\nYou have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)\\nYou have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt\\nYou care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection\\nYou are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers\\nYou communicate clearly and effectively\\n\\nOther experience you may have\\nThis is not meant to be a comprehensive list, but more so serve as examples of experience you may have\\nPipeline orchestration (Airflow, DBT)\\nDevOps (Git, Jenkins, GitLab)\\nAWS data related tools (kinesis, glue, S3, sagemaker)\\nBig data technologies (Hadoop, Spark)\\nContainerization technologies (kubernetes, ECS, docker)\\n\\nAbout Rocket Travel\\nWe make travel more rewarding than anyone else\\nRocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.\\n\\nOur journey\\nWe began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.\\n\\nWe now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.\\n\\nTravel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.\\n\\nA diverse and global team\\nOur teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.\\nAll of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.\\n\\nNote on general employment requirements\\nCandidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.\\n\\nEqual Opportunity Employer\\nAt Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.\\n\\nA Final Word:\\nTo all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.\",\n",
       " 'About us:\\nAt Cognira, we strongly believe that people are the biggest asset of our company.\\nOur hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.\\nOur industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.\\nFor the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.\\n\\nAre you ready to grow with us?\\nTo find out more about Cognira, please visit our website at www.cognira.com\\n\\nAbout this role\\nCognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.\\n\\nWhat you will do\\nCreate innovative, scalable, and fault-tolerant solutions\\nDesign configurable and reusable components for a multi-tenant environment\\nWork closely with project and product management, data scientists, and QA in a fast-paced team environment\\nInvestigate and resolve technical and performance issues\\nExperiment, fail-fast, and learn, as you build skills and experience\\nEnsure high quality with test automation\\n\\nWe would love to hear from you if you have\\nYou have experience with Java, Scala, Python, and other programming languages\\nYou have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm\\nYou are conversant in asynchronous programming techniques\\nYou are familiar with database design techniques in a distributed NoSQL cloud environment\\nYou are familiar with Apache Spark and big data analytics\\nYou are aware of data science tools and techniques, including Tensorflow and Keras\\nYou are conversant in agile team development using tools such as Git, Jenkins, Jira, and more\\nYou have excellent communication and presentation skills\\n\\nPerks\\nIn addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!\\n\\nYou get the choice to work on a Mac or a PC.\\nCasual dress code, social events, and after-works.\\nFlexible, diverse work environment.\\nRespectful, innovative team.\\nBut it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.\\nYou get 21 days of PTO, and major national holidays.',\n",
       " \"Airbus is an international pioneer in the aerospace industry. We are a leader in designing, manufacturing and delivering aerospace products, services and solutions to customers on a global scale. We aim for a better-connected, safer and more prosperous world.\\nA commercial aircraft manufacturer, with Space and Defense as well as Helicopters Divisions, Airbus is the largest aeronautics and space company in Europe and a worldwide leader.\\nAirbus has built on its strong European heritage to become truly international - with roughly 180 locations and 12,000 direct suppliers globally. The company has aircraft and helicopter final assembly lines across Asia, Europe and the Americas, and has achieved a more than sixfold order book increase since 2000.\\n\\nPosition Summary:\\nWe are looking to hire a talented big data engineer to develop and manage our company's Big Data solutions on Airbus Skywise Platform. In this role, you will be required to design and implement ELT data transformation processes on Skywise, collaborate with development teams, build cloud platforms, and maintain the production system.\\n\\nTo ensure success as a big data engineer, you should have prior knowledge of Apache Spark or similar frameworks, excellent project management skills, and high-level problem-solving skills. A top-notch Data Engineer understands the needs of the company and institutes scalable data solutions for its current and future needs.\\n\\nPrimary Responsibilities:\\nWrite code to transform real-world data into high-signal models.\\nLoading disparate data sets and conducting pre-processing transformation services.\\nExtract insights from airline and aviation data that will revolutionize our business strategy and communicate insights that resonate with partners.\\nCollaborate with the software development teams.\\nMaintaining production systems.\\nTraining staff on data resource management.\\n\\nQualified Experience / Skills / Training:\\nBachelor's degree in computer engineering or computer science.\\nPrevious experience as a big data engineer.\\n2-3 years of professional experience with programming languages such as Python, R, Java, or similar languages.\\nPrior experience working with Apache Spark is also desirable.\\nGood project management skills.\\nGood communication skills.\\nAbility to adapt to a dynamic work environment by consistently revising your approach in response to new information.\\n\\nEducation:\\nBachelor's Degree or equivalent work experience\\n\\nExperience:\\nExperience with Palantir big data platform.\\nData Science expertise.\\n\\nEligibility:\\nAuthorized to Work in the US\\nClearance:\\nNone\\n\\nEqual Opportunity: Airbus Group is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Airbus Group is also committed to compliance with all fair employment practices regarding citizenship and immigration status.\\nAs a leader in our field, Airbus Group provides relocation assistance for qualified positions and a comprehensive compensation and benefits package.\\nAirbus Group does not offer tenured or guaranteed employment. Employment with Airbus Group is at will, meaning either the company or the employee can terminate the employment relationship at any time, with or without cause, with or without notice.\",\n",
       " \"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.\\nRole and Responsibilities:\\nDesign and Develop scalable industry standard ETL pipelines using big data technologies.\\nLoading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.\\nIngest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.\\nBuild and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.\\nHigh-speed querying using in-memory technologies such as Spark.\\nDesign and implement data modeling\\nTranslate complex functional and technical requirements into detailed design\\nCode and query Optimization.\\nSkills & Qualifications\\n3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)\\nProficient in a scripting language and object-oriented language – Python/Scala preferred\\nProficient with a public cloud (AWS, Azure, GCP)\\nExpert in SQL development\\nProficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)\\nAdept in multi-threading and concurrency concepts.\\nJob Type: Full-time\\nPay: $100,000.00 - $151,000.00 per year\\nExperience level:\\n3 years\\nSchedule:\\nMonday to Friday\\nAbility to commute/relocate:\\nSan Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)\\nExperience:\\nScripting: 2 years (Required)\\nHadoop: 2 years (Preferred)\\nPython: 2 years (Required)\\nWork Location: One location\",\n",
       " \"Are you seeking a rewarding and fulfilling career in the Mental Health and Addictions field?\\nApply to be a Data Engineer today!\\nWhat will your day look like?\\nAt Horizon, you will enjoy a supportive, team-based work environment. Have a question? There is always someone there to help! We offer a seamless onboarding experience that will ensure your success in your new role.\\nAs a Data Engineer at Horizon you will...\\nAcquire and assemble datasets that align with business needs\\nIdentifying, designing, and implementing internal process improvements including optimizing data delivery, and automating manual processes\\nBuilding required infrastructure for optimal extraction, transformation and loading of data from various data sources\\nBuilding analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency\\nWorking with stakeholders including executive teams and assisting them with data-related technical issues\\nWorking with stakeholders to support their data infrastructure needs and understand company objectives\\nBuild, test, and maintain database pipeline architectures\\nCreate new data validation methods and data analysis tools\\nAssist and support development of data governance policies\\nCreate and distribute corporate reports using industry standard reporting tools and software such as SQL, Power BI, and Python.\\nDevelop and maintain the workflows and security controls related to data extract, transform, and load (ETL) of corporate data\\nProviding technical expertise in data storage structures, data mining, and data cleansing.\\nDevelop and maintain databases, data marts, models, data sets, and other key technical solutions\\nWhy choose Horizon to build your career?\\nBesides the fact that we’ve been named a Best Place to Work for 14 (yes, 14!) years in a row? At Horizon, you can be assured that you will make difference in the lives of others. Even better, your teammates will be just as motivated to make a difference!\\nWhat we offer that you’ll love…\\nCompany Culture: At Horizon, we pride ourselves on cultivating an atmosphere of teamwork where all employees feel heard and valued.\\nDiversity & Inclusion: We are committed to equity, racial justice, and equal opportunity for all, and strive toward this goal through the work of our Diversity, Equity, Inclusion and Belonging Council, frequent trainings, ongoing conversations, affinity groups, and more.\\nTrainings, Trainings, and More Trainings: We have an entire team dedicated to your personal development and professional growth.\\nTeam Building, Connection, and Relationships: At Horizon, we’re more than co-workers, we’re a community. We support each other, celebrate our achievements and milestones together, and have fun together!\\nRetirement: We know you want to retire comfortably and we’re here to help! Horizon offers 401(k) AND profit-sharing programs to make sure you’re set for the future.\\nStudent Loan Assistance: We help pay off our team members' student loans every month. One year after joining, you’ll have been able to pay off an extra $600!\\nPTO & Holidays: We believe self-care is essential. Combined with holidays, you’ll earn up to 22.5 paid days in your first year. By your 3rd anniversary, that's almost doubled with up to 37.5 paid days off!\\nWhat makes you a great candidate?\\nWe can’t wait to learn more about you! Here are a few specifics of what you’ll need for the job:\\nBachelor's degree in Computer Science, Information Systems, Engineering or equivalent\\n1+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL and reporting/analytic tools\\nStrong experience in coding languages like SQL and Python\\nFluent in relational based systems and writing complex SQL\\nStrong analytical and problem-solving skills\\nStrong understanding of database technologies and management systems.\\nStrong understanding of data structures and algorithms\\nThorough knowledge of Microsoft Office\\nAbility to work with multidisciplinary teams\\nAbility to multitask and manage competing deadlines\\nAbility to communicate appropriately with all levels of management\\nExcellent understanding of Microsoft Office suite\\nAbility to build and optimize data sets\\nAbility to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions\\nExcellent analytic skills associated with working on unstructured datasets\\nAbility to build processes that support data transformation, workload management, data structures, dependency and metadata\\nLocation:\\n55 Dodge Road, Getzville, NY 14068\\nHours:\\nFull-time position, Monday - Friday; 8:00am-5:00pm\\nDisclaimer:\\nHorizon endorses public health measures including vaccinations. We encourage all applicants to be mindful of the fact that Horizon is a healthcare agency proving in person services throughout our community.\\nThis information is intended to provide a general overview of the position; it is not a full job description.\\n\\nEqual Opportunity Employer/Protected Veterans/Individuals with Disabilities\\nThe contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)\",\n",
       " \"As a member of the data engineering team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premise data sources as well as cloud and remote systems.\\nResponsibilities:\\nActive contributor to code development in projects and services.\\nManage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\\nBuild and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.\\nResponsible for implementing best practices around systems integration, security, performance and data management.\\nEmpower the business by creating value through the increased adoption of data, data science and business intelligence landscape.\\nCollaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.\\nDevelop and optimize procedures to “productionalize” data science models.\\nDefine and manage SLA’s for data products and processes running in production.\\nSupport large-scale experimentation done by data scientists.\\nPrototype new approaches and build solutions at scale.\\nResearch in state-of-the-art methodologies.\\nCreate documentation for learnings and knowledge transfer.\\nCreate and audit reusable packages or libraries.\\nRequirements:\\n2+ years of overall technology experience that includes at least 2+ years of hands-on software development, data engineering, and systems architecture.\\n2+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\\n1+ years in cloud data engineering experience in Azure Certification is a plus.\\nExperience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.\\nExperience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\\nExperience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\\nExperience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.\\nExperience with version control systems like Github and deployment & CI tools.\\nExperience with Statistical/ML techniques is a plus.\\nExperience with building solutions in the retail or in the supply chain space is a plus\\nUnderstanding of metadata management, data lineage, and data glossaries is a plus.\\nWorking knowledge of agile development, including DevOps and DataOps concepts.\\nFamiliarity with business intelligence tools (such as PowerBI)\\nCovid-19 vaccination may be a condition of employment dependent on role and location. For specific information, please discuss role requirements with the recruiter\\nEducation\\nBA/BS in Computer Science, Math, Physics, or other technical fields\\nSkills, Abilities, Knowledge\\nExcellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\\nProven track record of leading, mentoring data teams.\\nStrong change manager. Comfortable with change, especially that which arises through company growth. Able to lead a team effectively through times of change.\\nAbility to understand and translate business requirements into data and technical requirements.\\nHigh degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\\nPositive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\\nStrong leadership, organizational and interpersonal skills; comfortable managing trade-offs.\\nFoster a team culture of accountability, communication, and self-management.\\nProactively drives impact and engagement while bringing others along.\\nConsistently attain/exceed individual and team goals\\nAbility to lead others without direct authority in a matrixed environment.\\nJob Type: Full-time\\nPay: $85,000.00 - $90,000.00 per year\\nSchedule:\\nMonday to Friday\\nWork Location: Hybrid remote in Plano, TX 75024\",\n",
       " 'Azure data factory, data bricks, data lake, automation and performance optimization of ETL\\npower BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning\\nPrior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.\\nJob Type: Full-time\\nSalary: From $150,000.00 per year\\nSchedule:\\n8 hour shift\\nAbility to commute/relocate:\\nMahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)\\nExperience:\\nData Engineer: 8 years (Preferred)\\nAzure Data factory: 8 years (Preferred)\\nETL: 8 years (Preferred)\\nDataLakes / Databricks: 6 years (Preferred)\\nCI/CD: 7 years (Preferred)\\nWork Location: One location',\n",
       " \"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience\\nAdvanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata\\nExtensive knowledge of coding and scripting languages, SQL, Python, etc.\\nStrong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus\\nExperience implementing metadata solutions for configurable codebase\\nAbility to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues\\nExperience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises\\nHands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.\\nSolid experience implementing solutions on AWS based data lakes\\nExperience in system analysis, design, development, and implementation of data ingestion pipeline in AWS\\nEnd-to-end data solutions (ingest, storage, integration, processing, access) on AWS\\nImplement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)\\nMigrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift\\nMigrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift\\nAWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred\\nJob Type: Full-time\\nPay: $111,544.00 - $131,964.00 per year\\nBenefits:\\n401(k)\\nDental insurance\\nHealth insurance\\nPaid time off\\nVision insurance\\nSchedule:\\n5x8\\n8 hour shift\\nDay shift\\nMonday to Friday\\nAbility to commute/relocate:\\nNewark, NJ: Reliably commute or planning to relocate before starting work (Required)\\nExperience:\\nInformatica: 1 year (Preferred)\\nSQL: 1 year (Preferred)\\nData warehouse: 1 year (Preferred)\\nWork Location: In person\",\n",
       " \"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience\\nAdvanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata\\nExtensive knowledge of coding and scripting languages, SQL, Python, etc.\\nStrong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus\\nExperience implementing metadata solutions for configurable codebase\\nAbility to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues\\nExperience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises\\nHands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.\\nSolid experience implementing solutions on AWS based data lakes\\nExperience in system analysis, design, development, and implementation of data ingestion pipeline in AWS\\nEnd-to-end data solutions (ingest, storage, integration, processing, access) on AWS\\nImplement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)\\nMigrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift\\nMigrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift\\nAWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred\\nJob Type: Full-time\\nPay: $111,544.00 - $131,964.00 per year\\nBenefits:\\n401(k)\\nDental insurance\\nHealth insurance\\nPaid time off\\nVision insurance\\nSchedule:\\n5x8\\n8 hour shift\\nDay shift\\nMonday to Friday\\nAbility to commute/relocate:\\nNewark, NJ: Reliably commute or planning to relocate before starting work (Required)\\nExperience:\\nInformatica: 1 year (Preferred)\\nSQL: 1 year (Preferred)\\nData warehouse: 1 year (Preferred)\\nWork Location: One location\",\n",
       " 'Bachelor’s Degree in Computer Science or a related discipline\\n5+ years of applicable engineering experience\\nStrong proficiency in Python with an emphasis in building data pipelines\\nAbility to write complex SQL to perform common types of analysis and aggregations\\nExperience with Apache Airflow or Google Composer\\nDetail-oriented and document all the work\\nAbility to work with others from diverse skill-sets and backgrounds\\nGCP solution architect - certified\\nExperience in GCP, Big Query\\nWorking experience in Databricks, Spark is expected\\nJob Types: Full-time, Contract\\nBenefits:\\n401(k)\\nHealth insurance\\nPaid time off\\nSchedule:\\n8 hour shift\\nWork Location: One location\\nSpeak with the employer\\n+91 (727) 216-7989',\n",
       " 'Become part of the Converse Team\\n\\nConverse is a place to explore potential, break barriers and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At Converse, it’s about each person bringing skills and passion to a challenging and constantly evolving world to make things better as a team.\\nConverse, Inc. Boston, MA. Work closely with Project Management and Business teams to completely define specifications to ensure the project acceptance. Involved in preparation of functional and technical specifications with different cross teams. Lead team, defining solution options, providing estimates on effort and risk, and evaluating technical feasibility in Agile development process, including Scrum and Kanban. Work on troubleshooting data and analytics issues and perform root cause analysis to proactively resolve issues. Develop data extracts and feeds from the full spectrum of systems in the Converse ecosystem, including transactional ERP systems, POS data, product and merchandising systems. Engineer data products for a variety of Operations analytics use cases, ranging from reporting and data visualization to advanced analytics/machine learning use cases. Support designing technical specifications and data transformation models for junior developers. Ensure development is on track and meets specifications as defined by product management and the business. Responsible for data integrity of current platform and QA of new releases. Support the development and maintenance of backlog items and solution feature. Participate in sprint planning activities from a development perspective. Responsible for designing cloud-based data architecture using AWS stacks. Design and develop Python data science and data engineering libraries dealing with structured and unstructured data. Work with a variety of database types (SQL/NoSQL, columnar, object-oriented) and diverse data formats. Responsible for ETL with Spark and building data pipelines/orchestrations in Airflow and working on ETL tools like Matillion. Responsible for DevOps toolchain and Continuous Development, Continuous Integration and Automated Testing using Jenkins. Ensure and use data engineering for advanced analytics/data science and Software development skills.\\nApplicant must have a Bachelor’s degree in Computer Science, Information Systems, or Information Technology and 5 years of progressive post-baccalaureate experience in the job offered or a related occupation. Experience must include:\\nData warehousing;\\nETL or ELT;\\nAmazon Web Service (AWS) Cloud Services, including AWS S3, AWS Lambda, AWS EC2, AWS EMR or AWS DynamoDB;\\nRelational Database Management Systems (RDBMS), such as Oracle, Teradata, SQL Server or Snowflake;\\nDatabase Development with writing stored procedures, functions, triggers, cursors or SQL queries;\\nHadoop, HDFS, Hive or Spark;\\nProgramming languages, including Java or Python;\\nBusiness Intelligence Tools, such as Tableau;\\nUnix Shell scripting; and\\nVersion control systems, such as Git, Bitbucket or Github\\n#LI-DNP\\nConverse is more than a company; it’s a worldwide advocate for self-expression. This belief motivates our employees, permeates our working environment and inspires our products. No two of us look or think exactly alike. We are each one-of-a-kind. Individually and as a culture, we have the freedom to create and grow professionally. Generous benefits packages only sweeten the experience. From Boston to Shanghai, from Brand Design to Finance, Converse is a brand that celebrates the unique and creative people of the world. Together, we’re different.',\n",
       " \"Boston Globe Media is New England's largest newsgathering organization - and much more. We are committed to being an indispensable, trusted, reliable source of round-the-clock information. Through the powerful journalism from our newsroom, engaging content from our content marketing studio, or through targeted advertising solutions, brands and marketers rely on us to reach highly engaged, educated and influential audiences through a variety of media and experiences.\\nResponsibilities:\\nCollect, organize, and document often-used data resources (maps, APIs, etc).\\nCreate scripts to scrape data from websites for stakeholders.\\nWith guidance, start creation of a data style guide.\\nTechnology:\\nBasic knowledge of HTML, CSS, and JavaScript.\\nBasic familiarity with PHP, Groovy, or another server side scripting language.\\nBasic familiarity of build tools such as Grunt, Gulp, or Webpack.\\nBasic familiarity with version control systems such as SVN or Git.\\nQualifications:\\nUnderstands and follows the team’s agile process.\\nAdheres to defined coding standards.\\nParticipates in code reviews.\\nA willingness to adapt and be audience focused, with a curious mindset and a commitment to creating an inclusive work environment\\nVaccination Statement:\\nWe require that all BGMP employees (including temporary employees, co-ops, interns, and independent contractors) be vaccinated from COVID-19, unless an exemption from this policy has been granted as an accommodation or otherwise. All BGMP employees, regardless of vaccination status or work location, must provide proof of vaccination status as instructed by the employee's designated Human Resources contact. Employees may request a reasonable accommodation or other exemption from this policy by contacting their designated Human Resources contact. Failure to comply with or enforce any part of this policy, or misrepresentation of compliance with this policy, may result in discipline, up to and including termination of employment, subject to reasonable accommodation and other requirements of applicable federal, state, and local law.\\nEEO Statement:\\nBoston Globe Media Partners is an equal employment opportunity employer, and does not discriminate on the basis of race, color, religion, gender, sexual orientation, gender identity or expression, age, disability, national origin, ancestry, genetic information, military or veteran status, pregnancy or pregnancy-related condition or any other protected characteristic. Boston Globe Media Partners is committed to diversity in its most inclusive sense.\\nwcZyZ7QvrB\",\n",
       " 'Boston, MA\\nFull-Time\\n\\nWe need your help.\\nWe’re looking for a self-motivated and detail-oriented data engineer to join our DataOps team. This opportunity will enable you contribute to the operation, support, and enhancement of our mission critical data operations platform and the development of data pipelines. Our data operations platform supports high volume, high velocity data ingestion and curation to support our existing, and rapidly expanding, health plan client base.\\nWhat you’ll do:\\nAs a data engineer, you will be responsible for the execution and management of inbound client and internal service-based data pipelines. This encompasses the development, management, and operation of our client data hubs, including data intake, data quality assessment/evaluation, data curation and enrichment processes. Our client data hubs consist of various health related data sources supporting Decision Point services including our AI/ML platform, analytics platform, and OPUS application suite. If this interests you, read on.\\nThe position:\\nDevelop and apply scalable data integration (ETL/ELT) processes (including ingestion, cleansing, curation, unification, etc.)\\nSupport various components of the data pipelines, including ingestion, validation, cleansing and curation\\nManage and ensure the success of ongoing data pipeline routines\\nCollaborate with our implementation team to assist with the identification and reconciliation of data anomalies\\nCreate and maintain documentation on data pipelines\\nEngage with our software engineering team to ensure precise data points per application specifications\\nProvide periodic support to our customer success team\\nSkills & experience:\\nBS / MS in Computer Science, Engineering, or applicable experience\\nExpertise with SQL, database design and data manipulation methodologies\\nExpertise with ETL/ELT and the development of automated validation and data pipelines\\nStrong data profiling and analytic skills; Ability to discover and highlight unique patterns/trends within data to identify and solve complex problems\\nKeen understanding of EDW, master data management and other database design principles\\nComfortable working with high volume data in a variety of formats\\nExperience with Pandas/Numpy in a production environment\\nExperience with CI/CD and version control tools: Git preferred\\nExperience working within hybrid cloud environment; AWS experience is a plus\\nExcellent verbal and written communication\\nExcellent listener and collaborator with senior leaders, peers, and staff\\nFamiliarity with data engineering and workflow management frameworks such as Airflow and dbt\\nFamiliarity with healthcare data is a plus\\nA little bit about Decision Point:\\nWe are a rapidly growing healthcare technology company changing the fundamentals of patient and provider engagement. For years, health plans have relied on descriptive data and reactive engagement. We empower our clients to understand and predict the whole member journey, enabling sustained improvements in member health outcomes and plan performance. We combine the latest, most practical technologies and a deep understanding of healthcare, bringing innovative, pragmatic solutions to an industry that touches us all.',\n",
       " 'Candidates Must be able to obtain/maintain a Public Trust (US Citizenship is required). This position is full-time w/benefits and 100% remote!\\nSanametrix, Inc. is a fast-growing small business headquartered in Arlington, VA. We are dedicated to providing federal agencies with legendary customer service and focused solutions for their business and technology needs. This role is responsible for building data pipelines for transferring data from source systems (virtual machines, Microsoft SQL Server) into AWS Cloud using AWS Native Tools. This resource has strong data modeling and scripting experience and has a strong knowledge of AWS Data Services.\\nResponsibilities:\\nPerform data processing, algorithm / structures, pipeline orchestration, data quality, governance, discovery.\\nWork with structured and unstructured data, blob data\\nDevelop and work with APIs\\nCollect and organize data using data warehousing technique and file storage technologies\\nPerform ELT and ETL processes\\nGather data requirements\\nDevelop and maintain scalable data pipelines and build out new API integrations to support continuing increases in data volume and complexity.\\nCollaborate with analytics and business teams to improve data models that feed business intelligence tools, increase data accessibility, and foster data-driven decision making across the organization.\\nImplement processes and systems to monitor data quality, to ensure production data accuracy, and ensure key stakeholder and business process access.\\nWrite unit/integration tests, contribute to engineering wiki, and documents.\\nPerform data analysis required to troubleshoot data related issues and assist in the resolution of data issues.\\nWork closely with a team of front-end and back-end engineers, product managers, and analysts.\\nDesign data integrations and dataquality framework based on established requirements.\\nMust be able to obtain a Public Trust (US Citizenship required to obtain Public Trust)\\nQualifications & Skills:\\nScripting\\nSQL & Scripting\\nPython\\nSpark\\nLinux / shell scripting\\nServices / Tools (six or more)\\nS3 Lambda\\nRedshift\\nLake Formation\\nGlue ETL\\nKinesis\\nDMS\\nGlue catalog/Crawlers\\nGit\\nJira\\nAirflow /Orchestration\\nEducation, Experience, and Licensing Requirements:\\nBS or MS degree in Computer Science or a related technical field\\n4+ years of Python or Java development experience\\n4+ years of SQL or NoSQL experience\\n4+ years of experience with schema design and dimensional data modeling\\nAbility in managing and communicating data warehouse plans to internal clients\\nExperience designing, building, and maintaining data processing systems\\nAWS Certified is preferred\\nJob Type: Full-time\\nPay: $110,000.00 - $120,000.00 per year\\nBenefits:\\n401(k)\\n401(k) matching\\nDental insurance\\nEmployee assistance program\\nFlexible schedule\\nFlexible spending account\\nHealth insurance\\nHealth savings account\\nLife insurance\\nPaid time off\\nProfessional development assistance\\nVision insurance\\nCompensation package:\\nYearly pay\\nExperience level:\\n5 years\\nSchedule:\\nMonday to Friday\\nWork Location: Remote',\n",
       " 'Changing Healthcare For Good\\nAt Angle Health, we believe the healthcare system should be accessible, transparent, and easy to navigate. As a digital-first, data-driven health plan, we are replacing legacy systems with modern infrastructure to deliver our members the care they need when they need it. If you want to build the future of healthcare, we\\'d love for you to join us.\\n\\nThe Role\\nAs a Data Engineer on the Strategy team at Angle Health, you will be part of an elite team of problem solvers tackling some of the hardest operational challenges across the business. You will be working closely with Technical Product Managers and other engineers in small teams embedded within functions across the company—from sales to operations to finance and more.\\n\\nIn partnership with operational stakeholders, your job will be to quickly understand business workflows, and design and implement technical, data-driven solutions to achieve the intended business outcomes. That may include architecting backend micro-services, building and maintaining robust data pipelines, standing up health metrics and automated alerts, and developing scalable technical solutions to support Angle Health\\'s day-to-day operations.\\n\\nEvery day will be a new learning experience in this role and may span discussing architecture with fellow engineers, wrangling large-scale data, coding a custom web app or micro-service, or speaking with customers and executives.\\n\\nThis role is ideal for entrepreneurial engineers, applied data scientists, and creative, intellectually curious thinkers that want to solve high-value problems in healthcare.\\n\\nThis position may be based in San Francisco, New York City, Salt Lake City, or Remote.\\n\\nWe are currently trialing various titles for the same role. Please consider the following posted roles as the same position: Deployed Engineer, Software Engineer, Business Operations, and Data Engineer. Please note these are the same, so if you have already applied for one position, there is no need to reapply for the others.\\nWhat We Value\\nA strong engineering background in computer science, software engineering, data science, mathematics, or similar technical field is required for this role\\nProficiency in programming languages (e.g. Python, SQL, Java, TypeScript/JavaScript, or similar) and data engineering frameworks\\nA highly analytical mindset and an eagerness to build technical solutions to complex business problems\\nHigh attention to detail and intellectual curiosity—you\\'re not satisfied with surface-level answers. You want to dive into the data, the \"how,\" and the \"why\" because \"the way it\\'s always been done\" is not always the way it should be done\\nLow ego—the outcome matters more than who gets the credit\\nDemonstrated ability to collaborate effectively in teams of technical and non-technical individuals\\nHighly organized with an ability to multitask, problem solve, and balance competing priorities in a rapidly changing environment\\nBecause We Value You:\\n\\nCompetitive compensation and stock options\\n100% company paid comprehensive health, vision & dental insurance for you and your dependents\\nSupplemental Life, AD&D and Short Term Disability coverage options\\nDiscretionary time off\\nOpportunity for rapid career progression\\nRelocation assistance (if relocation is required)\\n3 months of paid parental leave and flexible return to work policy (after 10 months of employment)\\nWork-from-home stipend for remote employees\\nCompany provided lunch for in-office employees\\n401(k) account\\nOther benefits coming soon!\\n\\nBacked by a team of world class investors, we are a healthcare startup on a mission to make our health system more effective, accessible, and affordable to everyone. From running large hospitals and health plans to serving on federal healthcare advisory boards to solving the world\\'s hardest problems at Palantir, our team has done it all. As part of this core group at Angle Health, you will have the right balance of support and autonomy to grow both personally and professionally and the opportunity to own large parts of the business and scale with the company.\\n\\nAngle Health is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. Angle Health is committed to working with and providing reasonable accommodations to applicants with physical and mental disabilities.',\n",
       " 'Chevron is accepting online applications for the position Wells Data Engineer through 03/08/2023 at 11:59 p.m. (CST).\\nChevron’s strategy is straight-forward: be a leader in efficient and lower carbon production of traditional energy, in high demand today and for decades to come, while growing lower carbon businesses that will be a bigger part of the future. To achieve these goals, we’ll build on the assets, experience, capabilities, and relationships we’ve developed over 140 years to incubate and grow new business.\\n\\nTechnology will play a crucial role in unlocking ever cleaner and more affordable sources of energy. Chevron is seeking innovative, technology professionals with a desire to thrive in the global digital environment and help us lead the global energy transition. An IT career at Chevron offers you the opportunity to work in a technical environment with a global reach. You’ll find that we make a business of investing in our people and encouraging your professional development through a learning culture and challenging on-the-job opportunities. We differentiate ourselves through the application of cutting-edge technology, and by taking a collaborative approach that includes in-house expertise, proprietary solutions, and strategic partnerships. We also offer flexible work schedules and very competitive benefits.\\n\\nJoin Chevron IT. Lend us your skills and enjoy a great career with Chevron.\\nWells Data Engineer responsible for multiple aspects of RBU Engineering Team data management and quality, automation, system connectivity, visualizations. This role is expanded through supporting performance related processes, benchmarking, and cost/metrics tracking in our Competitive Performance framework. The Data Engineer is also responsible for providing technical and analytical support in the area of data entry and standard templates for drilling, workover, intervention, and abandonments project management and execution.\\nThe role\\n\\nWells Data Engineer responsible for multiple aspects of RBU Engineering Team data management and quality, automation, system connectivity, visualizations. This role is expanded through supporting performance related processes, benchmarking, and cost/metrics tracking in our Competitive Performance framework. The Data Engineer is also responsible for providing technical and analytical support in the area of data entry and standard templates for drilling, workover, intervention, and abandonments project management and execution.\\nResponsibilities for this position may include but are not limited to:\\n\\nUnderstanding data systems architecture and management\\nData sources, data quality, and QA/QC mechanisms needed for systems of record\\nAdministrator level ownership of multiple data systems such as WellView, SharePoint, WellSafe systems, reporting systems, and data systems\\nWorking with multiple data systems to provide clear and impactful data visualizations and tracking for the Wells leadership team\\nWorking with the Wells Performance Engineer to support cost, metrics, and performance tracking processes\\nSupport WellView well/job creation, data quality review and support, field training, and regulatory data\\nUtilizing PowerBI, Spotfire, or Excel to collate cost tracking information in Wellview and Siteview\\nRequired to have strong written communication & excellent organizational skills.\\nRequired Qualifications:\\n\\nData science or engineering related training and related systems\\nAdvanced skills in Excel and other Microsoft Office Suite applications\\nGood communicator and able to collaborate effectively with multiple stakeholders (i.e. engineers, regulatory, operations, etc…)\\nPreferred Qualifications:\\n\\nExperience working with WellView, PowerBI and ArcGIS applications\\nExperience in supporting oil and gas well drilling, completion, workover, and asset retirement operations.\\nFlexible Working\\nChevron offers a complete package and provides career development opportunities to all employees. We do this through on-boarding, training and development, mentoring, volunteering opportunities and employee networking groups. We advocate work-life balance and offer employees access to various health and wellness programs.\\nWhat type of flex work does the position offer?\\nWe offer alternative work schedules including 9/80 (work 9-hour days, with every other Friday off)\\nWe offer a hybrid work model - work remotely from home 2-3 days a week\\n\\nRelocation & International Considerations\\nRelocation [ may / will not be] considered.\\nExpatriate assignments [ may / will not be ] considered.\\n\\nChevron regrets that it is unable to sponsor employment Visas or consider individuals on time-limited Visa status for this position.\\nWorking with us\\nChevron is one of the world’s leading integrated energy companies. We believe affordable, reliable and ever-cleaner energy is essential to achieving a more prosperous and sustainable world. Chevron produces crude oil and natural gas; manufactures transportation fuels, lubricants, petrochemicals and additives; and develops technologies that enhance our business and the industry. We are focused on lowering the carbon intensity in our operations and seeking to grow lower carbon businesses along with our traditional business lines. More information about Chevron is available at\\nwww.chevron.com\\n.\\nBenefits\\nChevron offers competitive compensation and benefits programs which includes, but is not limited to, variable pay, healthcare coverage, retirement plan, insurance, time off programs, training and development opportunities and a range of allowances connected to specific work situations. Details of such benefits and allowances are available at\\nhttps://hr2.chevron.com/\\n.\\n\\nThe compensation and reference to benefits for this role is listed on this posting in compliance with Colorado law. The selected candidate’s salary will be determined based on his or her skills, experience, and qualifications.\\n\\nRegulatory Disclosure for US Positions\\n\\nThe compensation and reference to benefits for this role is listed on this posting in compliance with applicable law. Please note that the compensation and benefits listed below are only applicable for U.S. payroll offers.\\nThe anticipated salary range for this position is $ 112,200 – $ 221,900 The selected candidate’s compensation will be determined based on their skills, experience, and qualifications.\\nChevron offers competitive compensation and benefits programs which includes, but is not limited to, variable pay, health care coverage, retirement plan, protection coverage, time off and leave programs, training and development opportunities and a range of allowances connected to specific work situations. Details are available at\\nhttp://hr2.chevron.com/\\n.\\nRegulatory Disclosure for US Positions:\\nChevron is an Equal Opportunity / Affirmative Action employer. Qualified applicants will receive consideration for employment without regard to race, color, religious creed, sex (including pregnancy, childbirth, breast-feeding and related medical conditions), sexual orientation, gender identity, gender expression, national origin or ancestry, age, mental or physical disability (including medical condition), military or veteran status, political preference, marital status, citizenship, genetic information or other status protected by law or regulation.\\nWe are committed to providing reasonable accommodations for qualified individuals with disabilities. If you need assistance or an accommodation, please email us at\\nemplymnt@chevron.com\\n.\\nChevron participates in E-Verify in certain locations as required by law.',\n",
       " 'Client: Agilisium\\nRole: Data Engineer\\nLocation: Chicago, IL – Hybrid role – Need to be onsite once/twice in a month.\\nType: Long term contract\\nJob Description\\nTitle: Data Engineer\\nWe are looking for a savvy Data Engineer to join our growing team. The hire will be responsible for expanding and optimizing our data and pipeline architecture and data flow and collection. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. In addition, the Data Engineer will collaborate and support software developers, implementation architecture, and data engineers on data initiatives and ensure optimal data delivery architecture is consistent throughout ongoing projects. In addition, they must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products.\\nResponsibilities:\\nBecause we work on a programmatic leading edge of many technologies, we need someone who is a creative problem solver, resourceful in getting things done, and can shift productively to or from working independently and collaboratively. This person would also take on the following responsibilities:\\nProcess unstructured data into a form suitable for analysis.\\nSupport the business with ad hoc data analysis and build reliable data pipelines.\\nImplementation of best practices and IT operations in mission-critical tighter SLA data\\npipelines using Airflow\\nQuery Engine Migration from Dremio to Redshift.\\nWe leverage: Multiple AWS Data & Analytic Services(e.g.,Glue, Kinesis, S3) , SQL (e.g.,\\nPostgreSQL, Redshift, Athena); NoSQL (e.g., DocumentDB, MongoDB); Kafka, Docker, Spark(AWS EMR and DataBricks), Airflow, Dremio, Qubole, etc.\\nWe use AWS extensively, so experience with AWS cloud and AWS Data & Analytics certification will help you hit the ground running.\\nBusiness Skills for Data Engineers:\\nCreative Problem-Solving: Approaching data organization challenges with a clear eye on what is important; employing the suitable approach/methods to make the maximum use of time and human resources.\\nEffective Collaboration: Carefully listening to management, data engineers, and architects to establish their needs.\\nIntellectual Curiosity: Exploring new territories and finding creative and pragmatic ways to solve data problems.\\nIndustry Knowledge: Understanding and perspective of how industry functions and how data can be collected, analyzed, and utilized for value creation; maintaining flexibility in the face of big data developments.\\nSkills and Qualifications:\\n3 to 5 years of real-world Data Engineering experience.\\nProgramming experience, ideally in Python and other data engineering languages like Scala\\nProgramming knowledge to clean structure and semi-structure datasets.\\nExperience processing large amounts of structured and unstructured data. Streaming data experience is a plus.\\nExperience building and optimizing ‘big data’ data pipelines, architectures, and data sets.\\nBackground in Linux\\nBuild the infrastructure required for optimal extraction, transformation, and loading of data\\nfrom various data sources using SQL and other cloud ‘big data’ technologies like DataBricks, Snowflake, Dremio, and Qubole.\\nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management.\\nA successful history of manipulating, processing, and extracting value from large disconnected datasets.\\nExperience creating a platform on which complex data pipelines are built using orchestration tools like Airflow, and Astronomer.\\nExperience with real-time sync between OLTP and OLAP using AWS technologies like real- time sync between AWS Aurora and AWS Redshift.\\nJob Type: Full-time\\nSalary: Up to $68.00 per hour\\nSchedule:\\n8 hour shift\\nWork Location: On the road\\nSpeak with the employer\\n+91 6305048577',\n",
       " 'Company Background:\\n\\nLiberty Source PBC combines state-of-the-art technology with a human overlay, enabling our clients to realize greater returns from their investments in artificial intelligence, machine learning, business intelligence and deep analytics platforms. We work with our clients’ Data Science teams, Data Operations staff, Data Quality functions, and other key stakeholders to refine and enhance the data that is vital to the success of their advanced technology initiatives. We are the Data Fitness experts.\\n\\nOur specialized recruiting mission focuses on the talents of veterans and families of active-duty military personnel to fulfill our brand promise of 100% U.S.-based operations and staff.\\n\\nFounded in 2014, Liberty Source PBC is based in Hampton, Virginia and is a Certified B Corporation.\\n\\nPosition Summary:\\n\\nAre you ready to put your data wrangling skills to the test and take your career to the next level? Liberty Source is seeking a Data Engineer to join our rapidly growing team. The right individual will expand and optimize our data and data pipeline architecture and build the systems that collect, manage, and convert raw – and often unstructured - data into usable information for our clients.\\n\\nThe ideal candidate can demonstrate experience in optimizing data flow, has collected and distributed data across teams and across companies, has built data systems (both large and small) from the ground up, and isn’t afraid to wade into the lake for some data cleansing and transformation. We’re looking for an individual who is self-directed and comfortable supporting the data needs of multiple teams, systems, and products, and excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. Beyond technical prowess, our new Data Engineer will have the soft skills for clearly communicating highly complex data ideas and issues to the leadership team.\\n\\nPosition Responsibilities:\\n\\nCreate and maintain optimal data pipeline architecture\\nBuild the infrastructure required for efficient ETL (extraction, transformation, and loading) of data from a wide variety of data sources using SQL, Hadoop and AWS ‘big data’ technologies.\\nBuild analytic views that utilize the data pipeline to provide actionable insights into operational efficiency and other key business performance metrics.\\nWork with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.\\nLeverage data to solve business problems, building and maintaining the infrastructure to answer questions and improve processes\\nHelp streamline our data science customer workflows, adding value to our product offerings and building out the customer lifecycle and retention models\\nWork closely with customer data science and business intelligence teams to develop data models and pipelines for research, reporting, and machine learning\\nBe an advocate for best practices and continued learning\\n\\nRequirements:\\n\\nBachelor’s degree in computer science, information technology, engineering, or related discipline.\\nFive or more years of experience in a Data Engineer role utilizing Python and data visualization/exploration tools.\\nAdvanced SQL capabilities and experience working with relational databases, query authoring (SQL) as well as hands-on experience with a variety of data constructs.\\nExperience building and optimizing ‘big data’ data pipelines, architectures, and data sets.\\nDemonstrated ability to work with unstructured data.\\nProven ability to build processes supporting data transformation, data structures, metadata, dependency.\\nProfessional certifications such as Cloudera Certified Professional (CCP), IBM Certified Data Engineer or Google’s Certified Professional is a plus.\\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\\nStrong project management and organizational skills.\\nExperience supporting and working with cross-functional, innovation-oriented teams in a dynamic environment.\\nGreat communication skills, especially for explaining technical concepts to nontechnical business leaders.\\n\\nBenefits:\\n\\nPaid Time Off (PTO) and 10 paid holidays\\nMedical, dental, vision, life insurance, and other ancillary benefits\\n401k Plan\\n\\nA pre-employment background check is required.\\n\\nTo learn more about our business, please visit our website at https://liberty-source.com/',\n",
       " \"Company Description\\nGovini is the leading Commercial Data company in the Defense Technology space. Built by Govini, Ark.ai is used at scale across the National Security sector of the U.S. federal government. This platform enables government analysts, program managers, and decision-makers to gain unprecedented visibility into the Companies, Capabilities, and Capital in National Security to solve challenges pertaining to Acquisition, Foreign Influence & Adversarial Capital, Nuclear Modernization, Procurement, Science & Technology, and Supply Chain. Govini has offices in Arlington, Virginia, San Francisco, California, and Pittsburgh, Pennsylvania.\\n\\nJob Description\\nWe are seeking an exceptional and experienced data engineer who shares our passion and obsession with quality. You'll be a core member of our product and engineering team dedicated to helping our clients replace time-consuming, manual processes to reach informed real-time decisions about government markets, competitors, and agency relationships.\\n\\nWe need a skilled and dedicated data nerd to join our team to lead us in uncovering truth and meaning in data. You must be a hands-on engineer with a strong understanding of both data management and governance standards. You must also have strong interpersonal skills to work cross-functionally across internal teams as well as directly with end users and Govini platform SMEs.\\n\\nIn order to do this job well, you must be a curious and eager problem solver with a hunger for delivering high-quality data solutions. You have a passion for great work and nothing less than your best will do. You share our intolerance of mediocrity. You’re uber-smart, challenged by figuring things out and producing simple solutions to complex problems. Knowing there are always multiple answers to a problem, you know how to engage in a constructive dialogue to find the best path forward. You’re scrappy. We like scrappy.\\n\\nThis role is a full-time position located in PIttsburgh, PA.\\n\\nScope of Responsibilities\\nDefine and lead Govini's data lifecycle strategy across data acquisition, data ingestion, data cleansing, normalization and linkage.\\nEnsure key entities within datasets are identified, resolved and linked to existing entities within the current master data repository.\\nApply various techniques to produce solutions to large-scale optimization problems, including data pre-processing, indexing, blocking, field and record comparison and classification.\\nImprove data sharing, increase data repurposing and improve cost efficiency associated with data management efforts.\\nBuild best practices that help with chain of custody of data so it can be easily traced back to the source for accuracy and consistency.\\nWork across functional teams to understand advanced statistical, machine learning, and text processing models and incorporate them into Govini’s existing data engineering infrastructure.\\nPerform exploratory data analyses, generate and test working hypotheses, prepare and analyze historical data and identify patterns.\\nWork directly with users as well as SMEs to establish, create and populate optimal data architectures and structures, as well as articulate techniques and results using non-technical language.\\nQualifications\\nUS Citizenship is required\\n\\nRequired Skills\\nBachelor's degree in Computer Science, Mathematics or related technical field\\n3-5 years experience with programmatically transforming data\\nExperience with RDBMS\\nAdvanced SQL programming skills\\nProficient usage of common data formats such as CSV, XML, and JSON\\nRequires strong analytical ability and attention to detail\\nAbility to work independently with little supervision\\nA burning desire to tackle hard problems and create sustainable solutions\\n\\nDesired Skills\\nExperience using Amazon Web Services\\nExperience in or exposure to the nuances of a startup or other entrepreneurial environment\\nWorking knowledge with large (multiple terabytes) amounts of data\\nWe firmly believe that past performance is the best indicator of future performance. If you thrive in ambiguity, are a self-starter, and care about solving technical problems in the national security domain, we’re eager to hear from you.\\n\\nGovini is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.\",\n",
       " 'ConnectiveRx is a leading, technology-enabled healthcare services company. We work strategically with hundreds of biopharmaceutical manufacturers to help commercialize and maximize the benefits of specialty and branded medications. Our mission is to simplify how patients get on and stay on therapy. We fulfill our mission by providing our customers with innovative services such as patient and provider messaging, the design and operation of copay, vouchers and patient affordability programs, and hub services, all of which accelerate speed-to-therapy and help improve outcomes for manufacturers, healthcare providers and patients.\\n\\nConnectiveRx was formed in 2015 by bringing together the industry-leading business of PSKW, PDR/LDM, Careform (2017) and The Macaluso Group (2018) to advance our technology-driven expertise in providing state-of-the-art commercialization solutions. To learn more about our company, visit ConnectiveRx.com\\n\\nJob Description\\n\\nWhat you will do:\\nLooking for a seasoned Senior Data Engineer to help us continue to build out our new Enterprise Data Platform. This person must have a strong understanding and demonstrated experience with data streaming architectures that leverage microservice & message-oriented integration patterns and practices within AWS cloud native technologies. This person will help to scale our data ingestion pipelines which are at the core of our Enterprise Data Platform which supports our client reporting as well as our internal analytics & operational teams.\\n\\nThe successful candidate will:\\nWork with senior leadership, architects, engineers, data analysts, product managers and cloud infrastructure teams to deliver a new features and capabilities.\\nWrite clean, robust, and well-thought-out code with an emphasis on quality, performance, scalability, and maintainability.\\nDemonstrate strong end to end ownership & craftsmanship - analysis, design, code, test, debug, and deploy\\nYour ability to traverse the full stack within AWS server-less technologies will be an asset to us as we evaluate the tradeoffs inherent in software engineering. You have the product driven development mindset and can work closely with BA’s and Product teams to breakdown requirements and translate business workflows into scalable technical solutions.\\n\\nWhat we’d like from you:\\nStrong Python & strong SQL\\nExtensive relational DB experience (Redshift, SQL Server, PostgresSQL) with exposure to document DBs such as DynamoDB. ElasticSearch.\\nExperience with designing solutions that run in AWS cloud technologies (Lambda, ECS, DynamoDB etc), docker containers\\nMessage oriented architectures, patterns and tools, CQRS, event streaming, Kafka, SQS\\nChange data capture concepts, Database Triggers, AWS DMS\\nData lake concepts, data catalogs, meta data etc\\nCICD Pipelines\\nEvent store processing, data validation, operational logging via AWS Cloud Watch\\nWhy work with us?\\nExcellent company culture, fun events, and volunteer opportunities\\nCompetitive benefits (medical, dental, vision & more)\\n401k package with dollar-for-dollar match-up\\nGenerous PTO and paid holidays days offered\\nOpportunities to grow professionally and personally\\nTeam-oriented atmosphere\\n#LI-BJ1\\n\\nEqual Opportunity Employer: This employer (hereafter the Company) is an equal opportunity employer and does not discriminate in recruitment, hiring, training, promotion, or other employment policies on the basis of age, race, sex, color, religion, national origin, disability, veteran status, genetic information, or any other basis that is prohibited by federal, state, or local law. No question in this application is intended to secure information to be used for such discrimination. In addition, the Company makes reasonable accommodation to the needs of disabled applicants and employees, so long as this does not create an undue hardship on the Company or threaten the health or safety of others at work. This application will be given every consideration, but its receipt does not imply that the applicant will be employed.',\n",
       " 'Data Engineer\\nDMI Software, the San Francisco branch of Dovenmuehle Mortgage, Inc, the leading sub-servicer of mortgage loans in the United States, is looking for a talented and enthusiastic data engineer. We work exclusively in Software Development. Our growing office offers the feel of a startup with the backing and security of a long-established company. We aspire to create elegantly scalable products while fostering the continued growth of each team member. The ideal candidate will have 5+ years relevant experience, including Hadoop Ecosystem or similar, and with a scripting language.\\n\\nHere we believe that the best software is created by an eclectic set of voices, and we strive to nurture an environment rich in differing opinion, belief, and background. Only in this way can we develop revolutionary products capable of meeting the varied needs of an increasingly interconnected world.\\n\\nWhat You’ll Be Doing:\\nDesign, implement, automate, and maintain large-scale enterprise ETL processes\\nEvolve data model and schema based on business and engineering needs\\nOversee systems tracking data quality and consistency\\nCollaborate with data analysts to bridge business goals with data delivery\\n\\nRequirements:\\n5+ years data engineering experience\\nHighly experienced using Python, SQL and and Hadoop\\nExcellent communication, analytical and problem-solving skills\\nKeen attention to detail while keeping an eye toward the big picture\\nYou are comfortable with the nuts and bolts of systems programming in the Linux environment (shell/bash scripting)\\nExperience working in an Agile environment\\nExcellent presentation and communication skills\\nExperience profiling, debugging, tracing, and or parallelizing/optimizing Python code\\nIdeal candidate is one who can adapt and adopt to our existing architectures while also making impactful improvements and suggestions.\\n\\nJob Type: Full-time',\n",
       " \"Data Engineer\\nPay Scale: $73 p/h, W2, no benefits\\nDuration: Full Time/Contract\\nStatus: US Citizen or Green Card only\\nReports To: Project Manager\\nWorking Hours: Normal business hours\\nWork Location: Onsite, Customer Premises, Vancouver, WA 98683\\n\\nSummary/Objective:\\nGlow Networks is a telecommunication staffing and consulting company based in Dallas, TX. We are seeking a Data Engineer, to work in Vancouver, WA 98683 location.\\n\\nThe data engineering role is a team member that will help enhance and maintain the Instant Ink Business Intelligence system. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to implement data engineering pipelines to build solution to help provide trusted and reliable data to customers.\\n\\nResponsibilities\\nDesign and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem.\\nAnalyzes design and determines coding, programming, and integration activities required based on general objectives.\\nReviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards\\nWrites and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture.\\nCollaborates and communicates with project team regarding project progress and issue resolution.\\nWorks with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.\\nCollaborates with peers, engineers, data scientists and project team.\\nTypically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.\\nWhat you bring :\\nBachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.\\n6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.\\n3+ years of experience with Cloud based DW such as Redshift, Snowflake etc.\\n3+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Hive & Delta Lake)\\n3+ years experience in Workflow orchestration tools such as Airflow etc.\\n3+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc.\\nLeverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc.\\nExperience with container management frameworks such as Docker, Kubernetes, ECR etc.\\n3+ year’s working with multiple Big Data file formats (Parquet, Avro, Delta Lake)\\nExperience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc.\\nStrong experience in coding languages like Python, Scala & Java\\nKnowledge and Skills\\nFluent in relational based systems and writing complex SQL.\\nFluent in complex, distributed and massively parallel systems.\\nStrong analytical and problem-solving skills with ability to represent complex algorithms in software.\\nStrong understanding of database technologies and management systems.\\nStrong understanding of data structures and algorithms\\nDatabase architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.\\nStrong analytical and problem-solving skills.\\nNice to Have\\nExperience with transformation tools such as dbt.\\nHave experience in building realtime streaming data pipelines\\nExperience in pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming etc\\n\\n.EEO Statement: Glow Networks. provides equal opportunity in all of our employment practices to all qualified employees and applicants without regard race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age, disability or genetic information and other characteristics that are protected by applicable law.\\n\\nOther Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. This description reflects management’s assignment of essential functions, it does not proscribe or restrict the tasks that may be assigned. Duties, responsibilities, and activities may change at any time with or without notice.\",\n",
       " \"Data Engineer\\nWhat does a successful Data Engineer do at Fiserv?\\nAs an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.\\nWhat you will do:\\nWork with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.\\nDevelop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.\\nDevelop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS\\nYou will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.\\nAdditionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.\\n\\nWhat you will need to have:\\n4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.\\nHands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.\\nPossess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML\\nExperience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.\\nStrong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.\\nTechnical leadership in developing data solutions and building frameworks\\nHands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)\\nExperience deploying code on containers\\nExperience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search\\nWhat would be great to have:\\n5+ years' experience building large scale big data applications development\\nExperience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)\\nExperience working with PCI Data and working with data scientists is a plus\\nExposure to Talend Big Data edition and solutions a strong plus\\nExperience with Cloud platforms (Google Cloud, AWS, Azure)\\nExperience in Design and architecture review in the Banking, Financial domain.\",\n",
       " 'Data Engineer (Hybrid):\\nWork with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.\\nQualifications:\\nChange agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.\\nBachelor’s degree in Computer Science, MIS, IS, or Engineering preferred',\n",
       " 'Data Engineer (Hybrid):\\nWork with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.\\nQualifications:\\nChange agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.\\nBachelor’s degree in Computer Science, MIS, IS, or Engineering preferred\\nEmotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.\\n3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.\\nKnowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.\\nCloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.\\nExperience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)\\nExperience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.\\nExperience with data migration, transformation, and refactoring to a cloud-based environment\\nExperienced in the use of standard ETL and ELT tools and techniques.\\n3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.\\n3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.\\nExpertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities\\nWorking knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.\\nKnowledge of security controls around authentication, authorizations, encryptions, and certificates\\nVersed in data visualization tools such as Microsoft Power BI, Tableau.\\nHas a working knowledge of various data structures and the ability to extract data from various data sources.\\nStrong verbal and written communication skills.\\nUnderstanding of cloud scaling technologies and the economic impact of doing so.\\nVersed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.\\nAbility to work in a fast-paced, rapidly changing environment.\\nJob Types: Contract, Permanent\\nSalary: $50.00 - $65.00 per hour\\nBenefits:\\n401(k)\\n401(k) matching\\nHealth insurance\\nTuition reimbursement\\nCompensation package:\\nPerformance bonus\\nSigning bonus\\nExperience level:\\n2 years\\n3 years\\n4 years\\n5 years\\nSchedule:\\n8 hour shift\\nMonday to Friday\\nApplication Question(s):\\nAre you willing to relocate at Milwaukee, WI?\\nAre you comfortable working on W2, full time role?\\nWork Location: One location\\nSpeak with the employer\\n+91 8135358173',\n",
       " \"Data Engineer 925\\nVisa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C\\nVisa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors\\nCandidates must be located in the United States\\nDuration: 6 month with potential to go long term or permanent\\nLocation: 100% remote position\\nInterview: Phone and Teams\\nRequired:\\nThe primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.\\nCandidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.\\nDatabricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.\\nDATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past\\nSSIS would be useful to have some SSIS experience as we're still building some of our extracts from here\\nThanks,\\nJay Kernes\\nCertec Consulting, inc\\n847-253-8968, Fax 888-523-7832\\nWe are certified as a Women's Business Enterprise (WBE)\",\n",
       " \"Data Engineer- Remote\\nRole: Data and Analytics is an evolving space which includes more software engineering, distributed systems, and cloud skills.\\nWIll develop, maintain, and enhance the data platform capabilities in an open and collaborative environment to build the central platform.\\nWill collaborate with internal data customers across IT and the Business to minimize the time from idea inception to analytical insight.\\nJob responsibilities will include: contributing to data infrastructure design efforts and collaborating with other platforms to integrate infrastructure into the client's systems and testing the feasibility and effectiveness of various technology options; supporting complex tools and solutions to manage orchestration, data pipelines, and infrastructure as code solutions the Data Engineering team builds.\\nRequired skills:\\nProven experience in designing, building, and supporting complex data pipelines using a variety of traditional and non-traditional data sources.\\nVersion Control and associated best practices\\nAdvanced programming experience in programming languages used in analytics and data science (e.g. Python, Java, Scala). Comfortable with Linux environments and shell scripting.\\nExperience with Cloud-based infrastructures (AWS)\\nExperience working with SQL/NoSQL\\nExperience utilizing data pipeline orchestration frameworks.\\nVerbal Communication\\nPreferred skills and experiences:\\nAnalysis\\nAPI Development\\nCI/CD\\nCreating Real Time or Streaming Systems\\nData Governance\\nData Lineage\\nData Metadata\\nData Testing\\nDistributed Databases\\nDomain Knowledge\\nSchema\\nSnowflake\\nVisual Communication\\nEDUCATION AND/OR EXPERIENCE REQUIRED:\\nEducation and/or experiences listed below are the minimum requirements for job entry.\\nBachelor's Degree or higher in an Information Technology discipline or related field of study and minimum of two years of work experience designing, programming, and supporting software programs or applications.\\nIn lieu of degree, minimum of four years related work experience designing, programming, and supporting software programs or applications may be accepted.\\nJob Types: Full-time, Contract\\nPay: $90.00 - $120.00 per hour\\nBenefits:\\nHealth insurance\\nExperience level:\\n6 years\\nSchedule:\\n8 hour shift\\nMonday to Friday\\nWork Location: Remote\",\n",
       " \"Description of Work:\\nTechnical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.\\nCollaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.\\nProvide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.\\nLead complex discussions and engagements that may involve multiple project teams from client.\\nBasic Qualifications: Minimum knowledge, skills, abilities needed.\\nBachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.\\nTechnical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.\\nStrong working experience with SQL and other databases (i.e., DB2 and Oracle)\\nStrong working experience with BI Development with Tableau and other BI tools\\nStrong working experience with ETL development\\nStrong oral and written communication skills and ability to communicate with all levels within the organization\\nStrong data analysis and problem-solving skills\\nStrong interpersonal skills with ability to collaborate with others effectively and efficiently.\\nAbility to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.\\nMust be able to obtain public trust clearance.\\nPreferred Qualifications: Candidates with these skills will be given preferential consideration.\\nDatabase development skills\\nFamiliarity with data modeling (Erwin data modeler)\\nEnterprise Data Catalog experience (Informatica)\",\n",
       " \"Details:\\nJOB DESCRIPTION:\\nWe are working with a client in need of a talented Data Engineer to support our client located in King of Prussia, PA. This contractor will be working on a high-level corporate initiative to build an enhanced customer service model for a leading utilities organization. This contractor will be working with our client on Data Engineering topics, including creating relevant data models, developing powerful data pipelines, exposing them through various mechanisms including APIs, and using data visualization tools to efficiently present data.\\nResponsibilities:\\n· Partner with our client’s leadership teams, engineers, program managers and data analysts to understand data needs.\\n· Design, build and launch efficient and reliable data pipelines transforming data into useful report ready datasets.\\n· Use your data and analytics experience to ‘see what’s missing,’ identifying and addressing data gaps, build monitors to detect data quality issues and partner to establish a self-serve environment.\\n· Broad range of partners equates to a broad range of projects and deliverables, including ML Models, datasets, measurements, services, tools and process.\\n· Leverage data and business principles to automate data flow, detect business exceptions, build diagnostic capabilities, and improve both business and data knowledge base.\\n· Build data expertise and own data quality for your areas.\\nQUALIFICATIONS:\\n· At least 4+ years' of advanced SQL experience (including at least one SQL DBMS and one no SQL).\\n· 4+ years' of Python development experience.\\n· 3+ years' experience with Data Modeling.\\n· Experience analyzing data to discover opportunities and address gaps.\\n· Experience working with cloud or on-prem Big Data/MPP analytics platform (i.e. SnowFlake, Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).\\n· BSc/BA in Data Science, Computer Science, Engineering.\\n· Familiarity with SAP order generation and invoicing modules\\nJob Types: Full-time, Contract\\nSalary: $75.00 - $85.00 per hour\\nExperience level:\\n10 years\\nSchedule:\\n8 hour shift\\nDay shift\\nMonday to Friday\\nExperience:\\nadvanced SQL (including at least one SQL DBMS, one no SQL: 4 years (Preferred)\\nPython development: 4 years (Preferred)\\nData Modeling: 3 years (Preferred)\\nanalyzing data to discover opportunities and address gaps: 1 year (Preferred)\\ncloud or on-prem Big Data/MPP analytics platform: 1 year (Preferred)\\nSnowFlake, Netezza, Teradata, AWS Redshift, Google BigQuery,: 1 year (Preferred)\\nAzure Data Warehouse: 1 year (Preferred)\\nSAP order generation and invoicing modules: 1 year (Preferred)\\nWork Location: One location\",\n",
       " 'EDI/Data Engineer:\\n6-month contract to hire. Must be in Tulsa/OKC and able to be on site weekly.\\nNeeded: Azure Data Factory; Microsoft SQL; Oracle; Experience with big data\\nNice to Haves: Experience monitoring data in and out; data warehouse provisioning to feed analytical requirements\\n________________________________________________________________\\nJOB SUMMARY:\\nThe Data Engineer will be responsible for expanding, optimizing and monitoring our data and data pipeline architecture, as well as optimizing data flow and collection across organizational teams. The Data Engineer will support our software engineers, database architects and data analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.\\nKEY RESPONSIBILITIES:\\nCreate and maintain optimal data pipeline architecture to support our next generation of products and data initiatives.\\nAssemble large, complex data sets that meet functional business requirements.\\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.\\nExperience in the development of SSIS, ETL and other standardized data management tools.\\nBuild analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.\\nPerforms other duties as required.\\nQUALIFICATIONS:\\nBuild processes supporting data transformation, data structures, metadata, dependency and workload management.\\nStrong project management and organizational skills.\\nAbility to work independently, handle multiple tasks and projects simultaneously.\\nEDUCATION/EXPERIENCE:\\nBachelors degree or equivalent experience required.\\nProject management skills preferred.\\nWillingness to work in a high-tech, continually evolving, innovative environment.\\nJob Types: Full-time, Contract\\nPay: $50.00 - $70.00 per hour\\nBenefits:\\n401(k)\\nDental insurance\\nFlexible schedule\\nHealth insurance\\nPaid time off\\nVision insurance\\nExperience level:\\n3 years\\n4 years\\n5 years\\nSchedule:\\n8 hour shift\\nMonday to Friday\\nAbility to commute/relocate:\\nTulsa, OK 74105: Reliably commute or planning to relocate before starting work (Required)\\nExperience:\\nSQL: 1 year (Preferred)\\nData warehouse: 1 year (Preferred)\\nWork Location: Hybrid remote in Tulsa, OK 74105',\n",
       " \"Empower Federal Credit Union offers excellent benefits including:\\nMedical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!\\n\\nPlease note: All candidates will be subject to a credit check to determine employment eligibility.\\n\\nRole:\\nDo you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.\\n\\nThe Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.\\n\\nThe Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.\\n\\nThe Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.\\n\\nThe Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.\\n\\nFamiliarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.\\n\\nReporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.\\n\\nThis is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.\\n\\nEssential Functions & Responsibilities:\\nE\\n50%\\n\\nBuild, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.\\n\\nE\\n20%\\n\\nCollaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.\\n\\nE 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.\\nE\\n5%\\n\\nShare your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.\\n\\nE 5% Other duties, as assigned.\\n\\nPerformance Measurements:\\n1. See Dayforce (HRIS) for Performance Goals upon hire.\\n\\nKnowledge and Skills:\\nExperience\\nEight to ten years of similar or related experience including:\\nProven experience in building/operating/maintaining fault tolerant and scalable data processing integration.\\nWell versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services\\nFamiliar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.\\nExperience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.\\nEducation\\n(1)\\u2003A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).\\n(2)\\u2003Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.\\n\\nInterpersonal Skills\\nThis role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.\\n\\nOther Skills\\nA passion for details, planning, workflow, systems, and process.\\nHighly efficient and strong prioritization skills and the ability to manage many projects concurrently.\\nExtremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.\\nProblem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.\\nPartnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.\\nDesire for innovation with the ability to initiate ideas and lead initiatives from inception.\\nExcellent verbal and written communicator.\\nAdept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.\\nAbility to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully\\nFinancial Institution experience a plus.\\nExperience in previous regulated industry a plus.\\nPhysical Requirements\\nThe physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.\\n\\nWork Environment\\nThe work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.\\n\\nThis Job Description is not a complete statement of all duties and responsibilities comprising the position.\\n\\nEmpower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.\\n\\nEmpower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.\\n\\nEmpower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:\\n\\na) Fax: 315-455-5423\\nb) US Mail: 1 Member Way Syracuse, NY 13212\\nc) Phone: 800-462-5000\",\n",
       " \"Enterprise Knowledge (EK) is hiring for a full-time Data Engineer to join our growing Data and Information Management Practice. This individual will support dynamic and fast-paced technical delivery projects for a range of commercial and federal clients that provide the opportunity to learn fast and grow quickly. We are seeking a well-organized, curious, and quick learner who will tackle diverse challenges with creative solutions. The right candidate will have a passion for working with diverse data types and applying new methods and approaches to data challenges including managing data at scale using popular data manipulation frameworks backed by scalable cloud architecture.\\nThe data engineer will be part of a team working on cutting-edge projects developing an orchestrated data solution to integrate and transform source data to build a Knowledge Graph that powers advanced search and Artificial Intelligence (AI) solutions.\\nAs an EK team member, you will join a fast-growing company that is committed to diversity and inclusion, have the opportunity to work in a collaborative workplace, take advantage of our unique benefits, and help build our innovative culture. Don’t just take our word for it – we were recently featured as one of Inc. Magazine’s 2022 Best Workplaces!\\nRequired Skills and Qualifications:\\nExperience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr.\\nAble to carefully identify and understand client needs in order to design practical solutions that surpass their expectations.\\nMinimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs.\\nExperience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions.\\nExperience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc., and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS.\\nProficiency in at least one database query language such as SQL, SPARQL, or Gremlin.\\nProven experience working directly with clients, providing briefings, facilitating meetings, and presenting work products.\\nAll of our employees are required to be fully vaccinated against COVID-19 regardless of the employee's location or work arrangement (e.g., telework, remote work, etc.), subject to such exceptions as required by law. If you are hired, we will require you to prove that you have received the COVID-19 vaccine, unless you have received a medical or religious exemption.\\nPreferred Skills and Qualifications:\\nExperience with the graph data structure such as Knowledge Graphs or other graph analytics use cases.\\nWorking knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross-Validation.\\nImplementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask.\\nExperience building, deploying and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as Code (IaC) templating.\\n“We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”\",\n",
       " 'FULL STACK DATA ENGINEER\\nUS CITIZEN OR GREEN CARD ONLY\\n5+ YEARS OF EXPERIENCE REQUIRED\\nJob Type: Full-time\\nPay: $55.00 - $75.00 per hour\\nBenefits:\\n401(k)\\nDental insurance\\nHealth insurance\\nPaid time off\\nTuition reimbursement\\nVision insurance\\nSchedule:\\nMonday to Friday\\nApplication Question(s):\\nwhat is your citizenship status?\\nwhat is your location?\\nplease send your resume to nicayla@nntechus.com for evaluation\\nwhat is your contact number and email?\\nWork Location: Remote',\n",
       " 'FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.\\nKey Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka\\nHere are some tasks that you could do day to day:\\nDesign and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.\\nPublish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.\\nExplore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.\\nTake part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.\\nWork with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.\\nExperience in Business Rule management systems like Drools will also come in handy.',\n",
       " 'FlexIT client is looking for a Data Engineer 12 months contract in Beaverton, Oregon.\\nLooking for local candidates to work on site.\\nTop skills: Python, SQL , AWS, Spark',\n",
       " 'GENERAL SUMMARY:\\nManufacturers bank is looking for a Data Engineer with strong software engineering backgrounds, a \"platform engineering mindset\" and deep passion for data. This Engineer will help to shape the technical direction of the data domain within the organization. The Engineer will also partner closely with internal business units and external vendors to develop a high performance, reliable and scalable data environment for the bank.\\n\\nPRINCIPAL DUTIES & RESPONSIBILITIES:\\nDevelop and automate high-performance data processing systems and visualization to ensure reliability and meet critical business requirements.\\nDesign, develop, implement, document and support Data & Analytics systems which adhere to the industry best practices.\\nLead data engineering projects, and overall strategy for data governance, security, privacy, quality, and retention.\\nTriage and resolve issues reported by Risk, Operations, and other business stakeholders.\\nMentor junior engineers and continue promoting data engineering and analytics tooling & standards.',\n",
       " \"General Information\\nReq #\\nWD00037891\\nCareer area:\\nHardware Engineering\\nCountry/Region:\\nUnited States of America\\nState:\\nNorth Carolina\\nCity:\\nMorrisville\\nDate:\\nWednesday, January 18, 2023\\nWorking time:\\nFull-time\\nAdditional Locations:\\nMorrisville - North Carolina - United States of America\\nWhy Work at Lenovo\\nHere at Lenovo, we believe in smarter technology that builds a brighter, more sustainable and inclusive future for our customers, colleagues, communities, and the planet.\\n\\nAnd we go big. No, not big—huge.\\n\\nWe’re not just a US$70 billion revenue Fortune Global 500 company, we’re one of Fortune’s Most Admired. We’re transforming the world through intelligent transformation, offering the world’s most complete portfolio of smart devices, infrastructure, and solutions. With more than 71,500 employees doing business in 180 markets, we help millions—not just the select few—experience our version of a smarter future.\\n\\nThe one thing that’s missing? Well… you...\\nDescription and Requirements\\nLenovo’s Infrastructure Solutions Group (ISG) is seeking a qualified candidate to join our global tools, data, and automation development team. This team designs, develops, deploys, and maintains software applications for productivity and automation solutions.\\nWhat You'll Do\\nThe position will develop automation to drive efficiency and effectiveness, automate manual tasks, and extend existing automation platforms for others to build on. Additionally, this role will provide Technical Leadership to a worldwide team and utilize problem solving skills to provide solutions based on data for worldwide product assurance issues. Responsibilities Include:\\nWork with team to develop data strategies: data model, tools, storage, parsing, etc.\\nDefine the physical components in data.\\nDefine the configuration of a collection of components over time in data.\\nDefine characteristics of a specific configuration over time in data.\\nIntegrate with existing business data\\nCoordinate with functional teams within Lenovo to develop and automate data pipelines to continuously supply our analysts with clean and reliable data.\\nWork with to existing team to integrate, adapt, or identify new tools to efficiently collect, clean, prepare, and store data for analysis\\n\\n\\nBasic Qualifications:\\nBachelor’s degree in Computer Science, Mathematics, Engineering, or in a related field\\n4+ years’ experience with object-oriented/object function scripting languages: Python, Scala, etc\\n4+ years’ experience building processes supporting data transformation, data structures, metadata, dependency and workload management\\n4+ years’ experience in data schema, data pipeline design and database management\\n4+ years’ experience in optimizing data pipelines, architectures and data sets Fluency in structured and unstructured data and management through modern data transformation methodologies\\n\\n\\nPreferred Qualifications:\\n4+ years’ experience with designing and managing data in modern ETL architect like Spark, Kafka, Hadoop, Snowflakes\\nExperience with cloud service environment\\nWorking experience with NoSQL\\nExperience with Power BI ETL pipeline\\nA successful history of manipulating, processing and extracting value from large disconnected datasets.\\nStrong analytical, problem solving, verbal and written communication skills\\n\\n\\n\\nThis position must sit in Morrisville, NC.\\nWe are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any federal, state, or local protected class.\\nLenovo adopted a COVID-19 Vaccination Policy for US-based employees. As a condition of employment, employees must adhere to Lenovo’s US Vaccination Policy and be fully vaccinated against COVID-19, subject to any applicable accommodations. To be fully vaccinated means individuals must receive the full series of a vaccine either approved by the FDA or WHO and listed by the CDC (e.g. two dose of the Moderna, AstraZeneca or Pfizer-BioNTech vaccines; or one dose of the Johnson & Johnson vaccine). This applies to all US-based employees, contractors and interns, regardless of work location. As a condition of employment, you must provide proof that you are fully vaccinated or follow Lenovo’s accommodation process.\\nTO BE DELETED - Multiple Cities (OLD)\\nMorrisville - North Carolina - United States of America\\nMultiple Countries (Posting Locations)\\nUnited States of America\\nMultiple States (Posting Locations)\\nNorth Carolina\\nMultiple Cities (Posting Locations)\\nMorrisville - North Carolina - United States of America\",\n",
       " \"Great Opportunity At Orange County's Credit Union\\nMust reside in the state of CA, AZ, NV or TX.\\nAre you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk.\\nOrange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture.\\nOur team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential.\\nAre you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY!\\nMore about Orange County's Credit Union:\\nWorkplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace.\\nAs a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service.\\nPutting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference!\\nOverview:\\nAre you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization.\\nEssential Functions:\\nCollaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making.\\nTranslate business requirements to technical solutions by applying technical knowledge and strong business acumen.\\nSolve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services.\\nDevelop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform.\\nDesign, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc).\\nTechnical Must Haves for this Role:\\n5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment.\\n3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases).\\n3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.).\\n2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores.\\n2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS).\\nExperience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.\\n\\nThe targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.\\n\\nWe perform thorough background checks and credit checks. EOE.\",\n",
       " 'Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!\\nThe role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.\\nWe need to see skills, Python, CI/CD, and either Kubernetes and/or Docker\\nAs a Data Engineer , You will deliver on below missions:\\nDeploy , covering all aspects from data discovery\\nWork closely with product development team to feed your experience from the frontline back to the\\ndevelopment road map, and with our climate experts to bring leading market advice to our customers\\nWHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)\\nWe’re looking for exceptional talent, with strong background in data engineer. You would typically have:\\n2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines\\nDeep knowledge of Python\\nExperience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions\\nExperience working in customer facing services, ideally in sustainability & greenhouse gas accounting\\ncompany.\\nSelf - driven, problem solving attitude\\nProven ability of database management to come up with better ways to wrangle & structure complex\\nsources of data\\nAn experience in a fast - growing environment , ideally working within SaaS company in critical product scale -\\nup phase\\nExperience working in an Agile development environment\\nWe need to see skills, Python, CI/CD, and either Kubernetes and/or Docker',\n",
       " 'Hadoop Big Data Engineer\\nRemote\\nMust Haves • Strong experience in Big Data, Cloudera Distribution 7.x, Cloud migration, RDBMS • Strong Experience with Amazon EMR/Databricks/Cloudera CDP • 4-5 experience building data pipelines using Hadoop components Sqoop, Hive, Solr, MR, Impala, Spark, Spark SQL., HBase. • 4-5 years of programming experience in Python, Java and Scala is must. • Strong experience with REST API development using Python frameworks (Django, Flask etc.) • Micro Services/Web service development experience using Spring framework is highly desirable.\\nJob Types: Full-time, Contract\\nPay: $55.00 - $65.00 per hour\\nSchedule:\\nMonday to Friday\\nWork Location: Remote',\n",
       " 'Here at Crisp, we value the strength in teamwork, and strongly believe that it’s the key to Crisp’s success. By bringing together bright, motivated creators wherever they live and work, we leverage diverse experiences and backgrounds to understand the challenges facing our food system and solve them together. Come join us, and help build the type of business you’d like to be a part of.\\nCrisp is a socially conscious, distributed team. We give you the opportunity to solve challenges in the global food industry while living where you’re most comfortable and working in areas where you can help foster and grow the community that you are a part of. We believe in transparency, diversity, and merit, and foster a culture of empowerment, personal impact and career growth.\\nAs a Data Engineer at Crisp, you will help unlock the potential of our customers’ data by highlighting and elevating the semantic context. Your responsibilities will include data cleansing, semantic labeling, normalization, and using modern BI Technology to efficiently convey insights to our customers. Being part of the engineering team, you will not only help clients leverage our data platform, you will also help evolve the platform itself by being a subject matter expert involved in product development.\\nThis is an evolving role with ample opportunity for growth. Whether you are coming from a startup or corporate background, you appreciate the significant impact to be had in smaller organizations and you relish the ability to shape your own role and the future of the company.\\nSigns of a great candidate for Crisp:\\nCollaborative: You know that your colleagues’ perspectives will make our customers successful. Similarly, you use your strengths to help us grow together. You propose ways for us to be more valuable to our customers.\\nCustomer focused: Our customers are at the forefront of your day. You prioritize our customers’ voices to ensure their needs are met.\\nAmbitious, curious, and resourceful: You are innately curious, and you aren’t afraid to work hard. You are self-driven and able to find creative results on your own, but you also take direction well. You are driven to succeed because your hard work and results make you proud.\\nDisciplined and reliable: You enjoy the benefits of working on a distributed team while consistently delivering what you have committed to. When you hit a snag, you communicate and reset expectations early.\\nAppreciative of honest feedback: You know that the best way to learn and grow is through constructive feedback delivered kindly. You view feedback given to you as an opportunity to get better and strive to do the same for others.\\nWork smarter and harder: You often identify a problem, create a solution, and bring it to a state of completion - with others, or even on your own. You find ways of eliminating or automating stuff that is uninteresting or wasteful.\\nSigns of a great candidate for a Data Engineer:\\nData oriented: You think about data in a rigorously structured manner. You live by the mantra “garbage in, garbage out” and are deeply experienced in the art of data cleansing.\\nFocus on the business problem: You are passionate about using visualizations to tell stories and glean actionable insights. In order to tell the story the right way, you need to understand how the business works and how to communicate with different stakeholders.\\nBrings business context to data engineering: You are the bridge between the business problem and the data pipeline. You are experienced in codifying business context via a semantic definition layer. You enable automation of labor intensive workflows.\\nStrong sense of aesthetics and user experience: You feel strongly about not just making Business Intelligence visually appealing, but also ensuring that it’s easy to learn and a pleasure to use.\\nDeep tooling expertise: Many BI tools have a point and click design layer, but you have a deep understanding of the modeling layer and how it interacts with the underlying data store. You are familiar with one or more tools that facilitate data exploration for the purposes of data cleansing or normalization. You hold strong opinions, born from experience, on the features that make a great semantic definition metadata capture tool. You are adept at transforming data in analytical databases using SQL.\\nWe are building a team of people with a breadth of combined experiences so that we can collaboratively enable our customers to be successful. There are no hard requirements on specific background, experience or geographical location. Instead we’re looking for individuals that are capable, reliable, and hoping to grow along with us. Do you have strengths you can share? If so, we’d love to hear from you!',\n",
       " 'Hi,\\nData Engineer\\nBay Area, CA – Onsite(Hybrid)\\nClient: Decision Minds/PANW\\nDuration: Contract\\nExp Level: 10+ Years\\nMust have skill: Google cloud exp\\nJob Responsibilities:\\nExpert in data engineering and GCP data technologies.\\nWork with client teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform.\\nWork with Agile and DevOps techniques and implementation approaches in the delivery\\nKey responsibilities: Architecture, Design and Development\\nRequired Skills:\\n10+ Year experience in BI and Analytics\\nHands on and deep experience ( at least 2 years) working with Google Data Products (e.g. BigQuery, Dataflow, Dataproc, ]etc.).\\nExperience in Spark (Scala/Python/Java) and Kafka, Airflow\\nData Engineering and Lifecycle (including non-functional requirements and operations) management.\\nE2E Solution Design skills - Prototyping, Usability testing and data visualization literacy.\\nExperience with SQL and NoSQL modern data stores.\\nThanks & Regards\\nJagadeesh\\nTeamware Solutions Inc |2838 E. Long Lake Road,Suite# 210, TROY, MI 48085\\nJob Type: Full-time\\nSalary: $60.00 - $65.00 per hour\\nExperience level:\\n10 years\\n11+ years\\n9 years\\nSchedule:\\nOn call\\nAbility to commute/relocate:\\nSouth San Francisco, CA 94080: Reliably commute or planning to relocate before starting work (Required)\\nExperience:\\nGoogle Cloud Platform: 4 years (Preferred)\\nData Engineer: 9 years (Preferred)\\nSpark: 4 years (Preferred)\\nWork Location: One location',\n",
       " 'Job Description\\n\\nAs a data engineer, you will design and maintain data platform road maps and data structures that support business and technology objectives. Naturally inquisitive and open to the deep exploration of underlying data, finding actionable insights, and working with functional competencies to drive identified actions. You also enjoy working both freely and as part of a team and have the confidence to influence and communicate with stakeholders at all levels, and to work in a fast-paced complex environment with conflicting priorities.\\n\\nAbout the role:\\n\\nReporting into the delivery leader, you will deliver consumable, contemporary, and immediate data content to support and drive business decisions. The key focus of the role is to deliver a custom solution to support various business critical requirements. You will be involved in all aspects of data engineering from delivery planning, estimating and analysis, all the way through to data architecture and pipeline design, delivery, and production implementation. From the beginning you will be involved in the design and implementation of complex data solutions ranging from batch to streaming and event-driven architectures, across cloud, on-premise, and hybrid client technology landscapes.\\n\\nBrief Description of Role:\\n\\nWe are looking for 3+ years of experience in data engineering in a customer or business facing capacity and experience in the following:\\nAbility to understand and articulate requirements to technical and non-technical audiences\\nStakeholder management and communication skills, including prioritizing, problem solving and interpersonal relationship building\\nStrong experience in SDLC delivery, including waterfall, hybrid and Agile methodologies\\nExperience of implementing and delivering data solutions and pipelines on AWS Cloud Platform.\\nA strong understanding of data modelling, data structures, databases, and ETL processes\\nAn in-depth understanding of large-scale data sets, including both structured and unstructured data\\nKnowledge and experience of delivering CI/CD and DevOps capabilities in a data environment\\nDevelop new inbound data source ingestions required within the multi-tiered data platform to support analytics and marketing automation solutions\\nSupports data pipelines - Builds the required dimensions, rules, segments, and aggregates\\nSupport all database operations: performance monitoring, pipeline ingestion, maintenance, etc.\\nMonitor platform health - data loads, extracts, failures, performance tuning\\nCreate/modify data structures/pipelines\\nLeveraging capabilities of Databricks Lakehouse functionality as needed to build Common/Conformed layers within the data lake\\nDevelop, document, and test software and environment setup to ensure that the outcome meets the needs of end-users and achieves business goals\\n\\nQualifications\\n\\nThe following skills are required:\\nTech Stack: AWS pipeline, Glue, Databricks, Python, SQL, Spark, etc.\\nBuilding the Data Lake using AWS technologies like S3, EKS, ECS, AWS Glue, AWS KMS, EMR\\nExtensive experience in ETL and audience segmentation\\nDeveloping sustainable, scalable, and adaptable data pipelines\\nAttention to detail in design, documentation, and test coverage of delivered tasks\\nStrong written and verbal communication skills, team player\\nIn addition, the candidate should have strong business acumen, interpersonal skills, and communication skills, yet also be able to work independently.\\nAt least 3 years of experience with designing and developing Data Pipelines for Data Ingestion or Transformation using AWS technologies\\nAt least 2 years of experience in the following Big Data frameworks: File Format (Parquet, etc.), Resource Management, Distributed Processing\\nAt least 3 years of experience developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps\\nAt least 1-2 years of experience with Spark programming (PySpark)\\nAt least 2 years of experience with Databricks implementations\\nFamiliarity with the concepts of \"delta lake\" and \"lakehouse\" technologies\\n\\nThe following skills are nice to have, and expertise is not required:\\nAdobe (Campaign, Audience Manager, Analytics)\\nMLFlow\\nMicrosoft Power BI\\nSAP Business Objects\\n\\nAdditional Information\\n\\nWhen you\\'re one of us, you get to run with the best. For decades, we\\'ve been helping marketers from the world\\'s top brands personalize experiences for millions of people with our cutting-edge technology, solutions and services. Epsilon\\'s best-in-class identity gives brands a clear, privacy-safe view of their customers, which they can use across our suite of digital media, messaging and loyalty solutions. We process 400+ billion consumer actions each day and hold many patents of proprietary technology, including real-time modeling languages and consumer privacy advancements. Thanks to the work of every employee, Epsilon has been consistently recognized as industry-leading by Forrester, Adweek and the MRC. Positioned at the core of Publicis Groupe, Epsilon is a global company with more than 8,000 employees around the world. Check out a few of these resources to learn more about what makes Epsilon so EPIC:\\nCulture: https://www.epsilon.com/us/about-us/our-culture-epsilon\\nDE&I: https://www.epsilon.com/us/about-us/diversity-equity-inclusion\\nCSR: https://www.epsilon.com/us/about-us/corporate-social-responsibility\\nLife at Epsilon: https://www.epsilon.com/us/about-us/epic-blog\\n\\nGreat People Deserve Great Benefits\\n\\nWe know that we have some of the brightest and most talented associates in the world, and we believe in rewarding them accordingly. If you work here, expect competitive pay, comprehensive health coverage, and endless opportunities to advance your career.\\n\\n#LI-SJ1\\n\\nREF186919L',\n",
       " 'Job Description\\n\\nAs a data engineer, you will design and maintain data platform road maps and data structures that support business and technology objectives. Naturally inquisitive and open to the deep exploration of underlying data, finding actionable insights, and working with functional competencies to drive identified actions. You also enjoy working both freely and as part of a team and have the confidence to influence and communicate with stakeholders at all levels, and to work in a fast-paced complex environment with conflicting priorities.\\nAbout the role:\\nReporting into the delivery leader, you will deliver consumable, contemporary, and immediate data content to support and drive business decisions. The key focus of the role is to deliver a custom solution to support various business critical requirements. You will be involved in all aspects of data engineering from delivery planning, estimating and analysis, all the way through to data architecture and pipeline design, delivery, and production implementation. From the beginning you will be involved in the design and implementation of complex data solutions ranging from batch to streaming and event-driven architectures, across cloud, on-premise, and hybrid client technology landscapes.\\nBrief Description of Role:\\nWe are looking for 3+ years of experience in data engineering in a customer or business facing capacity and experience in the following:\\nAbility to understand and articulate requirements to technical and non-technical audiences\\nStakeholder management and communication skills, including prioritizing, problem solving and interpersonal relationship building\\nStrong experience in SDLC delivery, including waterfall, hybrid and Agile methodologies\\nExperience of implementing and delivering data solutions and pipelines on AWS Cloud Platform.\\nA strong understanding of data modelling, data structures, databases, and ETL processes\\nAn in-depth understanding of large-scale data sets, including both structured and unstructured data\\nKnowledge and experience of delivering CI/CD and DevOps capabilities in a data environment\\nDevelop new inbound data source ingestions required within the multi-tiered data platform to support analytics and marketing automation solutions\\nSupports data pipelines – Builds the required dimensions, rules, segments, and aggregates\\nSupport all database operations: performance monitoring, pipeline ingestion, maintenance, etc.\\nMonitor platform health - data loads, extracts, failures, performance tuning\\nCreate/modify data structures/pipelines\\nLeveraging capabilities of Databricks Lakehouse functionality as needed to build Common/Conformed layers within the data lake\\nDevelop, document, and test software and environment setup to ensure that the outcome meets the needs of end-users and achieves business goals\\nQualifications\\nThe following skills are required:\\nTech Stack: AWS pipeline, Glue, Databricks, Python, SQL, Spark, etc.\\nBuilding the Data Lake using AWS technologies like S3, EKS, ECS, AWS Glue, AWS KMS, EMR\\nExtensive experience in ETL and audience segmentation\\nDeveloping sustainable, scalable, and adaptable data pipelines\\nAttention to detail in design, documentation, and test coverage of delivered tasks\\nStrong written and verbal communication skills, team player\\nIn addition, the candidate should have strong business acumen, interpersonal skills, and communication skills, yet also be able to work independently.\\nAt least 3 years of experience with designing and developing Data Pipelines for Data Ingestion or Transformation using AWS technologies\\nAt least 2 years of experience in the following Big Data frameworks: File Format (Parquet, etc.), Resource Management, Distributed Processing\\nAt least 3 years of experience developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps\\nAt least 1-2 years of experience with Spark programming (PySpark)\\nAt least 2 years of experience with Databricks implementations\\nFamiliarity with the concepts of “delta lake” and “lakehouse” technologies\\nThe following skills are nice to have, and expertise is not required:\\nAdobe (Campaign, Audience Manager, Analytics)\\nMLFlow\\nMicrosoft Power BI\\nSAP Business Objects\\n\\nAdditional Information\\n\\nWhen you’re one of us, you get to run with the best. For decades, we’ve been helping marketers from the world’s top brands personalize experiences for millions of people with our cutting-edge technology, solutions and services. Epsilon’s best-in-class identity gives brands a clear, privacy-safe view of their customers, which they can use across our suite of digital media, messaging and loyalty solutions. We process 400+ billion consumer actions each day and hold many patents of proprietary technology, including real-time modeling languages and consumer privacy advancements. Thanks to the work of every employee, Epsilon has been consistently recognized as industry-leading by Forrester, Adweek and the MRC. Positioned at the core of Publicis Groupe, Epsilon is a global company with more than 8,000 employees around the world. Check out a few of these resources to learn more about what makes Epsilon so EPIC:\\nCulture: https://www.epsilon.com/us/about-us/our-culture-epsilon\\nDE&I: https://www.epsilon.com/us/about-us/diversity-equity-inclusion\\nCSR: https://www.epsilon.com/us/about-us/corporate-social-responsibility\\nLife at Epsilon: https://www.epsilon.com/us/about-us/epic-blog\\nGreat People Deserve Great Benefits\\nWe know that we have some of the brightest and most talented associates in the world, and we believe in rewarding them accordingly. If you work here, expect competitive pay, comprehensive health coverage, and endless opportunities to advance your career.\\n#LI-SJ1\\nREF186919L',\n",
       " 'Job Description – Sr. Data Engineer:\\nMinimum experience of 5-8 years working as a Bigdata developer/engineer in PySpark\\nAbility to develop data integration and transformation code/pipelines in object oriented/scripting language: PySpark\\nHands on experience in Azure Cloud components such as Azure Databricks, Azure Data Factory, Azure Logic apps service, AKS, Azure Devops or any Cloud technologies\\nStrong skill in understanding database architecture, data models and writing complex SQL queries/code\\nHands on experience with integration of different data sources (Files, DBs, APIs)\\nAbility to work with large volume of data sets in performing data ingestion, loading, transformation and aggregation\\nKnowledge of various ETL techniques\\nExperience with data pipeline and workflow management tools [Azure DevOps]\\nStrong analytic skill to work with unstructured data\\nExperience working with Agile teams scrums.\\nStrong customer handling skills and communication skills\\nJob Type: Full-time\\nBenefits:\\n401(k)\\nDental insurance\\nHealth insurance\\nSchedule:\\n8 hour shift\\nAbility to commute/relocate:\\nDallas, TX 75287: Reliably commute or planning to relocate before starting work (Required)\\nExperience:\\nMicrosoft Excel: 1 year (Preferred)\\nMicrosoft Office: 1 year (Preferred)\\nCompliance management: 1 year (Preferred)\\nWillingness to travel:\\n25% (Preferred)\\nWork Location: One location\\nSpeak with the employer\\n+91 7272167989',\n",
       " 'Job Description:\\n12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.\\nComplete Description:\\nRequire the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals:\\n(1) Establish a data governance program,\\n(2) Perform a comprehensive data gap analysis,\\n(3) Design a master data architecture,\\n(4) Create a data warehouse for all data assets,\\n(5) Develop a front-end for program staff to quickly access workforce information and visualize program status,\\n(6) Create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities\\n(7) Foster relations with other agencies and improve inter-agency data integration.\\nThe Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff.\\nSpecific Duties:\\nDevelop, test, and maintain extraction, transformation, and load (ETL) processes.\\nDevelop and maintain an understanding of the data landscape including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.\\nSupport the Data Management Project team to develop and maintain data quality controls.\\nSupport the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.\\nSupport business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.\\nSupport the data stewards to troubleshoot and resolve data issues.\\nSupport business users to obtain requirements for enhancements and/or new analytic assets.\\nAssist in the Development of data asset training and documentation.\\nParticipate in the development and implementation of a data standard.\\nParticipate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.\\nJob Types: Full-time, Contract\\nPay: $66.00 - $74.00 per hour\\nSchedule:\\n8 hour shift\\nExperience:\\nin SQL, Python, R, JavaScript, JSON: 10 years (Preferred)\\nAgile Testing, Automation Testing, Black-box Testing: 10 years (Preferred)\\nWindows and Linux: 10 years (Preferred)\\nof BI tool architecture, Tableau: 9 years (Required)\\nWork Location: Remote',\n",
       " 'Job Description:\\nJob Title: Senior Data Engineer\\nDuration: 2 Months ()\\nLocation: New York, NY 10004 (100% Remote)\\n\"The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.\\nSUMMARY OF DUTIES AND RESPONSIBILITIES:\\n· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.\\nThe Senior Data Engineer will also be responsible for:\\n· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets\\n· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization\\n· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow\\n· Building back-end data migration tools and infrastructure to support data analytics work\\n· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization\\n· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management\\n· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.\\n· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)\\n· Data Engineer, Data and Analytics Unit (1)\"\\nCOMPUTER PROGRAMS/SOFTWARE OPERATED:\\n· Expertise in Python or equivalent programming language for automation\\n· Advanced knowledge of SQL\\n· Experience with API interfacing in a data engineering and analytics environment\\n· YEARS OF EXPERIENCE:\\n· 5+ years in a data engineering role in a large organization\\nPREFERRED SKILLS:\\n· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise\\n· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams\\n· Strong quality control abilities and exceptional attention to detail\\n· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines\\nJob Types: Contract, Full-time\\nSalary: $45.00 - $50.00 per hour\\nSchedule:\\n8 hour shift\\nExperience:\\nInformatica: 1 year (Preferred)\\nSQL: 1 year (Preferred)\\nData warehouse: 1 year (Preferred)\\nWork Location: Remote',\n",
       " 'Job Description:\\nROLE: Azure Data Engineer\\nLOCATION: Edison, NJ\\nGood knowledge in Azure\\nGood knowledge on data migration and data processing using Azure services: ADLS, Azure Data Factory, Azure Functions, Synapse/DW, Azure SQL DB, Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, Client Studio, AI/Client, Python\\nGood knowledge on DevOps in Azure platform\\nExperience developing and deploying ETL solutions on Azure\\nNice to have knowledge in IoT, event-driven, microservices, Containers/Kubernetes in the cloud.\\nJob Type: Full-time\\nSalary: $90.00 - $100.00 per year\\nSchedule:\\n8 hour shift\\nExperience:\\ndata factory: 1 year (Required)\\nSQL: 1 year (Preferred)\\nData mining: 1 year (Required)\\nWork Location: Remote',\n",
       " \"Job Description:\\nSchneider Electric has an opportunity for a Manufacturing Data Engineer in our Columbia, South Carolina location. The Manufacturing Data Engineer is a key contributor to the data strategies for the Columbia plant, specifically, in the areas of system design and performance.\\n\\nWhat will you do?\\nSpecializes in several areas of knowledge regarding production / manufacturing processes: process design, ergonomics, capacity, simulation tools, investment, and cost analysis\\nSupports, maintains, upgrades and troubleshoots applicable system infrastructure\\nDevelops a variety of applications and reports, leveraging large data sets to facilitate business operations and boost organizational efficiency.\\nAnalyzes business processes and develops reports, tools, scripts and procedures to bridge the gap between legacy systems\\nProvides end-user support, including researching user complaints/issues, answering technical questions, and/or assisting with application revisions.\\nManages Enterprise Resource Planning (ERP) by updating material, Bill of Materials (BOMs), and routings when modifications are required by Engineering\\nWhat qualifications will make you successful?\\nBachelor’s degree in a field of Engineering ME or EE\\nGood communication skills\\nRelevant experience preferred 3 plus years\\nExperience in Excel, Macros, SQL, and Tableau\\n\\nQualifications:\\nWhat's in it for me?\\nSchneider Electric offers a robust benefits package to support our employees such as flexible work arrangements, paid family leave, 401(k)+ match, and more.\\nWho will you report to?\\nManufacturing Engineering Manager\\n\\nLet us learn about you! Apply today.\\n\\nAbout Our Company:\\nWhy us?\\nSchneider Electric is leading the digital transformation of energy management and automation. Our technologies enable the world to use energy in a safe, efficient and sustainable manner. We strive to promote a global economy that is both ecologically viable and highly productive.\\n\\n€25.7bn global revenue\\n137 000+ employees in 100+ countries\\n45% of revenue from IoT\\n5% of revenue devoted for R&D\\n\\nYou must submit an online application to be considered for any position with us. This position will be posted until filled\\n\\nIt is the policy of Schneider Electric to provide equal employment and advancement opportunities in the areas of recruiting, hiring, training, transferring, and promoting all qualified individuals regardless of race, religion, color, gender, disability, national origin, ancestry, age, military status, sexual orientation, marital status, or any other legally protected characteristic or conduct. Concerning agencies: Schneider Electric does not accept unsolicited resumes and will not be responsible for fees related to such.\",\n",
       " 'Job Description: Involved in resource / volume ramp-up plan/ backfilling resources planned for rotation and resignation cases Tracking Performance of individual Projects within the delivery/SIPs. Participate / presenting SD performance during client visit Publish Daily, Weekly and Monthly reports on team productivity Reviewing the weekly / monthly dashboards Publishing dashboards to customer People Management Drive the team to reach the defined targets Ensure training and development of the team members Mentor team members on technical and communication aspects. onsite anni data enginneer-pheonix,az python,charlottee,nc ba-houston tx\\nJob Types: Full-time, Contract, Permanent\\nSalary: $39.87 - $86.24 per hour\\nBenefits:\\nDental insurance\\nCompensation package:\\n1099 contract\\nExperience level:\\n10 years\\n11+ years\\n8 years\\n9 years\\nSchedule:\\n8 hour shift\\nExperience:\\nInformatica: 1 year (Preferred)\\nSQL: 1 year (Preferred)\\nData warehouse: 1 year (Preferred)\\nWork Location: Remote\\nSpeak with the employer\\n+91 7842516676',\n",
       " 'Job Title :: Anaplan Data Engineer\\nLocation :: Remote\\nJob Types: Full-time, Contract\\nPay: $60.00 - $70.00 per hour\\nSchedule:\\n8 hour shift\\nWork Location: Remote',\n",
       " 'Job Title Data Engineer,\\nLocation: Dallas, TX\\nType of work- Onsite , C2C\\nJob Description\\nExperience 3-5 years\\nAt least 3+ years of enterprise experience in working with data bricks and highly proficient in SQL, Spark, Scala/Python.\\nSkilled in Big Data Technologies like Spark, Spark SQL, PySpark\\nExperience with one or more of the major cloud platforms & cloud services such as - Azure/AWS/GCP, Databricks\\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.\\nExperience building and optimizing ‘big data’ data pipelines, architectures and data sets\\nStrong analytic skills related to working with unstructured datasets\\nWorking knowledge of highly scalable ‘big data’ data stores\\nA successful history of manipulating, processing and extracting value from large, disconnected datasets\\nBuild processes supporting data transformation, data structures, metadata, dependency and workload management\\nExperience developing enterprise software products\\nExperience with at least one of these object-oriented/object function scripting languages: PySpark/Python, Scala, Java\\nBuild monitoring and automated testing to ensure data consistency and availability\\nExperience supporting and working with cross-functional teams in a dynamic environment\\nExperience working in an AGILE environment\\nJob Types: Full-time, Contract\\nSalary: $42.15 - $60.00 per hour\\nSchedule:\\n8 hour shift\\nAbility to commute/relocate:\\nDallas, TX 75201: Reliably commute or planning to relocate before starting work (Required)\\nWork Location: One location',\n",
       " 'Job Title: Azure Data Engineer\\nLocation: Mahwah, NJ\\nDuration: Full Time\\nSkills Required:\\nAzure data factory, data bricks, data lake, automation, and performance optimization of ETL\\nStrong Hands-on experience in ADF, data bricks, data lake, power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning\\nPrior Hands-on experience in end-to-end CI/CD implementation or Devops process in Data & Analytics context.\\nDesign, plan, develop and update technical docs & BI solutions.\\nUnderstand the requirements and define the data load strategy for data refresh.\\nCreate, debug, troubleshoot and deploy solutions.\\nWork on ETL design\\nDesigning and optimization of ETL Process using ADF\\nImplementing end-to-end automated ETL processes and monitoring of those processes using various options using Azure\\nExperience in Azure data factory, data bricks, data lake, automation, and performance optimization of ETL\\nJob Type: Full-time\\nWork Location: One location',\n",
       " 'Job Title: Big Data Engineer\\nLocation: Toronto, Ontario, Canada\\nJob Type: Full time\\n\\n\\nJob Description:\\n\\n\\nQualifications :\\n8+ years of software development experience in Big Data technologies (Spark/Hive/Hadoop)\\nExperience in working on Hadoop Distribution, good understanding of core concepts and best practices\\nGood experience in building/tuning Spark pipelines in Scala/Python\\nGood experience in writing complex Hive queries to derive business critical insights\\nGood Programming experience with Java/Python/Scala\\nExperience with AWS Cloud, exposure to Lambda/EMR/Kinesis will be good to have\\nExperience in NoSQL Technologies - MongoDB, Dynamo DB\\nRoles and Responsibilities :\\nDesign and implement solutions for problems arising out of large-scale data processing\\nAttend/drive various architectural, design and status calls with multiple stakeholders\\nEnsure end-to-end ownership of all tasks being aligned\\nDesign, build & maintain efficient, reusable & reliable code\\nTest implementation, troubleshoot & correct problems\\nCapable of working as an individual contributor and within team too\\nEnsure high quality software development with complete documentation and traceability\\nFulfil organizational responsibilities (sharing knowledge & experience with other teams/ groups)\\nConduct technical training(s)/session(s), write whitepapers/case studies/blogs etc.',\n",
       " \"Job type: Full-time position. Location: Mountain View, CA\\n\\nWhat inspires you? This is the question that drives most career decisions.\\n\\nIs it working with a fantastic team that is dedicated to a common goal? Is it the ability to make a significant impact on the success of a product and company? Perhaps it is to contribute to a product that ignites the creativity of content creators, small businesses, educators, and schoolchildren worldwide?\\n\\nWe have an immediate opening for a Senior Data Engineer. You will be working within the Analytics team at WeVideo to help build out and grow our data infrastructure. The team plays a central role in shaping the data ecosystem in our company, and enables business performance by providing users across the company with the insights, tools, infrastructure, and consulting to make data-driven decisions. Our org is hungry for data and needs you to help us get to the next level.\\n\\nWhat you will do in this role:\\nBe the lead engineer that builds and automates data pipelines for our Product data, as well as enable Data Integration projects.\\nDevelop and improve the technical architecture of current data warehouse (BigQuery)\\nDesign data models optimized for aggregation, visualization and advanced analytics (machine learning)\\nDesign, implement and maintain data pipelines from beginning to end. Our Product data pipeline is high velocity and we need to re-architect it using Apache Kafka. Our Business data pipes are standard fare, and we use ‘modern data stack’ tooling (ELT, reverse ETL, DBT, self-serve BI)\\nFacilitate data integrations and write optimized data transformations in SQL/Python\\nImplement automated QA and data quality checking systems for pipelines and warehouse\\nLeverage automation using Apache Airflow where possible to help the team scale\\nSkills and knowledge you possess:\\n2+ years in data engineering with a Bachelor's degree in CS, Data Science, or similar technical field, or equivalent professional experience\\nDemonstrated success building and automating batch and streaming data pipelines, as well as end-to-end Monitoring and Alerting solutions\\nHigh fluency with advanced SQL in BigQuery (or Redshift) environment\\nProficient in creating high quality, fast services and projects in Python\\nExperience building data pipes using Apache Kafka or Pub/Sub, Apache Airflow, GCP/AWS\\nStrong communication skills and interest in working with Marketing & Sales teams\\nBenefits & perks :\\nPotential remote opportunity\\nMedical, dental, vision and 401(k)\\nGenerous PTO policy\\nFree lunch and snacks\\nEnough free caffeine to keep you up for 2 weeks straight\\nEmployee development resources\\nWhy you might like working here:\\nWe’re a small, close-knit team that enjoys working and learning from each other.\\nPeople stick around. Some of your future colleagues have been for over 8+ years.\\nOur users love us; just take a look at the tweets shared by teachers.\\nAbout WeVideo:\\nWeVideo is a powerful, easy to use, cloud-based video creation platform that is the digital editing and storytelling choice of more than 22 million consumers, students, businesses, and third-party media solutions. WeVideo is available from virtually any computer or device at home, school, work, or on-the-go to capture, edit, view, and share videos. Built for the future in HTML5, WeVideo brings maximum speed, responsiveness, security, and expandability to browser-based video editing. WeVideo is a Google Play Editors' Choice selection with more than 9 million downloads to date. WeVideo is also the exclusive digital storytelling solution of Google’s Education Creative Bundle for Chromebooks and a Microsoft Education Partner. More than 6,500 schools use WeVideo to enhance classroom learning.\",\n",
       " 'Join the squad of experienced software experts and pump up your skills by building off-the-wall software solutions for Fortune 500 and Future 50 brands. We have opportunities in 90+ projects you can contribute to.\\nWhat you will do\\nWork collaboratively with other engineers, architects, data scientists, analytics teams, and business product owners in an agile environment\\nArchitect, build, and support the operation of Cloud and On-Premises enterprise data infrastructure and tools\\nDesign and build robust, reusable, and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data\\nBuild data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications\\nAssist in selection and integration of data related tools, frameworks, and applications required to expand platform capabilities\\nUnderstand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage\\nMust haves\\nBachelor’s degree in computer science, information systems, math, engineering, or other technical field, or equivalent experience\\n6 years of experience with Python or Java\\n4+ years of experience in building data lake, cloud data platform leveraging cloud (GCP/AWS) cloud native architecture, ETL/ELT, and data integration\\n3 years of development experience with cloud services ( AWS,GCP,AZURE) utilizing various support tools (e.g. GCS, Dataproc, Cloud Data flow, Airflow(Composer), Kafka , Cloud Pub/Sub)\\nExpertise in developing distributed data processing and Streaming frameworks and architectures (Apache Spark, Apache Beam, Apache Flink, )\\nIn-depth knowledge of NoSQL database technologies (e.g. MongoDB, BigTable, DynamoDB)\\nExpertise in build and deployment tools – (Visual Studio, PyCharm, Git/Bitbucket/Bamboo, Maven, Jenkins, Nexus, )\\n5 years of experience and expertise in database design techniques and philosophies (e.g. RDBMS, Document, Star Schema, Kimball Model)\\n5 years of experience with integration and service frameworks (e.g API Gateways, Apache Camel, Swagger API, Zookeeper, Kafka, messaging tools, microservices)\\nExpertise with containerized Microservices and REST/GraphQL based API development\\nExperience leveraging continuous integration/development tools (e.g. Jenkins, Docker, Containers, OpenShift, Kubernetes, and container automation) in a Ci/CD pipeline\\nAdvanced understanding of software development and research tools\\nAttention to detail and results-oriented, with a strong customer focus\\nAbility to work as part of a team and independently\\nProblem-solving and technical communication skills\\nAbility to prioritize workload to meet tight deadlines\\nThe benefits of joining us\\nProfessional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps\\nCompetitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities\\nA selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands\\nFlextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.\\nAbout our projects\\nOur team of data experts and engineers has helped create a supply chain management platform used by Forbes 500 enterprises and featured by Gartner. We’ve accompanied the platform from an early prototype to a full-fledged solution based on big data, AI, and machine learning. Data quality is integral to our current engagement with this project as we’re expanding the platform functionality.\\nAgileEngine has also been working on data and ML solutions in multiple industries and verticals like telecom, FinTech, energy efficiency, and more. Our work with our clients’ data has contributed to patented software products, Nasdaq-grade fraud detection, and products used by Deloitte and T-Mobile.\\nJob Type: Full-time\\nExperience level:\\n4 years\\nWork Location: Remote',\n",
       " \"Junior Data Engineer\\nJob Location: Fully Remote – UK/EU\\nJob Type: Full-time, Permanent\\nRelocation: No\\nWe’re pioneering the use of data to improve efficiency, transparency, and sustainability. Today, our aviation fuel management technology connects business functions, drives operational efficiencies, and enables environmental accountability across the globe. Our ambition is big, we’re transforming the aviation industry, and we need great people to make that happen.\\nBring Your Skills and Passion to Life at i6:\\nAs our company grows and matures from a start-up into a scale-up with an increase in customers and multi-tenancy applications, the opportunity for a curious and inspired Junior Data Engineer has opened up in our Data Team. This is your chance to join a team of innovative and like-minded engineers, analysts, and innovators pushing the boundaries of their products and themselves, working with our high-profile global customers to craft truly outstanding SaaS solutions. You will be responsible for creating, owning and driving the data solutions, including transforming data into critical assets for the business while ensuring security, reliability, privacy and performance by design.\\n\\nYour Career at i6:\\nIn your first month you’ll…\\nAsk lots of questions and begin to explore and really get to know our products, our clients and gain deeper insight into the Aviation industry\\nStart to build an idea of the challenges and requirements of our clients, think about what it really means to connect our biosphere to highly varied client architectures managing multiple interactions a second and dealing in near real time\\nBegin to translate business issues and requirements into data solutions\\nMaintain monitoring and alerting across the data processing systems\\nStart to learn and develop your skills in implementing data processes, ensuring consistency & re-usability\\nStart to work closely with data analysts ensuring data and its required structure & availability is accessible for reporting\\n\\nWithin 6 mths you’ll…\\nStart to develop the knowledge, expertise and confidence to identify pain points , challenge the status quo, then pioneer new ideas, develop designs and implement solutions helping us scale faster and drive the business forward\\nParticipate fully in, and contribute to, sprint planning, retrospectives, standups and other ceremonies in a constructive and can-do manner\\nBegin to gather data requirements, working closely with business stakeholders and analysts and design & build the required integrations into the data pipeline following industry best practices on ETL, ELT processes & data warehousing to support project delivery\\nWorking closely with the infrastructure team ensuring business data applications are well served and prioritise when required\\nWorking closely with Solutions Architects, Developers, Product Owners & Managers to ensure adherence to data models when developing new products or applications\\nStart implementing data analysts reporting requirements into the data warehouse or pipeline\\n\\nWithin 1 year, you’ll…\\nBe able to proactively identify gaps & solutions in the data functionality requirements of the business\\nEffectively manage challenges associated with handling large volumes of data while working towards tight deadlines\\nBe proposing, researching & supporting the implementation of technical solutions that help the business achieve its commercial objectives in a cost-efficient and scalable manner\\nFeeling proud of what you have achieved, the influence you have had and how you have made positive changes for yourself, your team, i6, your clients and the planet\\nWhat you’ll bring to the role:\\nBright, ambitious, humble and most of all inquisitive, you love telling stories with data, building reports and propositions that give real insight and impact, ensuring our products remain the best in the market. You love data and are a true evangelist, able to use your outstanding analytical and architecture skills to power internal insight and supercharge our data focused products. Data is the secret sauce to your superpower, and you are ready to share your expertise to help launch us to the next level.\\nWhat you will be working on: (You don't need all of this, but outstanding SQL is an essential)\\nEnterprise Systems\\nNear Real-Time Applications\\nGood understanding and experience using the following technologies: SQL, Python, Airflow, DBT, BigQuery, GitHub, GitHub Actions\\nNice to have understanding of the following technologies: BI Tools (e.g. Tableau), Docker, Kubernetes, Google Cloud Platform, AWS, Azure Cloud\\nBonus knowledge: Node.js, Typescript, MongoDB\\nSaaS\\nA dedicated, creative, and highly collaborative team of developers, engineers and analysts\\nA massive variety of systems and products for our major clients across the globe\\n\\nOur Interview Process:\\n\\nStage 1: Initial screening interview with Talent Manager\\nStage 2: In depth technical interview our Senior Data Engineer\\nStage 3: Cultural interview with our CTO\\nStage 4: Offer\\nStage 5: Hand in notice and join our awesome team at i6\\n\\nA Career With i6 Group\\n\\nOur Values, The Six I’s That Make i6:\\nImprove - we’re open to new ideas and deliver an amazing customer experience.\\nInfluence - we build strong relationships and make great decisions.\\nImpress - we work hard and smart to deliver great work on time.\\nInnovation - we share ideas, experiment and find new ways to solve challenges.\\nIntelligence - we continue to develop our skills, knowledge and behaviours.\\nIntegrity - we respect and embrace differences in people.\\n\\nOur Benefits\\n\\nSubsidised subscriptions to Headspace, gym membership and health insurance (UK Only)\\nVarious discounts and perks at high street shops, supermarkets and online vendors\\nFully remote – work from anywhere within the UK/ Europe (with occasional travel to Farnborough Airport)\\nClear goals, targets, and progression plan so you can maximise your career\\nCross training opportunities\\nSustainable workplace - Carbon footprint offset\\n\\nDiversity & Inclusion: Equal opportunities for everyone.\\nWe’re embracing diversity in all its forms and fostering an inclusive environment for all people to do their best work. This is integral to our mission of driving operational efficiency and environmental accountability across the world.\\n\\nQTPX6EGOKQ\",\n",
       " \"Junior Data Scientist or Data Engineer - ONSITE\\n\\nLocation: Army Cyber Institute, West Point, NY - ONSITE\\nBluePath Labs is looking for a Junior Data Scientist or Data Engineer to support our team at the Army Cyber Institute (ACI) in West Point, NY. This is a critical and exciting opportunity at the intersection of advanced cyber research and the latest cloud technologies. This effort will have direct operational relevance. The successful candidate will demonstrate a track record of successfully working in a collaborative environment. They will be working with leading Army researchers and programs and partners throughout the US Government. This position will start immediately.\\nPrimary Responsibilities\\nResponsible for designing, developing, testing, and maintaining data pipelines and data models that support advanced research and analysis for the Army Cyber community.\\nWork both alone and as part of a team with other developers, designers, and stakeholders to deliver high‐quality data‐driven insights, analytics, and models.\\nPerform exploratory data analysis, cleaning, and labeling on both structured and unstructured data.\\nImplement and train machine learning models on various data types, including text, tabular, time series and image data.\\nComfortable working with on‐premises and cloud‐native environments for the discovery, extraction, and analysis of patterns and trends to gather, process, and derive valuable insights needed to build AI‐enabled capabilities.\\nRequired Qualifications\\nMinimum of a bachelor's degree in data science (DS), data engineering (DE), computer science (CS), software engineering (SE), computer engineering (CE), electrical engineering (EE), mathematics, information technology (IT), information systems (ISYS) or a related field.\\nPrimary programming proficiency in Python and R.\\nAdditional experience designing and querying with relational and non‐relational databases (SQL, NoSQL, or time‐series).\\nFamiliarity with Debian-based Linux OSes, such as Ubuntu.\\nKnowledge of computer and network security principles, such as network segmentation and VLANs.\\nDesired Qualifications\\nU.S. or Five Eyes national citizenship.\\nAbility to obtain DoD Secret clearance.\\nAbout BluePath Labs\\nBluePath Labs is a fast-growing research and consulting company committed to solving complex problems for federal, state, and local government clients. We offer a range of professional, scientific, and technology services. Our specific areas of expertise include business consulting, research and data science, and technology integration.\\nBluePath Labs combines mission and business insights with advanced technologies to deliver measurable performance improvements for our clients. BluePath is dedicated to surpassing client expectations by always living by our core values of integrity, professionalism, and resilience. BluePath's extensive experience in Government, Military, Commercial, and Academic environments is unique among small businesses and a core differentiator of our solutions. Our multidisciplinary background allows us to solve diverse and complex problems. Most importantly, we work closely with our clients to frame problems correctly, optimize processes, leverage technologies, and implement enduring solutions. Labs are where ideas are born, experiments occur, and breakthroughs happen. It is the hallmark of BluePath's culture.\\nBluePathLabs.com\\nBluePath Labs is an equal opportunity employer.\",\n",
       " \"LENDEM Solutions is looking at add a Data Engineer to our business!\\nCORE COMPETENCIES\\nAbility to thrive in a dynamic and fast-paced environment, drive change, and collaborate effectively with a variety of individuals and teams\\nStrong analytical and problem-solving skills\\nExcellent communication skills, both verbal and written\\nStrong attention to detail and accuracy\\nResults-oriented and able to meet tight deadlines\\nPRINCIPAL DUTIES\\nShredding and parsing data to extract meaningful information\\nPreparing historical and live data for data studies to identify trends and patterns\\nPerforming adhoc analysis to answer specific business questions and provide insights\\nWorking with relational databases to model and query complex data relationships\\nUnderstanding and working with MySQL data in several different data environments\\nMining consumer loan data utilizing tools such as SQL, Python, R, or other comparable data mining tools.\\nEDUCATION AND EXPERIENCE\\nBachelor's degree in Computer Science, Engineering, or a related field Master’s degree preferred\\n3+ years of experience as a data engineer or similar role\\nREQUIRED SKILLS, ABILITIES, SOFT SKILL FACTORS\\n· Strong experience in ETL processes, data modeling, and data warehousing\\nExperience working with graph databases, such as Neo4j or Apache Cassandra\\nExpertise in programming languages such as SQL, Python\\nFamiliarity with big data technologies, such as Hadoop, Spark, or Kafka\\nAbility to analyze and manipulate large and complex data sets\\nStrong problem-solving skills and the ability to work independently and as part of a team\\nExcellent communication skills, both verbal and written\\nStrong attention to detail and accuracy\\nResults-oriented and able to meet tight deadlines\\nIf you are a data engineer with a passion for problem-solving and a strong background in ETL processes, data modeling, and graph databases, we encourage you to apply for this exciting opportunity. We offer a competitive salary, comprehensive benefits, and a dynamic work environment where you can continue to grow and develop your skills.\\nJob Type: Full-time\\nBenefits:\\n401(k)\\nDental insurance\\nHealth insurance\\nLife insurance\\nPaid time off\\nRetirement plan\\nVision insurance\\nExperience level:\\n3 years\\nSchedule:\\n8 hour shift\\nAbility to commute/relocate:\\nPlano, TX 75024: Reliably commute or planning to relocate before starting work (Required)\\nExperience:\\nPython: 5 years (Preferred)\\nSQL: 5 years (Preferred)\\nWork Location: Hybrid remote in Plano, TX 75024\",\n",
       " 'MOBE\\nMOBE guides people to better health and more happiness. Behind our innovative health solutions is uniquely human philosophy. We believe that person-to-person connections and understanding can make a difference in a world where self-care can be complicated, and health care is ever-evolving and complex.\\nMOBE works with health plans and large employers to identify individuals who are frequent users of health care but aren?t finding resolutions for their underlying health issues. We use a whole-person approach and guidance to impact health outcomes positively.\\nSupporting people is at the core of our business, employees included. MOBE is a high-growth organization with a culture built on trust and collaboration. Consistent across our teams and offerings is a belief in the power of people doing good together. We genuinely care about people and consider our workforce the most significant asset.\\nYour Role at MOBE\\nThis is an exciting time at MOBE and we are growing fast. At MOBE, we have a lot of data: eligibility, medical and pharmacy claims, marketing campaign impressions, transcripts from participant interactions, etc.\\nThis position is responsible for providing technical and project expertise to enable MOBE analytics and operations with structured and unstructured data. Responsibilities includes executing and/or leading user story development, data design and architecture, data pipeline development, testing and deployment in the Analytic Data Framework. This role will partner with internal and external business and technology teams to drive project deliverables and ensure high quality delivery of data architecture and integration.\\nResponsibilities\\nThe Data Engineer ensures the following capabilities and functions:\\nTranslate high level business processes into logical data processing steps\\nDesign data structures and pipelines that are flexible and scalable for MOBE analytics and operational requirements\\nSupport Analytic partners through collaborative and transparent development, information delivery, problem resolution, shared insights, and training\\nData processing definition, execution, and documentation, in a time appropriate way, to meet business priorities and requirements\\nData quality and maintenance consistent within the Analytic Data Framework\\nLead small to moderate sized projects and initiatives, following through on execution of chosen strategies and demonstrating the ability to work through obstacles and changing priorities.\\nDemonstrate ability and willingness to play multiple roles for different projects (e.g. planning/architecture, project development, hands-on technical resource/support for others, analysis and resolution of data issues)\\nIdentify and constructively communicate the need for improvements or enhancements in MOBE technology assets\\nAll other duties as assigned to help fulfill our Mission and abide by MOBE?s Guiding Principles',\n",
       " \"Meet Zartico, the world’s first Destination Operating System.\\nZartico’s mission is to empower DMOs to be better stewards of the world’s tourist destinations through improved data intelligence and decision-making. Makers of the first Destination Operating System, Zartico harnesses and streamlines complex data to provide a full spectrum of data science, benchmarking, and analytical services for use in marketing, community development, and sustainability efforts. Based in Salt Lake City, Utah, Zartico has over thirty years of experience in technology, tourism, and destination, travel and tourism marketing.\\nZartico is looking to add a Junior Data Engineer to the team. In this role, Primary Responsibilities include but are not limited to the following:\\nBe part of Data Engineering Team to develop data infrastructure that is able to ingest and transform data at scale coming from many different sources, different customers, and in many different varieties. You will build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources and build robust data pipelines that collect, process and compute business metrics from activity data. You will be responsible for creating critical datasets for machine learning, growth funnels, business forecasting, and many other strategic initiatives.\\nTo be successful in this role, you will need a strong background in Data Engineering, specifically, Python, SQL, Google Cloud Platform (BigQuery, GoogleCloud Storage, Airflow). In addition to needing an ability to problem-solve effectively, you will need to have strong written and oral communication skills.\\nWhat You’ll Do:\\nEffectively use, optimize, and automate processing pipelines, as well as explore new technologies to meet the expanding needs of our products.\\nExperience working as part of a Scrum team and familiarity with Scrum team ceremonies such as daily stand-up, backlog refinement, sprint retrospectives, sprint reviews, and sprint planning.\\nCollaborate with the product team to define high-level requirements for product development.\\nWhat you’ll bring :\\nYou will need 1-2 years of experience as a Data Engineer, with fluency in SQL and programming languages, specifically Python. Experience with Ruby and scripting languages will also be important. You will also need a successful history of manipulating, processing, and extracting value from large, disconnected datasets. You will be expected to be an expert in coding, with an ability to promote solid design and coding standards.\\nEducation and Certifications: BA/BS in a quantitative or computer science field.\\nWhy Zartico?\\nWe believe in a growth mindset. We are a learning organization.\\nWe emphasize focus because we know that to achieve big dreams, you have to execute and get the small stuff right.\\nWe lead with inclusion and value diversity. We believe in diversity of thought, perspective, and experience. Diversity of experience and perspectives creates a more robust product and a more beautiful world.\\nWe dream in color and code.\\nWe hustle.\\nWe are humble and know that the sum of our parts is greater than any one of us as individuals.\\nAbove all else, we do the right thing. We believe in transparency, honesty, and integrity.\\nWe believe travel and tourism are a force for good because it builds connection, understanding, and appreciation of our world’s cultures, history, and natural resources. We believe data and the right metrics allow us to make better decisions because transparent data helps focus on the right issues, problems, and therefore, solutions, to be better stewards of our world's most precious destinations.\\nWe’re building a global community—one that’s safe for people of all backgrounds. We are an equal-opportunity employer where our diversity and inclusion are central pillars of our company strategy. We look for applicants who understand, embrace and thrive in a multicultural and increasingly globalized world. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. When you join our team, you join the Zartico family.\",\n",
       " 'Metrohm Spectro is an advanced mobile spectroscopic instrumentation leader, developing, manufacturing, and servicing state-of-the-art analytical devices, including portable and handheld Raman analyzers.We provide solutions for the pharmaceutical, biomedical, safety and security, chemical, and academic research industries. We are constantly growing with new products and new opportunities, and are always looking for talented, dedicated employees to join us and grow together as a team.\\nWith the fast growth of our business, we have an immediate vacancy for a full-time Data Engineer for our Plainsboro, NJ location.\\nJob Description\\nIn this role, you will take responsibility for developing and maintaining databases within software products. You will be required to have hands-on problem solving, from the upkeep and generation of database, to data validation as well as the capability of data processing and analysis, and will be able to perform data processing algorithm validation with the knowledge of data science. To excel in this role, you need to be very organized with a fine eye for detail, and openness to learn new skills to meet growing business needs.\\nEducation\\n· Bachelor’s of Science degree from an accredited university or college in chemistry, physics, mathematics or computer science.\\nExperience:\\n· High-level proficiency in Microsoft Excel or other automated data management tool.\\n· Experienced in database programming and familiar with all popular database types. Good understanding of MySQL is a plus;\\n· Knowledge in MATLAB, R, Python or SAS tools for data processing and analysis;\\n· Knowledge in AI/machine learning and data mining basics;\\n· Knowledge in C/C++ programming for data processing algorithm;\\n· High-level proficiency in Microsoft Excel or other automated data management tool;\\n· Knowledge in Network/Cloud infrastructure will be a plus.\\nROLE AND JOB RESPONSIBILITIES\\n· Develop and maintain database for cross-platform software implementation on all BWTEK spectroscopic products.\\n· Assist in data process and analysis algorithm design and validation.\\n· Collaboration with entire software team for product enhancement and new product.\\nJob Type: Full-time\\nApplication Question(s):\\nWhat are your salary expectations?\\nWork Location: Plainsboro, NJ - on site\\nCan you commute to this location?\\nJob Type: Full-time\\nBenefits:\\n401(k)\\nDental insurance\\nHealth insurance\\nPaid time off\\nTuition reimbursement\\nVision insurance\\nSchedule:\\n5x8\\n8 hour shift\\nMonday to Friday\\nAbility to commute/relocate:\\nPlainsboro, NJ 08536: Reliably commute or planning to relocate before starting work (Preferred)\\nApplication Question(s):\\nWill you need sponsorship to work in US?\\nWork Location: In person',\n",
       " \"Mid-to-Senior Level Data Engineer/Data Scientist\\nSalary:100,000-180,000 yearly\\n\\nGet to know etrailer.com\\netrailer.com is an industry leader in helping people lead fulfilling lives by empowering them to complete work, go on vacations, and experience the outdoors. We achieve this through unmatched industry expertise, putting the customer first, and expert service. We want to supercharge our customer interactions to provide individualized, incredible customer experiences.\\n\\nWe are looking for...\\netrailer.com is looking for experienced Data Engineers/Data Scientists that will be key players in propelling the company beyond its current capabilities. Preferred candidates will be able to suggest and implement solutions with new toolsets, methodologies, and industry standards to help improve the etrailer data ecosystem.\\n\\nRequired Qualifications\\nBachelor's degree in Computer Science, Electrical/Computer Engineering, Statistics, Mathematics, or other related fields with 5 years of experience OR Master's degree in Data Science or Data Engineering with 3 years of experience\\nStrong Python experience\\nSQL experience\\nNice to Have Qualifications\\nAzure experience\\nSplunk experience\\nC# experience\\nExperienced in designing, implementing, and maintaining data capture and data processing solutions. Some example technologies include:\\nKafka\\nSQL\\nSplunk\\nPython\\nC#\\nExperienced in designing, implementing, and maintaining monitoring, alerting, and visualization solutions. Some example technologies include:\\nSplunk\\nPower BI\\nTableau\\nExperienced in designing, implementing, and maintaining machine learning solutions through the entire model life cycle. Some example technologies include:\\nPython\\nAzure cloud\\nData bricks\\nML Flow\",\n",
       " 'My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.\\nSnowflake Data Engineer\\nExperience – 6 to 8 Years\\nWork Location: Peachtree Corners GA ( our office)\\nWork model: Initially Onsite and then remote\\nClient: Goldman Sachs/Appridat Solutions, LLC\\nDuration: On going project – Long term\\nJob description for Data Engineer (Snowflake Data Engineer)\\nPosition Title: Mid-level to Sr. Data Engineer\\nRole Overview\\nWe are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.\\nTo ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.\\nQualifications/Requirements\\nBachelor’s Degree in Computer Science, IT, or related field.\\nEssential Skills\\nTechnical:\\no 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.\\no 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.\\no 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.\\no Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.\\no Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).\\no Work experience on JIRA and /or other Quality Management software.\\nFunctional:\\nDevelop and maintain datasets for business use.\\nBuild data systems and pipelines.\\nImproving data quality and efficiency.\\nCombined raw information from different sources.\\nClient facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.\\nA team player who works collaboratively and provides coaching and support to transfer technical & data knowledge\\nDemonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.\\nExperience in partnering with a diverse team in multiple locations.\\nTroubleshooting any issues that may arise.\\nProviding maintenance support.\\nThanks, and Regards\\nG Dileep Kumar\\nSr. Technical Recruiter\\nFuturetech Consultants, LLC\\n912-623-4772\\nJob Types: Full-time, Contract\\nSalary: $40.00 - $45.00 per hour\\nSchedule:\\n8 hour shift\\nAbility to commute/relocate:\\nNewton, MS 39345: Reliably commute or planning to relocate before starting work (Required)\\nApplication Question(s):\\nshare only who are willing to work On W2\\nExperience:\\nInformatica: 4 years (Preferred)\\nSQL: 4 years (Preferred)\\nData warehouse: 4 years (Preferred)\\nWork Location: One location\\nSpeak with the employer\\n+91 912-623-4772',\n",
       " 'Nursa is revolutionizing the temporary healthcare staffing industry by delivering an unparalleled level of transparency and accessibility to healthcare providers, and living out our mission to enrich the life of every clinician with innovation and opportunity. We’re committed to helping professionals such as registered nurses (RNs), licensed practical nurses (LPNs), and certified nursing assistants (CNAs) advance their careers by giving them the power to choose from available shifts provided by facilities in their area. In turn, Nursa’s marketplace platform technology empowers healthcare facilities to bring in the talent they need without spending time and resources on traditional nursing recruitment processes. By connecting healthcare clinicians and healthcare facilities to fill per diem and on-call shifts, the Nursa marketplace increases access and transparency to the $17 billion temporary clinician staffing industry. We’re a profitable, venture backed startup founded in March 2020 and are rapidly expanding. Your impact will not only be on the business, but also on the daily lives of healthcare providers and the quality of care everyone experiences.\\nRole overview:\\nOur data team is growing and we are expanding the Data Engineering pod within the Data Team. You will function as a Data Engineer and ETL Developer and will report directly to the Data Engineering Manager. You will help to expand our tech stack by implementing processes for ETL, data pipeline, and build monitoring tools. You will help manage our data warehouse, Segment (CDP / session tracking tool), and data integrations for third-party platforms (Salesforce, Iterable, etc). This Data Engineering pod works closely with Business Intelligence, Data Science, Development, as well as many other business teams. The ideal working situation would be hybrid, but highly qualified remote candidates will be considered.\\nThis position requires for someone to be onsite in Murray, UT.\\nResponsibilities:\\nTake a proactive role in expanding tech stack and building data engineering team\\nDevelop pipelines between systems using Rest or similar APIs\\nIntegrate and manage ETL for systems such as Salesforce, Iterable, RedShift, NetSuite\\nWork continually to monitor and improve data accuracy and reliability\\nCollaborate with and mentor others on data team and across the organization\\nQualifications:\\n3+ year experience in Data Engineering or ETL Development\\nProficiency in Python scripting, SQL, and database management (AWS Redshift)\\nMust have experience creating data pipelines to send and pull data with REST or similar API\\nExperience using third-party integrations (CRM or CDP)\\nAWS experience would be beneficial\\nWhat you get in return:\\nOpportunity to revolutionize healthcare industry and build both relationships and teams that make a tangible impact\\nWe empower team members to act intelligently and be owners, believing that execution is everything, and have designed a learning-focused environment where you get ongoing support and regular feedback to help you grow\\nAn opportunity to join an international team with a work culture that is based on trust, flexibility, and curiosity\\nCompetitive salary and benefits\\nClosing:\\nNursa is an equal opportunity employer. We aim to build a workforce of individuals from different backgrounds, with different abilities, identities, and mindsets. Even if you do not meet all of the qualifications listed above, we encourage you to apply!',\n",
       " \"Oddball believes that the best products are built when teams understand and value the things they are working on. We value learning and growth and the ability to make a big impact at a small company. We believe that we can make big changes happen and improve the daily lives of millions of people by bringing quality software to the federal space.\\nAs a Database Engineer you will be responsible for helping to develop production-quality analytics tools, utilizing existing data sources to empower stakeholders to inform research and development. Act as a liaison between engineers and analysts. Implement and enforce standards and procedures to ensure data is managed consistently and properly integrated.\\nExperience Requirements\\n5+ years of proven data and performance engineering experience.\\nA proven track record of developing custom-built data/analytics solutions and experience in supporting large scale programs.\\nExperience with system analysis, data analysis or programming, using a variety of computer languages and procedures.\\nExperience working in Agile environments.\\nExperience in developing, analyzing, and presenting data models.\\nKnowledge of data management, data standardization, and data governance.\\nAbility to analyze source data for potential data quality issues.\\nExpert in SQL and/or SQL based languages.\\nCreate supporting documentation, such as metadata and diagrams of entity relationships, business processes, and process flow.\\nCreate plans, test files, and scripts for data warehouse testing, ranging from unit to integration testing.\\nThis is a Remote position. This is a salaried role. Oddball offers both a tech, and a continuing education stipend.\\nMust be located within the United States.\\nClearances\\nAbility to obtain low-level federal clearance is required.\\nOddball is an Equal Opportunity Employer and does not discriminate against applicants based on race, religion, color, disability, medical condition, legally protected genetic information, national origin, gender, sexual orientation, marital status, gender identity or expression, sex (including pregnancy, childbirth or related medical conditions), age, veteran status or other legally protected characteristics. Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation.\\nAny applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation.\\nJob Type: Full-time\\nBenefits:\\n401(k)\\n401(k) matching\\nDental insurance\\nFlexible schedule\\nFlexible spending account\\nHealth insurance\\nHealth savings account\\nPaid time off\\nParental leave\\nProfessional development assistance\\nReferral program\\nRetirement plan\\nVision insurance\\nCompensation package:\\nBonus pay\\nExperience level:\\n5 years\\nSchedule:\\nMonday to Friday\\nEducation:\\nBachelor's (Preferred)\\nWork Location: Remote\",\n",
       " \"Our Company\\n\\nChanging the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.\\n\\nWe’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!\\n\\nJob Description\\nAdobe Customer Solutions is looking for a full time Data Engineer with experience in building data integrations using AWS technology stack as part of the team's Data as a Service portfolio for Adobe’s Digital Experience enterprise customers.\\nCustomer facing Engineers who enjoy tackling complex technical challenges, have a passion for delighting customers and who are self-motivated to push themselves in a team oriented culture will thrive in our environment\\nWhat you'll Do\\nCollaborate with Data architects, Enterprise architects, Solution consultants and Product engineering teams to gather customer data integration requirements, conceptualize solutions & build required technology stack\\nCollaborate with enterprise customer's engineering team to identify data sources, profile and quantify quality of data sources, develop tools to prepare data and build data pipelines for integrating customer data sources and third party data sources with Adobe solutions\\nDevelop new features and improve existing data integrations with customer data ecosystem\\nEncourage team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.\\nCollaborate with a Project Manager to bill and forecast time for customer solutions\\nWhat you need to succeed\\n\\nProven experience in building/operating/maintaining fault tolerant and scalable data processing integrations using AWS\\nProven track record in Python programming language\\nSoftware development experience working with Apache Airflow, Spark, MongoDB, MySQL\\nExperience using Docker or Kubernetes is a plus\\nBS/MS degree in Computer Science or equivalent proven experience\\nAbility to identify and resolve problems associated with production grade large scale data processing workflows\\nExcellent interpersonal skills\\nExperience crafting and maintaining unit tests and continuous integration.\\nPassion for crafting I ntelligent data pipelines that customers love to use\\nStrong capacity to handle numerous projects are a must\\nAt Adobe, you will be immersed in an exceptional work environment that is recognized throughout the world on Best Companies lists. You will also be surrounded by colleagues who are committed to helping each other grow through our outstanding Check-In approach where feedback flows freely.\\nIf you’re looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the significant benefits we offer.\\nAdobe is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, religion, age, sexual orientation, gender identity, disability or veteran status.\\n\\nOur compensation reflects the cost of labor across several U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position\\u202fis $101,500 -- $194,300 annually. Pay\\u202fwithin this range varies by work location\\u202fand may also depend on job-related knowledge, skills,\\u202fand experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.\\n\\nAt Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).\\n\\nIn addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.\",\n",
       " \"Our client is seeking a Data Engineer who would be responsible for design of data warehouse schemas as well as end-to-end design and implementation of fault tolerant and scalable data processing pipelines using a variety of technologies for orchestrating data movement, primarily Snowflake , DBT and Airflow . You'll work with a team of data engineers to design and implement solutions that provide business-critical insights.\\nSalary : Mid-Level - 130K to160K+ bonuses and equity options\\nLocation: REMOTE WORK 100% is an option: (locations limited to CO, AZ, TX, VA, CA, GA, HI, MN, MI, NJ, NY, NC, OR, PA, WA)\\nHow You’ll Do It\\nWorking collaboratively with a team of other data engineers and developers of varying experience levels and areas of expertise, you'll be responsible for:\\nResponsibilities\\nDesign and implement data warehouse schemas to store large data sets that can be easily and efficiently queried and joined together for reporting and analytical purposes.\\nDesign and develop ETL/ELT pipelines to efficiently move and aggregate data, while optimizing for maximum code and data reuse.\\nCollaborate with other team members on improvements to existing systems\\nInvestigate data anomalies and provide quick resolutions.\\nProvide technical support to business users and analysts.\\nWhat We’re Looking For\\nTOP SKILLS: ETL, Data Warehousing, SQL, Jinga Scripting, Python and Snowflake.\\n3+ years of working experience with large scale data warehouse systems.\\nVery strong knowledge of SQL and data manipulation best practices\\nExperience in building efficient and fault tolerant ELT/ETL data pipelines\\nVery strong knowledge of working with large scale datasets and data modeling and data warehouse design\\nExperience with Snowflake is preferable\\nKnowledge of DBT, Jinja scripting and Airflow is a big plus\\nNice to Have\\nExperience working cross functionally with product and engineering teams\\nDesire to wear many hats and work your tail off for a bit - great earning potential and equity available.\\nBenefits: A diverse benefit package, sign on bonus, options, etc. is offered through our client.\\nAbout ITEOM: ITEOM Digital Technology Talent Partners collaborates with top technologists and top employers to align purpose and passion and re-incorporate belonging to make the job search and work experience more human. We are an equal opportunity employer; however, this role is not eligible for sponsorship at this time. Employees eligible for full time W2 employment are encouraged to apply.\",\n",
       " 'Our client, one of the largest Amazon Web Services (AWS) partners for data services, is looking for a true Mid level Big Data Engineer to contribute to join their team of technologists to build and contribute to large-scale, innovative projects. Technological and career growth opportunities are a natural and every day part of the working environment.\\nThis role is only available for W2 or individual contracts. Please no C2C.\\n100% Remote Work.\\n\\nResponsibilities:\\nAnalyze system requirements and design responsive algorithms and solutions.\\nUse big data and cloud technologies to produce production quality code.\\nEngage in performance tuning and scalability engineering.\\nWork with team, peers and management to identify objectives and set priorities.\\nPerform related SDLC engineering activities like sprint planning and estimation.\\nWork effectively in small agile teams.\\nProvide creative solutions to problems.\\nIdentify opportunities for improvement and execute.\\n\\nRequirements:\\nMinimum 5 years of proven professional experience working in the IT industry.\\nDegree in Computer Science or related domains.\\nExperience with cloud based Big Data technologies.\\nExperience with big data technologies like Hadoop, Spark and Hive.\\nAWS experience is a big plus.\\nProficiency in Hive / Spark SQL / SQL. Experience with Spark.\\nExperience with one or more programming languages like Scala & Python & Java.\\nAbility to push the frontier of technology and independently pursue better alternatives.\\nKubernetes or AWS EKS experience will be a plus.\\n\\nThanks for applying!\\nU3GJMKlbkr',\n",
       " \"Overview\\nGrowth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.\\nThey are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.\\nLet Buchanan & Edwards help you unleash your potential and reach your goals.\\nAre you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.\\nThe success of our mission relies on you!\\nClearance\\nTS clearance\\nResponsibilities\\nResponsible for:\\nUnderstanding of programming and data engineering concepts and best practices.\\nExperience with Python, SQL, and/or Spark.\\nExperience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.\\nAbility to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics\\nAbility to work both independently and collaboratively.\\nHave experience with data pipelines or be willing to learn a pipeline from bottom to top\\nWill be able to trouble shoot files against an architecture to see where the upload process is failing.\\nWill be able to understand unit tests and add to them to increase stability to the entire pipeline.\\nFamiliar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.\\nQualifications\\nBachelors Degree\\n10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.\\n5 years of experience architecting solutions based on customer requirements.\\n5 years of experience leading technical teams.\\n3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.\\nExperience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.\\nJob Type: Full-time\\nPay: $120,000.00 - $157,000.00 per year\\nBenefits:\\n401(k)\\nDental insurance\\nHealth insurance\\nPaid time off\\nVision insurance\\nCompensation package:\\nYearly pay\\nExperience level:\\n10 years\\nSchedule:\\n8 hour shift\\nAbility to commute/relocate:\\nWashington, DC 20535: Reliably commute or planning to relocate before starting work (Required)\\nEducation:\\nBachelor's (Required)\\nExperience:\\nprogramming and software development: 10 years (Required)\\nPython, SQL, and/or Spark: 10 years (Required)\\nSecurity clearance:\\nTop Secret (Required)\\nWork Location: One location\",\n",
       " 'Overview\\nThe Data Engineer is a critical member of our growing data science team. In this role, you will have the chance to define and develop a core data asset which provides a representative and culturally aware view of the individuals and the communities that CINQCARE serves. You will work to evolve this asset over time using a product roadmap that includes identification and closure of gaps in existing data, introduction of new data sources, generation of proprietary data while quantifying and eliminating areas of structural biases. CINQCARE seeks to fix gaps that have persisted for generations in the delivery of care to Black and Brown populations and to so, we must also seek to fix gaps in data that ignore and marginalizes the Black and Brown communities that we serve.\\n\\nAn idea candidate for this role will embody CINQCARE’s core values, including, Trusted, Empathetic, Committed, Humble, Creative and Community-Minded. At CINQCARE, we don’t have patients or customers – we have Family Members.\\nJob Responsibilities\\nThe Data Engineer will have the following responsibilities:\\nThe Data Engineer will be responsible for the design, development and delivery of data pipelines and value-added data assets, leveraging a variety of data warehousing methodologies and disciplines to ingest the data from heterogeneous sources into cloud-based Data-Lake Environment in AWS.\\nManage initiatives & projects of significant complexity and risk. Excellent business and communication skills to be able to work with business owners providing input to prioritized roadmaps, develop work estimates, and ensure successful delivery to support strategic planning and initiatives, improve organizational performance, and advance progress towards CINQCARE’s goals.\\nAssist in the overall architecture of the ETL Design and proactively provide inputs in designing, implementing, and automating the data pipelines.\\nInvestigate and mine data to identify potential issues within the data pipelines, notify end-users and propose adequate solutions.\\nEnsure data quality and integrity within the data lake in the AWS environment with a focus on compliance to HIPAA and state level compliance requirements.\\nOversee user permissions and configurations for adherence to documented access management standards and policies.\\nIndependently (with minimal oversight) develop and maintain trusted advisor relationships with business, clinical, and operations leaders at the senior leadership level and with external partners, that include guidance for optimizing use of analytic capabilities and deliverables, and prioritization based on strategic vision.\\nUse coding/scripting pipelines and APIs to uncover and turn data into assets that are analysis-friendly using AWS services like Athena Data Catalogue, Quick Sight or any other big data tool on AWS.\\nCreate a high-quality catalog of all pertinent data with the primary goal of establishing a single source of truth and significantly increasing productivity by reducing the time required for data search and discovery.\\nCrossing team boundaries, educate/advise on data projects, on how to combine and aggregate client data across platforms or technologies, and how to make the greatest use of data.\\nLead consistent adherence to the Software Development Life Cycle framework and governance processes including, but not limited to leading planning sessions, collecting, and documenting requirements, identifying design patterns, create and define custom transformations, aggregations, and other data manipulations, developing data pipelines, creating documentation, developing test plans, performing unit testing, conducting peer review sessions.\\nUse knowledge of healthcare industry, market environment, and clinical and business workflows and activities, to inform solution design and development to execute high-quality or differentiated solutions in an established problem space.\\nPerform other job-related duties as assigned.\\n\\nGeneral Duties\\nThe Data Engineer should have the following duties:\\nLeadership: The Data Engineer will lead the continued build out of the data asset to create business value, including collaborating with their team to design, develop, and execute those strategies and solutions to deliver desired outcomes.\\nStrategy: The Data Engineer will contribute to the business strategy and roadmap: (1) improve outcomes for CINQCARE Family Members; (2) enhance the efficacy of other CINQCARE. business divisions; and (3) develop and deliver external market opportunities for CINQCARE products and services. In establishing the business strategy, the Data Engineer will define and innovate sustainable revenue models to drive profitability of the Company.\\nCollaboration: The Data Engineer will ensure that AI capabilities form a cohesive offering, including by working closely with other business divisions to learn their needs, internalize their knowledge, and define solutions to achieve the business objectives of CINQCARE.\\nKnowledge: The Data Engineer will provide subject matter expertise in the AI solutions, including determining and recommended approaches for designing and building elegant data structures in support of existing reporting tools and custom visualization platforms.\\nCulture: The Data Engineer is accountable for creating a productive, collaborative, safe and inclusive work environment for their team and as part of the larger Company.\\nQualifications\\nThe Data Engineer should have the following qualifications:\\nEducation: Bachelor’s degree in Computer Science, Engineering, Software Engineering, or related field; Master’s degree preferred.\\nExperience: The ideal candidate should have at least 3+ years of experience in healthcare data engineering. Experience with a variety of data projects and environments, whether on-prem or in-cloud (5+ years in SQL Server, ETL Tools, Business Intelligence & Analysis, Architecture). Familiarity with the Microsoft Stack; experience with other platforms is a plus. Mastery of Python and SQL. Strong foundational knowledge of data lakes and AWS products such as AWS Glue. Experience with healthcare eligibility and claims, implementing APIs, HL7/FHIR standards, ETL scheduling solutions, SQL, and healthcare data security.\\nEntrepreneurial: CINQCARE seeks to fix gaps that have persisted for generations in the delivery of care to Black and Brown populations. This position is accountable for ensuring CINQCARE is positioned to innovatively deliver on its promise.\\nCommunication: Strong analytical and collaboration skills is required. Excellent verbal, written communication and presentation skills; ability to clearly articulate and present concepts and models in an accessible manner to CINQCARE’s team, investors, partners, and other stakeholders.\\nRelationships: Ability to build and effectively manage relationships with business leaders and external constituents.\\nCulture. Good judgement, impeccable ethics, and a strong team player; desire to succeed and grow in a fast-paced, demanding, and entrepreneurial Company.\\n\\nLocation: New York, NY\\nCompensation: $100,000-$120,000\\nMy3ehHtP4z',\n",
       " 'Overview of the Role\\nWe are looking for an exceptional Data Engineer who is interested in joining a small and rapidly growing Product and Services team at our company. We view our data-driven products and services as strategic assets, and you will be a key part of our team and company. The successful candidate will have 5+ years of experience designing and implementing data solutions and services. This role will report to our VP, Product Engineering and will be an instrumental member of our product development team.\\nThe Data Engineer will participate in the design of new information intensive product and service offerings. They will be responsible for the development and implementation of the data pipelines and services underlying these new products. They will deepen their understanding of the political, policy, financial and business aspects of the health plan industry so as to be able to provide insight into the best manner to collect and manage information related to these businesses.\\nThe Data Engineer will work closely with the Product Management function to understand customer needs and help identify capabilities for new offerings. They will also partner with the Data Science team to understand the analytical needs of the business and assist in the design and implementation of data solutions to support these efforts.\\nAbout Predict Health, Inc.\\nPredict Health is transforming how health plans, providers and their partners increase retention, quality, and lifetime value of their customer or patient relationships. The core competency of the organization is the curation, analysis, and presentation of information to support targeted operational improvements in healthcare enterprises. The company is an early-stage enterprise based in Washington, D.C. which focuses on three solution categories: Member Acquisition, Quality Management, and Retention Management.\\nOur company, Predict Health was founded in 2019 and is led by an experienced team of healthcare experts, data scientists and healthcare entrepreneurs. We are focused on building services and solutions designed to improve the healthcare experience of our nation’s seniors. We founded our company after seeing first-hand how challenging it is for the seniors in our lives - including our parents, grandparents and friends - to deal with the complexities of Medicare with little or no real help.\\nDetailed Job Description\\nThe Data Engineer will initially support the deployment of an Azure solution to help collect, aggregate, transform, and distribute information related to Predict Health’s current service offerings. The Data Engineer is responsible for designing the solution, building the data integration and associated pipelines, design and deploying data structures required to support current products and analytical efforts, and automating the operational aspects of the data platform. The Data Engineer responsibilities will include:\\n· Data acquisition and ingestion - Identify data sources and build pipelines using various ETL tools such as, but not limited to, Azure Synapse, Azure Data Factory and Azure pipelines.\\n· Translate business requirements from product owners and analysts into technical code.\\n· Identify, fix and document bugs, bottlenecks in workflows and pipelines.\\n· Enforce security compliance, fine tune performance and promote code quality standards.\\n· Identify and curate new data sources required to support business requirements\\n· Explore, learn the latest Azure data management technologies to add to current competencies.\\n· Support development of data intensive solutions in a fast-paced, dynamic environment\\n· Support Business Intelligence Applications (e.g. Power BI) as required.\\nQualifications\\nRequires minimum of five years relevant experience in design, development, implementation, and testing of data services\\n· Experience in data structures, data modeling, data warehousing and data pipeline development on Azure\\n· 3+ years of hands-on experience in the design, implementation and performance tuning of Azure Data Factory, Synapse, or other Azure data services\\n· Hands on experience in orchestrating data flows for scalable, highly available data engineering and data science solutions in Azure\\n· Experience developing software code in one or more programming languages (Java, Python, etc.)\\n· Hands-on experience in DevOps and Infrastructure as Code preferably using Azure DevOps (ADO) and Terraform\\n· Experience with data migration\\n· Experience defining system architectures and exploring technical feasibility trade-offs.\\n· Ability to prototype and evaluate applications and interaction methodologies.\\nOther Knowledge, Skills, and Abilities Required\\n· Demonstrated capability for problem solving, decision making, sound judgement and assertiveness.\\n· Ability to change direction to meet demand in a high-paced work environment\\n· Ability to focus on details and technicalities while simultaneously retaining a comprehensive vision of organizational goals\\n· Excellent collaborator and able to influence and direct work contributions from others to meet critical program deliverables and timelines\\n· Advanced interpersonal, written, and oral communication skills.\\n· Proficiency in Microsoft Office including Excel and PowerPoint\\nWhat we Offer\\nThis is a full-time position and compensation, and benefits will be highly competitive.\\nPredict Health is committed to bringing together people from different backgrounds and perspectives to deliver real results for our clients and end-users. We recruit, employ, compensate, and promote regardless of race, religion, color, national origin, gender identity, disability, age, veteran status, and other protected status as required by applicable law and as a matter of our company values.\\nThis position will be based in our Arlington, VA office. We offer a flexible work schedule with a minimum of three days in the office each week.\\nJob Type: Full-time\\nPay: $100,000.00 - $130,000.00 per year\\nBenefits:\\nDental insurance\\nEmployee discount\\nFlexible schedule\\nHealth insurance\\nLife insurance\\nPaid time off\\nVision insurance\\nSchedule:\\nMonday to Friday\\nSupplemental pay types:\\nBonus pay\\nAbility to commute/relocate:\\nArlington, VA: Reliably commute or planning to relocate before starting work (Required)\\nWork Location: One location',\n",
       " \"Overview:\\nEsri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?\\n\\nAs an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.\\n\\nIf you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!\\nResponsibilities:\\nAnalyze, enhance, prepare and load spatial data into normalized databases on regular intervals\\nWrite and maintain Python and SQL scripts for data processing and loading\\nEvaluate new data sources for quality and attribution to support product requirements\\nReview data specifications and changes, adjust scripts and processes where necessary\\nRequirements:\\n2+ years of experience developing Python scripts or tools\\nExperience writing scripts to perform spatial operations, advanced queries, and joins in SQL\\nIntermediate to advanced experience working with large, normalized databases\\nExperience transforming data from flat to normalized databases\\nPrior experience with ETL workflows\\nAbility to read data product specifications and translate into database models and tables\\nAbility to manage priorities and tasks as needed in a fast-paced work environment\\nA keen attention to detail and drive to resolve any issues encountered\\nBachelor's in computer science, information systems, GIS, or related field, depending on position level\\nRecommended Qualifications:\\nExperience working with commercial data from data vendors such as Here, TomTom, or Infutor\\nExperience working with open-source spatial data such as OpenStreetMap, or Tiger/Line\\nExperience creating stored procedures for optimizing common tasks and operations\\nExperience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing\\nPrior experience with geocoding, geocoding data, or address data\\nKnowledge of Agile software development using Scrum\\nPrior experience with software development and release of commercial software products\\nMaster’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level\\nThe Company:\\nOur passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.\\n\\nEsri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.\\n\\nIf you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.\\n\\nEsri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.\\n\\nA reasonable estimate of the base salary range is $72,800.00 - $124,800.00.\\n\\n#LI-EL1\\n#IND1\",\n",
       " \"POSITION TITLE: Junior Data Engineer YEARS OF EXPERIENCE: 1-3\\nLOCATON: 100% remote\\n*****Please Note: Aretec, Inc. does not offer Corp - 2 - Corp (C2C) employment. *****\\nAretec is looking for a Junior Data Engineer. The Junior Data Engineer will be primarily responsible for design, development, support and enhancement of the data pipelines developed in AWS.\\n\\nRESPONSIBILITIES:\\nYou'll write clean and functional code on the front- and back-end\\nYou'll write reusable and maintainable code\\nCoordinate with data migration plans\\nAbility to communication and collaborate with various teams and vendors.\\nParticipates in functional and technical design.\\nParticipation in Agile activities Scrum, Kanban.\\nEnsure coding, testing, debugging and implementation activities completed as required.\\nFlexible and adaptable with the ability to align to changing priorities\\nThe developer should have great communication skills and be able to discuss and develop requirements with multiple levels of staff from corporate and field locations\\nAn interest in and ability to understand financial reporting, accounting concepts and related accounting data\\nParticipate in data flow diagramming and/or process modeling (code architecture)\\nDocuments work and steps to completion as required\\nFollows AWS best practices to integrate with ecosystem and infrastructure\\nAbility to partner with domain architects to implement the defined solution architecture including application, infrastructure, data, integration, and security domains\\n\\nREQUIRED SKILLS:\\n1-3 years of software engineering experience\\n1+ years of real industry experience\\nExperience with website development, web services and API development\\nHands-on experience performing data engineering and transformation tasks using Python\\nExperience implementing backend in Python using frameworks such as Django or Flask\\nKnowledge of web technologies - both back and front-end development including, but not limited to JavaScript, React, CSS, HTML, T-SQL, and Python\\nUnderstand log monitoring and analytics\\nExperience Meeting both technical and consumer needs\\nExperience testing software to ensure responsiveness and efficiency\\nA general knowledge of index migrations, debugging and researching concepts are major pluses\\nMust be aware of CI/CD pipelines and well-versed in using GitLab for creating required pipelines for CI/CD\\n\\nEDUCATION: Bachelors Degree in Mathematics/Statistics/Technology/Science/Engineering/Applied Mathematics or related field\\n\\nCERTIFICATIONS: N/A\",\n",
       " \"Position Purpose:\\nReporting to the Manager of the Data Warehouse team, part of the larger Health Informatics group, the data engineer applies their technical expertise to meet the needs of the department and Steward Health Care Network (SHCN).\\n\\nKey Responsibilities:\\nETL/Automation\\nDesign configurable data process flows with full automation\\nDevelop ETL processes for data loading and data extraction\\nSchedule ETL processes for full process automation\\nData Engineering\\nResponsible for data analysis to support building data processes and reporting\\nDesign useful and accurate data marts that meet requirements\\nApply SQL skills when designing and building data marts and data flows\\nQuality\\nEstablish and utilize QC processes to ensure data integrity\\nIncorporate standard error logging and alerts to ensure data is loaded as expected\\nDocumentation\\nCreate and maintain clear documentation\\n\\nEducation / Experience / Other Requirements\\n\\n\\nEducation:\\nBachelor's degree in Computer Science, Mathematics, Statistics or related experience\\n\\n\\nYears of Experience:\\n5+ years of database related work\\n2+ years of focus on healthcare data\\n\\nSpecialized Knowledge:\\nKnowledge of healthcare data\\nExperience using relational databases, SQL Server experience preferred\\nExperience using ETL tools (SSIS, Informatica, etc.)\\nStrong SQL programming skills\\nExperience with scripting languages (PowerShell, R, Python, etc.)\\nExperience automating data flows\\nExperience with Health Catalyst tools preferred, but not required\\nDeep understanding of database structures and data design.\\nCreative, flexible, and self-motivated with sound judgment\\nStrong communication skills\\n\\n\\n\\n\\nLocation: Steward Health Care Network · 1301.72330 Steward Health Care Network\\nSchedule: Full Time, Day Shift, 40 hours\",\n",
       " 'Position can be 100% remote but preferred to have candidates who can periodically (2 x month) work at headquarters.\\n(Data Engineer)\\nJob Description:\\nAssigned Personnel to provide data analytic support to the Data Analytics/Data Integration Project for Judicial Branch Statistical Information System (JBSIS) data reporting.\\nThis position will perform high level data engineering and data analytics on a variety of agency data sources, but primarily on the Judicial Branch Statistical Information System (JBSIS).\\nPartnering with IT staff, this position will reengineer JBSIS to create new technical documentation for JBSIS; create mappings for the Court Statistics Report and other JBSIS products, make policy recommendations, create and/or implement new governance standards, enhance data auditing and data quality controls, and create data visualizations.\\nThese same tasks may be performed with additional agency datasets.\\nSpecific Skills/Qualifications Required\\nTechnical project management and documentation skills.\\nAbility to analyze issues from system documentation and recommend solutions.\\nExperience managing technical projects, including conflict resolution, issue escalations, status reporting and resource management.\\nExperience creating and executing data mappings and scripts to clean, compile and analyze data\\nAbility to assess and maintain data pipeline, data quality in the database, and address data reporting issues.\\nExperience developing and implementing testing protocols for data and system quality\\nExperience in R and Stata.\\nExperience with data visualization and software such as Tableau and Power BI.\\nExcellent oral, written, analytical and communication skills with the ability to lead a technical discussion to both technical and non-technical staff.\\nExcellent analytical, verbal and conflict resolution skills.\\nAdditional Skills/Qualifications Desired:\\nGeneral:\\nUnderstanding of courtroom operations and workflow.\\nExperience in government (State) setting\\nExcellent presentation skills for both technical and non-technical audiences, including creating and presenting executive summaries to management and technical committees.\\nTechnical:\\nExposure and experience with Cloud computing.\\nConceptual understanding of Amazon Web Services, Microsoft Azure, Google Cloud, IBM and Oracle Cloud Platforms.\\nPrior experience using Snowflake\\nExperience using Python or other database query languages.\\nJob Types: Full-time, Contract\\nPay: $112,604.02 - $150,000.00 per year\\nBenefits:\\n401(k)\\n401(k) matching\\nDental insurance\\nEmployee assistance program\\nFlexible schedule\\nFlexible spending account\\nHealth insurance\\nHealth savings account\\nPaid time off\\nProfessional development assistance\\nReferral program\\nRelocation assistance\\nRetirement plan\\nVision insurance\\nSchedule:\\nMonday to Friday\\nAbility to commute/relocate:\\nSan Francisco, CA 94102: Reliably commute or planning to relocate before starting work (Required)\\nExperience:\\nData science: 9 years (Required)\\nWork Location: Hybrid remote in San Francisco, CA 94102',\n",
       " 'Position: Azure Data Engineer\\n\\nThe Azure Data Engineer will work with our business operations and technology teams to build out an enterprise-wide Data Warehouse and related systems in the cloud. Will collaborate with our internal teams and 3rd party vendor in building our future state, business process, system interfaces, governance and technology platforms needed to enable a single source of truth for product and customer data, drive a more sophisticated data environment and be positioned to scale for future business growth.\\n\\nRoles and Responsibilities\\nIn collaboration with our 3rd party vendor, implement strategic roadmap and operational framework for Enterprise Data Management, in partnership with key stakeholders\\nCollaborate on the strategy on new cloud data stores and migration of existing data in Azure\\nWork collaboratively with teams to design, document, and ensure continuous improvement of SDLC best practices for data in the cloud\\nBe an advocate for quality and security through data quality and accuracy validation and auditing of security best practices\\nFocus on understanding business problems and determine what aspects require optimization; articulate those aspects in a clear and concise manner\\nWork collaboratively with cross-functional teams to support Data Models in alignment with business needs\\nPartner with IT, on design and oversight of APIs and other data sharing capabilities in collaboration with short / long term business needs\\nPartner with IT to integrate new data management technologies and software engineering tools (automation projects, process improvements, etc.) into existing systems to enable single source of truth for product and customer data\\nAdhere to identified key performance indicators for quality and compliance metrics to ensure data related policies and standards are followed\\nActs as a SME to explain all facets of EDM data exchange and processing, business user involvement, and overall architecture\\nSupport Monogram as necessary with any tasks required to deliver excellent personalized kidney care and perform all other duties as assigned\\nUphold the mission and values of Monogram Health in all aspects of your role and activities\\nPosition Requirements\\nNashville, TN\\nBS or equivalent experience in crucial duties and responsibilities\\n3-5 years’ experience in cloud computing\\nSolid understanding of enterprise data management in Healthcare Data\\nSolid understanding of data, databases, data tools (e.g. SQL) and data processing experience\\nSolid understanding of standard ETL tools and techniques, sourcing, maintaining, and updating data, data warehousing, data cleansing, and other analytical techniques required for data usage.\\nExperience with database systems, Azure cloud storage, and significant exposure to or experience with modern data and BI platforms (such as Databricks, MSSQL Data Warehouse (Synapse), Snowflake, and Tableau).\\nPrevious experience with data migration to cloud-based environment\\nInterest and passion in designing and implementing data management, querying, and storage, with a particular focus on purpose-built data stores.\\nFunctional knowledge of data visualization tools such as PowerBI and Tableau.\\nStrong intuition for business and analytical skills\\nTeam player with excellent communication skills\\n\\nBenefits\\n\\nOpportunity to work in a dynamic, fast-paced and innovative value-based provider organization that is transforming the delivery of kidney care\\nCompetitive salary and opportunity to participate in the company’s bonus program\\nComprehensive medical, dental, vision and life insurance\\nFlexible paid leave and vacation policy\\n401(k) plan with matching contributions\\n\\nAbout Monogram Health\\nMonogram Health is a next-generation, value-based chronic condition risk provider serving patients living with chronic kidney and end-stage renal disease and their related metabolic disorders. Monogram seeks to fill systemic gaps and transform the way nephrology, primary care and chronic condition treatment are delivered. Monogram’s innovative, in-home approach utilizes a national nephrology practice powered by a suite of technology-enabled clinical services, including case and disease management, utilization management and review, and medication therapy management services that improve health outcomes while lowering medical costs across the healthcare continuum. By focusing on increasing access to evidence-based care pathways and addressing social determinants of health, Monogram has emerged as an industry leader in championing greater health equity and improving health outcomes for individuals with chronic kidney and end-stage renal disease.\\n\\nAt Monogram Health we believe in fostering an inclusive environment in which employees feel encouraged to share their unique perspectives, leverage their strengths, and act authentically. We know that diverse teams are strong teams, and welcome those from all backgrounds and varying experiences.\\nExperience\\nRequired\\nSQL\\nhealthcare data\\ncloud computing\\nAzure\\nPreferred\\nTableau\\nSnowflake\\nData Warehouse\\nDatabricks\\nPower BI',\n",
       " 'Primary Skills\\nSCALA, SPARK, SQL, HADOOP, AWS, HIVE, Spark SQL, HIVE QL, CICD (Continuous Integration/Continuous Delivery), VCS (GIT HUB)\\nCoding in Scala\\nDesigning in of HADOOP ecosystem\\nHands-on experience on AWS tools like EMR, EC2\\nHands-on experience of SQL in Big Data: SQL, Spark SQL, Hive QL\\nProficient in working with large data sets and pipelines\\nProficient with workflow scheduling / orchestration tools\\nWell versed with CICD process and VCS\\nDatabricks is a big PLUS\\nJob Types: Full-time, Contract\\nPay: $50.00 - $58.00 per hour\\nSchedule:\\n8 hour shift\\nMonday to Friday\\nExperience:\\nSpark: 4 years (Required)\\nScala: 4 years (Required)\\nHadoop: 3 years (Required)\\nAws: 3 years (Required)\\nHive: 3 years (Required)\\nCI/CD, VCS: 3 years (Required)\\nDatabricks: 1 year (Required)\\nWork Location: Remote',\n",
       " 'PrizePicks\\nJob Posting\\nJob Title: Data Engineer\\nLocation:\\nHours: Full-time\\nAbout Us: PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on increasing sports fan engagement. With our flagship game, PrizePicks, we set out to build the most fast & simple fantasy sports game possible.\\nJob Description: The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.\\nKey Responsibilities:\\nCreate, maintain and orchestrate optimal data pipeline architecture\\nAssemble large, complex data sets that meet functional / non-functional business requirements.\\nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.\\nQualifications:\\nAdvanced working SQL knowledge and experience working with relational databases\\nExperience building and optimizing ‘big data’ data pipelines, architectures and data sets.\\nBuild processes supporting data transformation, data structures, metadata, dependency and workload management.\\nStrong organizational skills.\\nExperience supporting and working with cross-functional teams in a dynamic environment.\\nExperience with object-oriented/object function scripting languages: Python.\\nPreferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.\\nExperience with AWS or like cloud services: EC2, EMR, RDS, Redshift, GCP\\n\\nHiring Process:\\nRecruiter Resume Screening\\nHiring Manager Initial Interview\\nTechnical Assessment (Should take < 4 hours)\\nPeer Interview\\nPeer Interview\\n\\nHow You’ll Ramp:\\n30 Days: Familiar with the data tech stack - comfortable querying basic datasets and requests for the team\\n60 Days: Contributing to internal conversations on data organization and structure\\n90 Days: Rolled out your first data pipeline to support analytics teams w/ a new data source\\nWant to Learn More?\\nPrizePicks agrees DFS partnership with Atlanta Falcons (SBC Americas)\\nPrizePicks Continues to Post Record Months to Begin 2021, Expands Leadership Team With an Eye Toward Category Growth (Yahoo Finance)\\nPrizePicks Stacks Its Chips for the Future, Closes Round of Strategic Investors (AP News) ? CSL Esports joins forces with PrizePicks (Esports Insider)\\n#LI-remote\\n#LI-DM1',\n",
       " 'Required Skills:\\n5+ years of hands-on data analysis & SQL\\nMust have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling\\nETL/SSIS\\nWill be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.\\nWill participate in daily stand up calls and other scrum ceremonies\\nWill work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.\\n*\\nNice to Haves: (NOT REQUIRED)\\nAI/ML\\n-Hadoop\\nR, Python\\nLinear Algebra and Calculus\\nJob Types: Full-time, Contract\\nPay: $40.00 - $70.00 per hour\\nSchedule:\\n8 hour shift\\nWork Location: Hybrid remote in Braham, MN 55006',\n",
       " 'Required Skills:\\nMust have 5-8+ Years of experience as Data Engineer\\nSolid experience in pyspark & SQL\\nExperience working with AWS services (S3, EMR) and Databricks platform\\nFollowing skills are important to have:\\nSQL\\nPython\\nSpark\\nMDM (Master Data Management)\\nQA/ UAT\\nCloud\\nJob Types: Full-time, Contract\\nSalary: $65.00 - $70.00 per hour\\nBenefits:\\nDental insurance\\nHealth insurance\\nCompensation package:\\nPerformance bonus\\nExperience level:\\n8 years\\nSchedule:\\nMonday to Friday\\nExperience:\\nData engineer: 8 years (Required)\\nMDM (Master Data Management): 1 year (Required)\\nQA/UAT: 1 year (Required)\\nAWS: 1 year (Required)\\nPython: 1 year (Required)\\nWork Location: Remote',\n",
       " 'Requirements\\nExperience in developing, deploying and operating on large scale distributed systems on a commercial scale Experience working in Cloud-based Big Data Infrastructure - GCP Good working experience on Cloud, Delta Lake, ETL processing. Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc. Working knowledge on Python and PySpark Programming.',\n",
       " 'Responsibilities\\nFunction as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.\\nOptimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components\\nDeploy and customize Daman Standard Architecture components\\nMentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions\\nProvide feedback and enhance Daman intellectual property related to data management technology deployments\\nAssist in the development of task plans including schedule and effort estimation\\nQualifications\\nBachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required\\n1+ year experience working with Azure analytical stack\\nDeep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)\\nExperience with Delta Lake is nice to have but experience with Azure Data Lake is required\\nExperience with Databricks is nice to have\\nExperience building high-performance, and scalable distributed systems\\nContinuous Data Movement/ Streaming/ Messaging:\\nExperience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS\\n3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)\\n3+ years experience in software engineering, leveraging Java, Python, Scala, etc.\\nBachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required\\nJob Type: Full-time\\nPay: $72,727.90 - $158,843.83 per year\\nSchedule:\\n8 hour shift\\nMonday to Friday\\nWork Location: In person',\n",
       " 'Responsibilities\\nProvide Information and Architecture solutions to ensure efficient manipulation, organization and sharing of information, data and/or master data in data-lake, lake-house and data-mart constructs.\\nCollaborate with Cloud Enterprise Architect to define the appropriate Cloud Database solution for current and new systems, such as AWS Redshift, Database Migration Service, Glue, EMR, Elastic Compute Cloud (EC2), S3, Relational Database Service (RDS)/Aurora, and Amazon Kinesis.\\nDefine access patterns, storage methods and integration patterns for the movement and maintenance of data into unstructured, relational and noSQL databases\\nDevelop and design the models for complex analytical and data warehouse systems including performing tasks related to database design, data analysis, data quality, metadata management and support.\\nWork with highly diverse data from a wide variety of sources, in a wide variety of formats, and employed by a wide variety of highly siloed systems.\\nApply best practices to design and build complex ETL processes\\nSupport services vendor and third party product vendors in the migration of data to the Cloud\\nCompetences\\nYrs.\\nBachelor’s degree in computer science or engineering or equivalent work experience\\nAWS Solutions Architect – Associate certification within past three years\\nAWS Certified Data Analytics – Specialty or AWS Certified Big Data – Specialty certifications\\nSolutions architecture experience in an enterprise environment with emphasis on data systems\\n5\\nSpecific experience designing, executing and supporting AWS data lakes at scale\\n3\\nExperience designing, optimizing, and maintaining relational and non-relational databases including MySQL, MS SQL, Redshift, AWS RDS and other NoSQL databases\\n5\\nExperience with the design and building of ETL packages, data pipelines and connecting these to BI applications\\n5\\nKnowledge of data movement and data presentation tools such as; Informatica, MS SSIS, AWS Glue, Cognos, Tableau\\n3\\nAbility to pass a background check and pass the criteria for a CJIS environment\\nWillingness to research and self-study to keep technical skills relevant in a highly complex environment\\nAbility to multi-task and prioritize deadlines as needed to deliver results\\nAbility to work independently or as part of a team\\nMentors and coaches colleagues and seeks opportunities for continuous improvement\\nExcellent verbal and written communication skills with great attention to detail and accuracy\\nExperience working in an Agile/Scrum environment\\nJob Types: Full-time, Contract\\nPay: $89,677.00 - $125,000.00 per year\\nBenefits:\\n401(k)\\nDental insurance\\nHealth insurance\\nPaid time off\\nVision insurance\\nSchedule:\\nMonday to Friday\\nOn call\\nExperience:\\nAWS Cloud Engineer: 8 years (Required)\\nAWS Cloud Architecture: 5 years (Required)\\nData warehouse: 3 years (Required)\\nLicense/Certification:\\nAWS Certification - Professional (Required)\\nWork Location: One location',\n",
       " 'Responsibilities\\n· Analyze and organize raw data\\n· Build data systems and pipelines\\n· Evaluate business needs and objectives\\n· Interpret trends and patterns\\n· Conduct complex data analysis and report on results\\n· Prepare data for prescriptive and predictive modeling\\n· Build algorithms and prototypes\\n· Combine raw information from different sources\\n· Explore ways to enhance data quality and reliability\\n· Identify opportunities for data acquisition\\n· Develop analytical tools and programs\\n· Collaborate with data scientists and architects on several projects\\nRequirements\\n· Previous experience as a data engineer or in a similar role\\n· Technical expertise with data models, data mining, and segmentation techniques\\n· Knowledge of programming languages (e.g. Java and Python)\\n· Hands-on experience with SQL database design\\n· Great numerical and analytical skills\\n· Degree in Computer Science, IT, or similar field (STEM)\\nJob Types: Full-time, Contract\\nSalary: $60,000.00 - $80,000.00 per year\\nBenefits:\\nDental insurance\\nEmployee assistance program\\nLife insurance\\nProfessional development assistance\\nCompensation package:\\nBonus pay\\nExperience level:\\n1 year\\nUnder 1 year\\nSchedule:\\n8 hour shift\\nMonday to Friday\\nNo nights\\nNo weekends\\nExperience:\\nSQL: 1 year (Preferred)\\nWork Location: On the road',\n",
       " 'Responsibilities:\\n8+ years professional experience as a data engineer\\nStrong programming skills (some combination of Python, Java, and Scala preferred)\\nExperience working with Databricks\\nAnalyzing the data sources, building, and scaling Pipelines & reports to meet the needs of the Analytics team and other engineering teams\\nAs a COSMOS Developer the mandatory technical skills include TSQL, SSIS, SSAS COSMOS SCOPE Scripting, Azure data stack (ADF, SQL Azure, ADL)\\nExperience with creating and monitoring date pipeline with ADF, Azure analytics services\\nGreat problem-solving skills, understanding proposed data models and alignment with business requirements\\nKnowledge of C# to understand assembly / Custom packages is desirable\\nAbility to understand vast amounts of data, identify and fix data issues\\nKnowledge in data modeling is desirable\\nKnowledge of data warehousing concepts\\nExperience writing SQL, structuring data, and data storage practices\\nExperienced building data pipelines\\nKnowledge of working with microservices\\nQualifications:\\nA passion for building and running continuous integration pipelines\\nMore than 2 years hands on experience on Azure Services like Databricks, Azure Data Factory, Azure Storage Accounts & Azure Data Lakes and 1 year of experience working with Cosmos DB or any other document based noSQL databases (MongoDB)\\nMore than 3 years of experience in programming knowledge on Python, Scala\\nExperience with SQL and NOSQL Databases preferred\\nPreferred:\\nDevOps – CD/CI Implementations\\nFramework Development and Automation Techniques\\nExperience in implementation of Data Catalogue and Data Lake Implementations\\nExperience in Data Management Solution Development with strong experience in SQL and NoSQL data bases\\nJob Types: Full-time, Contract\\nPay: $60.00 - $70.00 per hour\\nBenefits:\\n401(k)\\nDental insurance\\nFlexible spending account\\nHealth insurance\\nHealth savings account\\nReferral program\\nVision insurance\\nExperience level:\\n8 years\\nSchedule:\\n8 hour shift\\nMonday to Friday\\nApplication Question(s):\\nHow many years of Cosmos experience do you have?\\nWork Location: Remote',\n",
       " 'Responsibilities:\\nDevelop predictive models\\nDevelop optimization models\\nDevelop re-activation and retention models\\nAdvanced analytics to drive incremental revenue\\nIdentify performance metrics definition, algorithm development and automation\\nReporting and visualization\\nComplex data analysis tasks\\nData anomaly detection and correction modeling\\nConversion of data into stories for internal and external consumption\\nCross-team support for CRM and Database Marketing Teams.\\nQualifications:\\nBachelor’s degree in an Analytical field (Business, Marketing) required.\\nAt least three (3) years casino database experience required, or the equivalent combination of education and experience in data analysis.\\nSAS programming level 1 or higher certification required.\\nAbility to use data to solve complex business problems.\\nAdvanced SQL skills\\nStrong industry experience of Microsoft Office Suite, including Excel, Word, Access, required\\nJob Type: Full-time\\nSalary: $1.00 per hour\\nSchedule:\\n8 hour shift\\nExperience:\\nInformatica: 1 year (Preferred)\\nSQL: 1 year (Preferred)\\nData warehouse: 1 year (Preferred)\\nWork Location: One location',\n",
       " 'Responsibility:\\nCollaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies.\\nWork with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems.\\nUtilize programming languages like Python, ReactJs, JavaScript and Open Source RDBMS and Cloud based data warehousing services such as Snowflake.\\nShare your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community.\\nCollaborate with product managers and deliver robust cloud-based solutions.\\nPerform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance\\nBasic Qualifications:\\nBachelor’s Degree\\nAt least 4 years of experience in application development (Internship experience does not apply).\\nAt least 1 year of experience in data technologies.\\nHands on Experience in application development with Python, Pandas, NumPy, SQL, Docker.\\nPreferred Qualifications:\\n2+ years of experience in application development including Python, SQL, Scala, or Java\\n2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud).\\n1+ year experience working on real-time data and streaming applications like Kafka is big plus.\\n1+ years of data warehousing experience (Redshift or Snowflake)\\n1+ years of experience with UNIX/Linux including basic commands and shell scripting\\n2+ years of experience with Agile engineering practices\\nJob Type: Full-time\\nBenefits:\\n401(k)\\nHealth insurance\\nPaid time off\\nExperience level:\\n3 years\\nSchedule:\\nMonday to Friday\\nAbility to commute/relocate:\\nNew York, NY: Reliably commute or planning to relocate before starting work (Required)\\nWork Location: One location\\nSpeak with the employer\\n+91 9599382735',\n",
       " 'Role: AWS Data Engineer\\nLocation: Atlanta, GA (Meet customer Monthly once post client selection & onboarding)\\nJD for AWS Data Engineer\\nExperience with the core AWS services, plus the specifics mentioned in this job description.\\nExperience with Docker . Having used ECS , SNS/SQS, Athena, Lambda,S3, RDS and layers in the past is a plus.\\nProficiency in at least in Python, Java\\nStrong notions of security best practices (e.g. using IAM Roles, KMS, etc.).\\nExperience with monitoring solutions such as CloudWatch, Cloud Trail.\\nPrevious exposure to large-scale systems design.\\nKnowledge of writing infrastructure as code (IaC) using CloudFormation or Terraform.\\nExperience with building or maintaining cloud-native applications.\\nPast experience with the serverless approaches using AWS Lambda is a plus. For example, the Serverless Application Model (AWS SAM).\\nJob Types: Full-time, Contract\\nSchedule:\\n10 hour shift\\n8 hour shift\\nWork Location: Remote',\n",
       " 'Role: Azure Data Engineer (Azure Data Factory, Databricks)\\nLocation: Atlanta, GA\\nDuration: Fulltime\\n\\nJob Description for Data Engineer:\\nCandidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).\\nMust be able to analyze data and develop strategies for populating data lakes.\\nCandidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.\\nResponsibilities Work as on Cloud Data and Analytics solutions.\\nParticipate in development of cloud data warehouses, data as a service, business intelligence solutions.\\nDatabricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).\\nDeveloping Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)\\n\\nSkills/Qualifications\\nExpertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.\\nFamiliarity with implementing Data warehousing Solutions.\\nExperience as Data Engineer in Azure Big Data Environment is nice to have.\\nProgramming experience in Pyspark or Python, SQLs\\nHands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.\\nGood understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.\\nGood understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.\\nGood analytical and problem-solving skills.\\nI1dfOh43IR',\n",
       " 'Role: Azure Data Engineer (with Azure Data Bricks)\\nJob Location: Santa Clara, CA\\nJob Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)\\nInterview: 1st Level – Video Interview and 2nd Level – In-Person\\nSelected Candidates need to work On-Site (NO Remote Work)\\nOnly local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).\\nPrefer Fulltime\\nExperience: Minimum 10 Years (Total IT exp) with 8 years in Data.\\nCandidate should have knowledge for Databricks and Lakehouse and understands the architecture.\\nJob Description:\\nAt least 8+ years’ experience, ideally within a Data Engineer role.\\nCI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.\\nDemonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.\\nExcellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.\\nPrior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).\\nGood to have some experience in AWS/Azure.\\nCapability of developing highly-scalable RESTful APIs.\\nExcellent team player and can work well in an individual capacity as well.\\nDetail-oriented and possess strong analytical skills.\\nPays strong attention to detail and deliver work that is of a high standard.\\nHighly goal-driven and work well in fast-paced environments. Good to have data science background.\\nJob Types: Full-time, Contract\\nPay: $121,702.09 - $140,000.00 per year\\nExperience level:\\n10 years\\nSchedule:\\n8 hour shift\\nAbility to commute/relocate:\\nSanta Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)\\nExperience:\\nData Engineer: 10 years (Preferred)\\nData warehouse: 1 year (Preferred)\\nWork Location: In person',\n",
       " 'Role: Azure Data Engineer (with Azure Data Bricks)\\nJob Location: Santa Clara, CA\\nJob Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)\\nInterview: 1st Level – Video Interview and 2nd Level – In-Person\\nSelected Candidates need to work On-Site (NO Remote Work)\\nOnly local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).\\nPrefer Fulltime\\nExperience: Minimum 10 Years (Total IT exp) with 8 years in Data.\\nCandidate should have knowledge for Databricks and Lakehouse and understands the architecture.\\nJob Description:\\nAt least 8+ years’ experience, ideally within a Data Engineer role.\\nCI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.\\nDemonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.\\nExcellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.\\nPrior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).\\nGood to have some experience in AWS/Azure.\\nCapability of developing highly-scalable RESTful APIs.\\nExcellent team player and can work well in an individual capacity as well.\\nDetail-oriented and possess strong analytical skills.\\nPays strong attention to detail and deliver work that is of a high standard.\\nHighly goal-driven and work well in fast-paced environments. Good to have data science background.\\nJob Types: Full-time, Contract\\nPay: $121,702.09 - $140,000.00 per year\\nExperience level:\\n10 years\\nSchedule:\\n8 hour shift\\nAbility to commute/relocate:\\nSanta Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)\\nExperience:\\nData Engineer: 10 years (Preferred)\\nData warehouse: 1 year (Preferred)\\nWork Location: One location',\n",
       " 'Role: Azure Tech Lead/ Sr Data Engineer – Onsite role – Preferred locals\\nLocation: Jersey City, New Jersey / Fort Mill, South Carolina.\\nFull-time\\nMandatory Skills: MS Azure using Azure Data Factory, MS Synapse, Scala, Spark, Data Warehousing\\nSkills:\\nOver all 12 to 15 years of experience with Data Management, Data Warehousing and Analytics.\\nAt least 4 to 5 years of experience in Architecting and Implementing Data Solutions.\\nAt least 3 years of experience in implementing the data solutions on MS Azure using Azure Data Factory, MS Synapse.\\nOne to two years of experience in Azure Synapse Analytics is plus.\\nInstalling and configuring ADF integration runtimes and linked services.\\nAt least one hands on experience with Big data platform tool selection POC.\\nTwo years of experience in data migrations to Azure by using data box or Data migration Services.\\nApache Spark experience using Scala or PySpark or pre-packaged tools like Databrick is must.\\nExtensive hands-on experience in data warehousing design, tuning and ETL/ELT process development by using cloud native technologies.\\nAt least one year experience with unified data governance solution using MS Purview.\\nDeveloping the CICD pipeline for Azure Infrastructure, version control strategy and Integrate source control ( Azure repos)\\nIn-depth understanding of various storage services offered by Azure.\\nExperience with implementation of data security, encryption, PII/PSI legislation, identity and access management across sources and environments.\\nExperience with data process Orchestration, end-to-end design and build process of Near-Real Time and Batch Data Pipelines.\\nCertification in Azure data engineering and solution architecture Azure is must.\\nStrong client-facing communication and facilitation skills.\\nJob Type: Full-time\\nSalary: $81,075.29 - $186,473.81 per year\\nBenefits:\\n401(k)\\nDental insurance\\nHealth insurance\\nPaid time off\\nSchedule:\\n8 hour shift\\nExperience:\\nAzure: 4 years (Preferred)\\nSQL: 1 year (Preferred)\\nData warehouse: 1 year (Preferred)\\nWork Location: On the road',\n",
       " 'Role: Data Engineer\\nEssential Skills:\\nSQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language\\nData pipelines in python is required\\nPython–Statistical Programming\\nData Visualization\\nStrong Python programming.\\nStrong SQL programming.\\nStrong Data Modeling.\\nExcellent communication and presentation skills.\\nBasic knowledge of Tableau.\\nShould have interest in creating BI reports.\\nJob Types: Full-time, Contract\\nPay: From $30.00 per hour\\nBenefits:\\nDental insurance\\nHealth insurance\\nPaid time off\\nVision insurance\\nSchedule:\\n8 hour shift\\nAbility to commute/relocate:\\nMenlo Park, CA: Reliably commute or planning to relocate before starting work (Required)\\nExperience:\\nSQL: 3 years (Required)\\nWork Location: One location',\n",
       " 'Role: Data Engineer Lead\\nLocation: Oakland, CA\\nDescription\\nQualitest is looking for a Data Architect to join our team.\\nMust Have-\\nSeasoned data professional with experience implementing solutions across Data Quality, Data Assurance,\\nData Governance and Test Data Management\\nAbility to assess current state, recommend solutions and build a rollout strategy\\nAnalyze existing data landscape, business processes and create knowledge articles (architectures, data flows, tools)Design, Build and Deliver quality gates across data lifecycle for automated and continuous data quality assurance.\\nIntegrate data testing solutions with Automation, CI/CD Pipelines for Continuous Testing\\nWorking experience on various database technologies such as Oracle, SQL Server, MongoDB, Snowflake, Teradata\\nGood understanding about QA Test Management, Defect Management and Testing methodologies. Evaluate tools, build proof of concepts, and implement NextGen solutions\\nWork with customer executives and cross functional teams for seamless project implementation and delivery\\nMentor onshore/offshore teams to deliver projects in time and assist in case of delays/clarifications\\nExperience working on data quality validation tools such as RightData, Icedq, Informatica, Tosca DI, Datagaps\\nExperience working on TDM tools such as Informatica TDM , IBM Optim, Delphix, CA TDM/Broadcom TDM\\nProficient in AWS Cloud Services such as Glue, Lambda, RedShift, Dynamo DB and Databricks\\nExperience working on Data Warehouse, Data Lake, Big Data platforms\\nRequirements\\nGood to Have-\\nKnowledge about Enterprise Applications such as Salesforce, SAP, Veeva CRM\\nAI/ML knowledge on building Intelligent data validations and eliminate data anomalies\\nJob Types: Full-time, Contract\\nSalary: From $41.80 per hour\\nSchedule:\\n8 hour shift\\nMonday to Friday\\nAbility to commute/relocate:\\nOakland, CA 94619: Reliably commute or planning to relocate before starting work (Required)\\nExperience:\\nData modeling: 1 year (Preferred)\\nWork Location: One location\\nSpeak with the employer\\n+91 +12019071788',\n",
       " 'Role: Lead Informatica / Data Engineer – On Prem –ETL (Onsite role) / Senior Informatica / Mid-Level Informatica\\nLocation: Mount Laurel, NJ / Charlotte, NC\\nDuration : FTE\\nClient :: Hexaware / TD Bank\\nKey Skills: Informatica Power Centre, Autosys, Unix\\nMust Have\\nMore than 12+ years of IT experience in Datawarehouse and ETL\\nHands-on Experience on ETL Informatica Power Centre\\nExperience on Autosys, Unix and scripting knowledge on Python, Shell Scripts\\nExperience on Oracle Database\\nAbility to understand ETL Design, Source to target mapping (STTM) and create ETL specifications documents\\nFlexibility to operate from client office locations\\nAble to mentor and guide junior resources, as needed\\nBanking experience on RISK & Regulatory OR Commercial OR Credit Cards/Retail\\nNice to Have\\nAny cloud experience on Azure or AWS or Informatica cloud connector\\nAny relevant certifications\\nJob Type: Full-time\\nSalary: $69,919.38 - $166,922.18 per year\\nBenefits:\\n401(k)\\nDental insurance\\nHealth insurance\\nPaid time off\\nSchedule:\\n8 hour shift\\nExperience:\\nInformatica: 1 year (Preferred)\\nSQL: 3 years (Preferred)\\nData warehouse: 1 year (Preferred)\\nWork Location: On the road',\n",
       " \"Second Wave Delivery Systems, LLC (SWDS) is determined to improve clinical outcomes in the United States while addressing the rising costs of healthcare. We are developing a technology-enabled, integrated system tailored to optimize value-based arrangements for payors and providers serving Medicare and Medicaid enrollees. Through an increased focus on primary and preventive care, we strive to provide patients with significantly better outcomes and satisfaction in each community we serve.\\n\\nThe Technology organization at SWDS is responsible for building the next generation of value-based healthcare management and optimization through a family of systems and the foundational technologies required to support SWDS’ ambitious mission. Predicated on the latest technologies and built upon a mantra of Engineering and Operational Excellence, we develop mission-critical applications and infrastructure quickly, reliably, and while having fun. We strive to grow, foster, and maintain an entrepreneurial, diverse, collaborative, and flexible team and structure. As a fully remote, fast-paced, and quickly growing organization, we are always looking for talented, passionate, and ambitious individuals who seek to solve exciting and challenging technology problems while having a transformative impact on US healthcare.\\n\\nCore values and ways of working\\nWe are a tight group of engineers focusing on engineering and operational excellence while delivering high quality solutions to our clients and business partners\\nWe take great pride in our team and the products we deliver, and treat both with utmost respect and care\\nWe are firm believers in engineering high-quality, fault-tolerant systems that scale with business needs with minimal unforeseen disruption\\nWe value great work and great ideas, not ego. We're looking for talented individuals that are accustomed to working within a fast-paced environment and want to ship often to make an impact\\n\\nResponsibilities\\nBuild and maintain scalable, high-performance data processing systems (batch and/or streaming)\\nLead data engineering projects to ensure pipelines are reliable, efficient, testable, & maintainable\\nDesign our data models for optimal storage and retrieval and to meet critical product and business requirements\\nContribute to tooling & standards to improve the productivity and quality of output across the company\\nWork and consult with various non-technical stakeholders\\nParticipate in interviewing and onboarding of new team members\\n\\nJob Benefits\\nHealth insurance\\nVision insurance\\nDental insurance\\nLife insurance\\nSimple IRA matching\\nPaid time off\\n\\nMinimum Qualifications\\n5+ years working in Data Engineering or Data Infrastructure on building Data Engineering Tools and Frameworks\\nExpert knowledge of relational databases and query authoring (SQL).\\nStrong experience developing in one of the following - Python, Java, Scala\\nExperience with building and managing data pipelines\\nExperience with big data technology, e.g: Airflow, Spark, Google Cloud Composer\\n\\nPreferred Qualifications\\nDemonstrated experience developing machine learning algorithms in the healthcare field\\nExperience extracting and correlating medical findings from unstructured data such as PDF charts\\nExperience with big data analysis tools such as BigQuery and Looker\",\n",
       " 'Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.\\nThis opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.\\nSoftware Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.\\nJob Responsibilities:\\nSolve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review\\nResponsible for implementing diverse engineering tasks including gap, data, and impact analysis\\nIndependently perform low-level design configuration, code customization, refactoring and review\\nGain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle\\nEnsure adequate documentation and provide assistance as a reliable engineer to all stakeholders\\nSpecialization:\\nwe work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.\\nWe work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.\\nWe have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.\\nJob Type: Full-time\\nSalary: $110,000.00 - $125,000.00 per year\\nExperience level:\\n10 years\\nSchedule:\\n8 hour shift\\nExperience:\\nInformatica: 1 year (Preferred)\\nSQL: 1 year (Preferred)\\nData Engineer: 10 years (Preferred)\\nSnowflake: 7 years (Preferred)\\nWork Location: On the road\\nSpeak with the employer\\n+91 727 216 7642',\n",
       " 'Specific Duties:\\nDevelop, test, and maintain extraction, transformation, and load (ETL) processes.\\n\\nDevelop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.\\n\\nSupport the Data Management Project team to develop and maintain data quality controls.\\n\\nSupport the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.\\n\\nSupport business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.\\n\\nSupport the data stewards to troubleshoot and resolve data issues.\\n\\nSupport business users to obtain requirements for enhancements and/or new analytic assets.\\n\\nAssist in the Development of data asset training and documentation.\\n\\nParticipate in the development and implementation of a DOES data standard.\\n\\nParticipate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.',\n",
       " 'Sr. Data Engineer\\nLocation: Must be local to Atlanta Metro area, will not consider relocations at this time.\\nHybrid Remote, a few days in office in Alpharetta, GA.\\nContract Length: 6-12 month contract to full time.\\nSummary\\nOur Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.\\nRequired Experience\\nExperience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.\\nSkilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.\\nThis candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.\\nPreferred Experience\\nDesigning and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks\\nShow efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.\\nIntegrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.\\nExcellence using standard methodologies\\nComfortable using PySpark APIs to perform advanced data transformations\\nFamiliarity with implementing classes with Python.\\nJob Types: Full-time, Contract\\nPay: $75.00 - $85.00 per hour\\nExperience level:\\n7 years\\nSchedule:\\nMonday to Friday\\nApplication Question(s):\\nHow many years of experience do you have as a Data Engineer?\\nHow many years of experience do you have building data pipelines with Spark or Databricks?\\nCan you work on W2 without sponsorship? No C2C engagements for this position.\\nAre you currently located in Atlanta Metropolitan area?\\nExperience:\\nPython: 4 years (Preferred)\\nSpark: 3 years (Preferred)\\nAWS: 3 years (Preferred)\\nWork Location: One location',\n",
       " 'Sr. Data Engineer - Alpharetta GA\\nLocation: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.\\nHybrid Remote, a few days in office in Alpharetta, GA.\\nContract Length: 6-12 month contract to full time.\\nSummary\\nOur Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.\\nRequired Experience\\nExperience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.\\nSkilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.\\nThis candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.\\nPreferred Experience\\nDesigning and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks\\nShow efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.\\nIntegrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.\\nExcellence using standard methodologies\\nComfortable using PySpark APIs to perform advanced data transformations\\nFamiliarity with implementing classes with Python.\\nJob Types: Full-time, Contract\\nPay: $70.00 - $84.00 per hour\\nSchedule:\\n8 hour shift\\nEvening shift\\nAbility to commute/relocate:\\nAlpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)\\nExperience:\\nInformatica: 8 years (Preferred)\\nSQL: 8 years (Preferred)\\nData warehouse: 8 years (Preferred)\\nWork Location: Hybrid remote in Alpharetta, GA 30005',\n",
       " 'Summary\\nPosted: Dec 22, 2021\\nWeekly Hours: 40\\nRole Number:200327520\\nAs part of our Video Engineering group, you’ll help deliver creative solutions to various problems that could impact the people all over the world. This Data Engineer will work closely with other members of the Video Engineering group to mine data, implement model evaluation pipeline, analyze large scale data, visualize data, and ensure the delivery is of the highest quality. This position will also require strong coding skills, presentation skills, and collaborating with multiple teams (ex: machine learning, cloud infrastructure support).\\nKey Qualifications\\nA curious mind\\nAn obsession for quality\\nBackground in Data science, Data mining, Multivariate statistics, Computer vision, Machine learning\\nExperience working with large scale data sets\\nSolid programming skills including:\\nPython\\nC/C++\\nExperience with data visualization and presentation, familiar with data analysis tools such as Tableau\\nExcellent problem solving and communication skills\\nDescription\\nThe responsibilities of this position includes the following for current and future products: - Implement algorithm evaluation methods - Analyze data and build data analysis tools - Deep-dive failure analysis - Discover new perspectives for old data - Produce / Present meaningful data visualization to higher-ups and across various involved teams\\nEducation & Experience\\nMasters in Computer Science or relevant experience\\nAdditional Requirements\\nPay & Benefits\\nAt Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $50.72 and $76.44/hr, and your base pay will depend on your skills, qualifications, experience, and location.\\n\\nApple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.\\n\\nNote: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.',\n",
       " 'The Data Engineer is responsible for the development, integration, and implementation of ETL Feeds in support of various modern architecture projects. This role requires hands-on knowledge of modern Integrations Platform solutions, file, and database ETL, and XML transformation. The incumbent will act as a technical resource, participating in the successful design, development, unit testing, implementation, and support of new and existing ETL integrations.\\nEssential Functions\\nMust be able to acquire and interpret business requirements, create technical artifacts, and determine the most efficient/appropriate solution design.\\nWorking with manager, gathering business requirements for the development of new applications.\\nPerform programming assignments based on established standards, methods, and best practices.\\nDevelop ETL solutions as assigned in supporting new or updated business solutions and requirements.\\nHelp business users develop functional requirements for integration.\\nProvide support during user acceptance testing.\\nPrepare documentation for code changes.\\nCode ETL feeds, and conduct tests to verify correct program functionality, reliability, and data integrity.\\nMaintain and modify ETL feeds; make approved changes by amending application documentation, developing detailed programming logic, and coding changes.\\nProvide support and resolution for application problems and issues.\\nProvide user and system documentation for operational and technical support.\\nAssist in the discovery and investigation of production problems as required.\\nSupport ongoing daily use of ETL environments and Integrations platforms.\\nProvide on-call support for critical application problems and issues.\\nAnalyze program performance and take action to correct deficiencies based on consultation with clients and approval of supervisor.\\nrequired.Work outside the standard office 7.5 hour workday may be\\nExperience Requirements\\n2+ years of IT experience.\\n1+ years of experience building ETL feeds using any ETL tool (Talend, Informatica, etc.)\\nKnowledge of the entire software development lifecycle i.e., requirements, design, implementation, integration testing, deployment, and post-production support.\\nUnderstanding of XML, JSON, XSD, WSDL, REST, and SOAP APIs.\\nExperience in developing SQL programs.\\nJob Type: Full-time\\nPay: From $80,000.00 per year\\nBenefits:\\n401(k)\\nDental insurance\\nHealth insurance\\nPaid time off\\nVision insurance\\nSchedule:\\n8 hour shift\\nSupplemental pay types:\\nBonus pay\\nWork Location: Hybrid remote in Cleveland, OH 44115',\n",
       " \"The Data Engineer must be able to meet the key criteria below:\\nLocation: 100% telework\\nYears' Experience: Mid and Senior level openings\\nEducation: Bachelor’s in IT related field\\nClearance: Must be able to obtain and maintain a Public Trust clearance\\nWork Authorization: Must be a US Citizen\\nEmployment Type: Full-Time, W-2\\nKey Skills:\\nExperience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required\\nETL experience required\\nExperience modeling solutions and deploying production enterprise applications in AWS required\\nAI/ML knowledge a bonus\\nOverview\\nDo you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.\\nResponsibilities\\nDesign, implement and maintain data architecture and performing data analytics\\nImplement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data\\nIncorporate a variety of statistical and machine learning techniques in your projects\\nWrite programs to cleanse and integrate data in an efficient and reusable manner\\nUse leading edge and open-source tools such as Python and our AI application suites\\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors\\nCommunicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions\\nEvaluate modeling results and communicate the results to technical and non-technical audiences\\nBuild reusable code and libraries for future use\\nEnsure the technical feasibility of UI/UX designs\\nCreate work estimates and meet project deadlines\\nParticipate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation\\nTranslate application storyboards, requirements docs and use cases into functional applications\\nWork both independently and collaboratively with business and development team to create great user experiences for our customers\\nTroubleshoot and resolve any bugs assigned.\\nQualifications\\nEnterprise data architecture and management experience\\nExperience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling\\nExperience modeling solutions in AWS.AI/ML experience a bonus\\nExperience deploying production enterprise applications in AWS.\\nExperience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment\\nMust be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices\\nExperience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support\\nExperience working with large scale database requirements supporting diverse data types\\nExperience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices\\nDebug, troubleshoot, design and implement solutions to complex technical issues\\nStrong technical and analytical abilities\\nBasic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau\\nProficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs\\nBasic understanding of Cloud (AWS, Azure, etc.)\\nAbility to thrive in a team-based environment\\nHands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;\\nExcellent work experience with R/Python scripting language;\\nExperience with UNIX; and text manipulation using vi or similar text editors.\\nKnowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.\\nExpertise in Agile and DevSecOps approaches\\nExperience with AWS Cloud (ie. EC2, S3, Redshift)\\nExperience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management\\nAbout IBR\\nImagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:\\nNationwide medical, dental, and vision insurance\\n3 weeks of paid time off and 11 paid federal holidays\\n401k matching\\nLife insurance, short term disability and long term disability at no cost to our employees\\nHealth Care Flex Spending and Dependent Care Flex Spending accounts\\nTraining and education assistance opportunities\\nIBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.\\nLearn more at http://www.teamibr.com\\nIf alternative methods of assistance are needed with the application process, additional information has been provided below:\\n407.459.1830.\\nJob Type: Full-time\\nBenefits:\\n401(k)\\n401(k) matching\\nDental insurance\\nFlexible spending account\\nHealth insurance\\nLife insurance\\nPaid time off\\nTuition reimbursement\\nVision insurance\\nSchedule:\\n8 hour shift\\nWork Location: Remote\",\n",
       " 'The Sunwater Institute is a nonprofit, nonpartisan public policy organization focused on improving the foundations of our social systems. The Sunwater Institute’s current programs focus on improving congressional performance, preserving and advancing constitutional rights and liberties, strengthening and enlarging property rights, and exploring the foundations of group decision-making.\\nThe Sunwater Institute’s interdisciplinary, long-term orientation and focus on the building blocks of social systems sets it apart as a haven for bold, fresh ideas and free, constructive dialogue. The Sunwater Institute is located in North Bethesda and maintains a presence on Capitol Hill.\\nPOSITION\\nThe Sunwater Institute is seeking a Data Engineer who has a background in engineering or computer science and is highly skilled in SQL and database services technologies. The ideal candidate will possess strong problem-solving skills, excellent communication, and a great technical aptitude. The individual in this position will provide project support and help drive team initiatives. The candidate will be creative, and comfortable working in small teams and coordinating with US and non-US-based developers.\\nResponsibilities\\nDevelop data models to store and manage large amounts of structured and unstructured data\\nDesign efficient ETL processes to move and integrate data from different sources into the enterprise warehouse\\nWrite complex queries using SQL Server etc. to analyze large datasets\\nIdentify trends, patterns, and correlations within datasets by performing exploratory analysis\\nEnsure high quality of data by implementing best practices on storage, maintenance, indexing techniques, etc\\nCoordinate project status updates between teams and communicate deliverables and timelines\\nREQUIREMENTS\\nBachelor’s degree in Computer Science or Engineering related field\\n4+ years of relevant experience in a similar role\\nStrong knowledge of relational databases including SQL Server\\nProficient skills in writing complex queries using SQL language; developing stored procs, views, etc.\\nExperience with end-to-end ETL (Extract Transform Load) Processes\\nFamiliarity with Big Data platforms such as AWS etc.\\nAbility to develop effective reports that help drive business decisions\\nMust be able to work effectively with teams but be able to work independently on tasks to meet tight deadlines\\nExceptional communication skills verbal, written, and active listening\\nExtreme attention to detail and accuracy\\nJob Type: Full-time\\nBenefits:\\n401(k)\\n401(k) matching\\nDental insurance\\nHealth insurance\\nLife insurance\\nPaid time off\\nParental leave\\nVision insurance\\nCompensation package:\\nYearly bonus\\nYearly pay\\nExperience level:\\n4 years\\nSchedule:\\n8 hour shift\\nMonday to Friday\\nAbility to commute/relocate:\\nRockville, MD 20852: Reliably commute or planning to relocate before starting work (Required)\\nWork Location: One location',\n",
       " \"Title: Big Data Engineer\\nLocation: Irving TX (Hybrid)\\nExperience : 10 +\\nDuration: Long Term\\nResponsibilities:\\nDesign application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.\\nManage Hadoop, NoSQL, and/or MPP infrastructure supporting data.\\nWrite applications to solve analytical problems.\\nCode, test, release, and support Big Data.\\nHelp in the design and build of the data platform over Big Data technologies.\\nSolve big data engineering problems.\\nAnalyze, recommend, and implement data technologies for the platform.\\nInvolved in case studies about Big Data.\\nResponsible for efficient deliveries.\\nWork with others to propose the best technical solutions.\\nHelp design the best backend data warehouse platform to support the capacity and performance.\\nParticipate in the proof of concept application.\\nMUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP\\nDESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows\\nAmazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.\\nJob Types: Full-time, Contract\\nSalary: $60.00 - $65.00 per hour\\nBenefits:\\n401(k)\\nDental insurance\\nHealth insurance\\nPaid time off\\nExperience level:\\n10 years\\nSchedule:\\n8 hour shift\\nExperience:\\nRDBMS: 3 years (Preferred)\\nNoSQL: 1 year (Preferred)\\nAWS: 1 year (Preferred)\\nHadoop: 1 year (Preferred)\\nWillingness to travel:\\n25% (Preferred)\\nWork Location: On the road\",\n",
       " \"Title: Data Engineer\\nLocation: Permanent remote role\\nClearance: Not required: Start date not contingent on a having or completion of a clearance, however one could be offered upon starting for future programs\\nOverview:\\nIntelliBridge is seeking a Data Engineer to collaborate with technical and non-technical data and development team members to define requirements, mine and analyze data, integrate data from a variety of sources, and deploy high quality data pipelines in support of analytics that provide help ensure national security. You'll be able to gain experience in designing cloud architectures while providing critical support to the client's mission. You will be responsible for designing and building smart data pipelines that are secure, robust, and alerting. You will also create innovative ways to combine disparate data sources and build integrated datasets for advanced analytics.\\nAs a direct employee of IntelliBridge, you would receive a benefit package that includes health/dental/vision insurance coverage, 401K with company match, PTO & paid holidays, and annual tuition/training assistance. For more information, please visit our website.\\nResponsibilities/Duties:\\nBuild and maintain the infrastructure to support integration, extraction, transformation, and loading (ETL) of data from a wide variety of data sources, such as relational SQL and NoSQL databases, and other platform APIs\\nDesign data pipelines that are robust and secure including pipeline monitoring and alerting mechanisms\\nCreate innovative ways to orchestrate data ETL processes\\nGuide and support the implementation of new data engineering solutions to enable adoption and growth\\nIntegrate disparate data sources into powerful datasets for advanced analytics\\nRecommend tools and capabilities based on understanding the current environment and knowledge of various on-premises, cloud based, and hybrid capabilities/technologies\\nMonitor existing metrics, analyze data, and lead partnership with other Data and Analytics personnel to identify and implement system and process improvements\\nDevelop processes to convert aggregated data from teams, collection tools, and dashboards\\nConfigure and manage data analytic frameworks and pipelines using databases and tools\\nDevelop Python packages to improve application capabilities\\nApply distributed systems concepts and principles such as consistency and availability, liveness and safety, durability, reliability, fault-tolerance, consensus algorithms\\nAdministrate cloud computing and CI/CD pipelines to include Amazon Web Service (AWS)\\nInvestigate legacy code to determine areas of improvement and automation\\nRequired Qualifications:\\nExcellent verbal and written communications\\nBachelor’s Degree in a STEM filed or Master’s Degree in Operations Research, Industrial Engineering, Applied Mathematics, Statistics, Physics, Computer Science, or related fields\\n5+ years of experience with Python, SQL, Unix(Linux), and handling semi-structured data (JSON)\\n3+ years of experience with Elasticsearch, Logstash, and Kibana (ELK stack)\\n3+ years of experience with Amazon Web Services (AWS) or other cloud provider\\nProficient in Docker\\nProficient in Agile Development\\nProficient in Git Operations\\nExperience understanding requirements, analyzing data, discovering opportunities, addressing gaps and communicating them to multiple individuals and stakeholders\\nDemonstrated expertise in technical data engineering on integrating complex applications, systems, software, and project activities and integrating them into cloud-based resources\\nGeneral knowledge in machine learning for building efficient and accurate data pipelines that occur for downstream users, such as for data scientists to create the models and analytics that produce insight\\nPreferred Qualifications:\\nOrganizational skills and a love of documentation\\nExperienced in Airflow\\nExperience with demonstrated strength in data lake/warehouse technical architecture, infrastructure components, and ETL/ELT pipelines\\nExperience with geo-spatial data\\nExperience with deployments via Kubernetes\\nExperience with configuring and aggregating logs for data analysis using Splunk or ELK solutions\\nExperience with developing and managing machine images or templates to automate cloud deployments\\nAbout Us:\\nIntelliBridge delivers IT strategy, cloud, cybersecurity, application, data and analytics, enterprise IT, intelligence analysis, and mission operation support services to accelerate technical performance and efficiency for Defense, Civilian, and National Security & Federal Law Enforcement clients.\",\n",
       " \"Today’s financial system is built to favor those with money. Grid’s mission is to level that playing field by building financial products that help users better manage their financial future. The Grid app lets users access cash, build credit, spend money, file their taxes and lots, lots more.\\n\\nGrid is a fast growing team that’s deeply passionate about making a difference in the lives of millions. We’re solving huge problems and believe that a merit driven culture allows every team member to play a big role. Join our growing team in our Bay Area headquarters or additional offices across the US.\\n\\nOur Data Engineers are responsible for converting the data surrounding a customer's financial life into tools that make our customers smarter and better equipped financially for the future. We value making intelligent decisions backed by good data and tools. We're looking for people who share our values, particularly if you have experience analyzing, processing, and learning from large data sets.\\n\\nWe’re looking for a seasoned data engineer who can help us lay the foundation of an exceptional data engineering practice. The ideal candidate will be confident with a programming language of their choice, be able to learn new technologies quickly, have strong software engineering and computer science fundamentals and have extensive experience with common big data workflow frameworks and solutions. You will be writing code, setting style guides and collaborating cross-functionally with product, engineering and leadership.\\nProblems we work on:\\nAnalytics: Collect all the data for a user into tools that help our customers\\nMachine Learning: Using a variety of techniques to reach better insights\\nData Processing: Managing data & statistics using scalable and efficient technologies\\nVisualization: Envision our data as beautiful graphs and tools that allow customers to explore their data & ask their own questions\\nRisk: Analyze data for anomalous patterns and build tools that allow us to find bad actors quickly\\nWe Practice:\\nOpen collaboration\\nCode reviews\\nTesting\\nAgile development\\nWe Use:\\nGo\\nPython\\nMySQL\\nGoogle Cloud Platform\\nKubernetes\\nKubeflow\\nDocker\\nGoogle Pubsub\\nBigQuery\\nFirebase\\nWe’re looking for Engineers to:\\nDesign and implement platform services, frameworks and ecosystems\\nBuild a scalable, reliable, operable and performant big data workflow platform for data scientists/engineers, AI/ML engineers, and product/operation team members\\nDrive efficiency and reliability improvements through design and automation: performance, scaling, observability, and monitoring\\nRequirements:\\nStrong programming skills with Python\\nStrong programming skills with a typed programming language, such as Java, Scala, Go, etc.\\nDisciplined approach to development, testing, and quality assurance\\nExcellent communication skills, capable of explaining highly technical problems in English\\nUnderstand data processing and ETL, hands on experience building pipelines and using frameworks such as Hive hdfs, Presto, Spark, etc.\\nReally strong candidates may have:\\nReally strong candidates may have:\\nActively contribute to open source software\\nWorked with a strong, lean-based development environment\\nPrevious work experience in a start-up environment\\nAbility to recognize the right tool for the right situation/problem.\\nWill have strong programming skill in Go\",\n",
       " 'Top skills:\\nConfluent or Apache Kafka (ability to ingest data into snowflake)\\nSnowflake is preferred, almost a must\\nPython for automation\\nSQL server or other DB knowledge\\nPreferred:\\nPublic cloud knowledge, specifically Azure\\nETL tools\\nResponsibilities:\\nBuild constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.\\nDirect deployment of enterprise data strategy vision through successful program delivery\\nCollaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability\\nEffectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.\\nIncorporate best practices and standards in metadata and report/dashboard development\\nTranslates highly complex concepts in ways that can be understood by a variety of audiences\\nFunction as a mentor for other team members, providing assistance where needed\\nJob Types: Full-time, Contract\\nSalary: $20.00 - $50.00 per hour\\nSchedule:\\nMonday to Friday\\nWork Location: Hybrid remote in Richville, MN 56576',\n",
       " \"Tripoint Solutions is seeking a Data Engineer to join our team.\\nThe Data Engineer will be part of a team responsible for ensuring the success of a highly visible, results-driven federal client through the development of a cloud-based next generation system.\\nThis position requires the applicant to parse disparate data sources, including structured and unstructured elements, to find the patterns and meaning in large quantities of data. The successful candidate will leverage machine learning as well as best of breed pipeline technology to process and store a variety of data elements.\\nLocation: This position is eligible for fully remote work. Selected candidates living within a 25 miles radius of the NITAAC office in Rockville, MD will be required to come into the office once a week. The selected candidate must be currently located in, or willing to relocate to, a state supported by Tripoint Solutions corporate offices (AL, DC, FL, IL, LA, MD, MI, MN, MS, NJ, NC, PA, TN, TX, or VA).\\nThe successful candidate will be accountable to:\\nCreating and maintaining optimal data pipeline architecture.\\nAssembling large, complex data sets that meet functional / non-functional business requirements.\\nIdentifying, designing, and implementing internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\\nBuilding the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.\\nBuilding analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.\\nKeeping data separated and secure across national boundaries through multiple data centers and AWS regions.\\nStrong interest to learn and stay up to date on relevant technologies, trends, industry standards and identify new ones to implement.\\nWhat you bring\\nExperience, Education & Training:\\nBachelor's degree in computer science, Math, Analytics, Statistics, Informatics, Information Technology or equivalent quantitative field.\\n5 years of experience working in a Data Engineer or Data Scientist role.\\nExperience with cloud data services (AWS preferred).\\nExperience solutioning and applying Natural Language Processing (NLP) and or Machine Learning (ML) technologies\\nExperience building and optimizing ‘big data’ data pipelines, architectures and data sets.\\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\\nExperience with Microsoft SQL, database development and design.\\nExperience building processes supporting data integration, transformation, data structures, metadata, dependency and workload management.\\nDemonstrated success in manipulating, processing and extracting value from large disconnected data sets.\\nDemonstrated accomplishments in designing, coding, testing and supporting data analytics and reporting solutions in a cloud environment.\\nExperience with object-oriented/object function scripting languages: Python, Java\\nConcept experience; information retrieval, search engine, document data extraction\\nPreferred experience with AWS cloud services: Textract, Comprehend, GlueMaker, Athena, Notebook\\nWorking knowledge of message queueing, stream processing, and highly scalable ‘big data’ data stores.\\nClearance Requirements:\\nApplicants selected may be subject to a government security investigation and must meet eligibility requirements for potential access to classified information. Accordingly, US Citizenship or Green Card is required.\\nWhat we offer\\nAbout Tripoint Solutions\\nWe are technology innovators, partnered with state-of-the-art providers, such as AWS, ServiceNow, and UiPath, to drive digital transformation in the federal space. TPS teams are bringing automation and data science into areas of the government that are crying out for fresh tech—making positive impacts felt by tens of thousands of users, countless citizens, and all six branches of the military each day. Our Agile teams are responsible for envisioning, launching, and operating the massive data systems and analytics platforms used to manage $14.5B in government procurements and $200B in military real estate assets globally. At TPS, we apply the power of cloud technologies to help the government think smarter and function better—for everyone.\\nTPS Company Values\\nWe value and respect each employee's dedicated work and unique contributions; as they directly impact who we are and what we do.\\nYour talent and innovative thinking bring leading-edge solutions to our customers.\\nOur success is driven by the dedication of our employees.\\nEmployee-generated solutions have sustained our continued success and customer satisfaction\\nBenefit Offerings\\nTripoint Solutions builds flexibility into health benefit plan choices, covers most of the monthly premiums, and helps employees build a career with impact through our generous professional development program.\\nWe offer all full-time employees:\\nMedical, Dental, Vision benefits with a national provider network (company pays 100% of Vision and Dental premiums)\\nFlexible Spending and Health Savings Accounts (FSA & HSA)\\nCompany-paid Life and Disability insurance including Short-Term, Long-Term, and Accidental\\nPaid-time off (PTO), accruing with each year of service, up to 20 days, plus 11 paid holidays\\n401(k) Retirement Plan - No waiting period to contribute and company makes 3% contribution of eligible pay in addition to annual profit-sharing contribution option\\nEligibility to receive impact bonuses each quarter\\nReferral Program\\nProfessional Development Reimbursement Program to pursue undergraduate, graduate, training, and certifications\\nMonthly transportation, parking, and cell phone service reimbursement\\nCOVID-19 Related Information\\nTripoint Solutions does not have a vaccination mandate applicable to all employees. However, to protect the health and safety of its employees and to comply with customer requirements, Tripoint Solutions may require employees in certain positions to be fully vaccinated against COVID-19. Vaccination requirements will depend on the status of the federal contractor mandate and customer site requirements. Furthermore, remote work arrangements are subject to change based on customer site requirements.\\nTripoint Solutions is an Equal Opportunity Employer/Veterans/Disabled\\nJob Type: Full-time\\nPay: $145,000.00 - $155,000.00 per year\\nBenefits:\\n401(k)\\nDental insurance\\nEmployee assistance program\\nFlexible spending account\\nHealth insurance\\nHealth savings account\\nLife insurance\\nPaid time off\\nProfessional development assistance\\nReferral program\\nRetirement plan\\nTuition reimbursement\\nVision insurance\\nExperience level:\\n5 years\\nSchedule:\\nMonday to Friday\\nApplication Question(s):\\nWhat cloud services have you worked with?\\nDo you have experience with Machine Learning or NLP?\\nDoes the advertised salary align to your expectations?\\nUS citizenship or green card is required. Do you meet this requirement?\\nThis is a remote position (See description for details and requirements). Where are you located?\\nAre you willing to undergo a federal background check?\\nEducation:\\nBachelor's (Required)\\nExperience:\\ndata scientist or data engineer role: 5 years (Required)\\ncloud services: 2 years (Required)\\nMicrosoft SQL (development and design)?: 2 years (Required)\\noptimizing ‘big data’ pipelines, architectures and data sets: 2 years (Required)\\nAWS: 1 year (Preferred)\\nPython: 1 year (Required)\\nJava: 1 year (Required)\\nWork Location: Remote\",\n",
       " 'We are hiring for Training and placement opportunity for our client requirements.\\nThese are W2 roles.\\nLocation: Plainsboro, NJ\\nBelow are the benefits that you will get from us\\n- Training and mock interview support\\n- Placement assistant\\n- H1 Sponsorship\\n- STEM extension support\\n- Accommodation support for in class training\\n- Will provide you on job support\\nJob Types: Full-time, Contract\\nSalary: $70,000.00 - $75,000.00 per year\\nExperience level:\\n8 years\\nSchedule:\\n8 hour shift\\nAbility to commute/relocate:\\nPrinceton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)\\nExperience:\\nInformatica: 1 year (Preferred)\\nSQL: 1 year (Preferred)\\nData warehouse: 1 year (Preferred)\\nWork Location: One location',\n",
       " 'We are looking for a Data Engineer in Chicago, IL (Hybrid) for a direct-hire position.\\nJob Description:\\nPosition: Data Engineer - Centralized Data Science and Analytics (CDSA)\\nLocation: Chicago, IL (Hybrid)\\nDuration: Direct-hire position\\nClient: Direct Client\\nNote: This is a W2 direct-hire role. Looking for candidates who are open to work independently on W2.\\nRequirements:\\n· Experience building and optimizing \"big data\" data pipelines, architectures and data sets.\\n· Working knowledge of message queuing, stream processing and highly scalable \"big data\" data stores.\\n· Advanced working SQL knowledge and experience working with cloud and relational databases.\\n· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\\n· Experience building processes supporting data transformation, data structures, metadata, dependency and workload management.\\n· A successful history of manipulating, processing and extracting value from large, disconnected datasets.\\n· Experience using the following software/tools:\\n· Relational SQL and NoSQL databases, including Postgres.\\n· Data pipeline and workflow management tools.\\n· Azure cloud services.\\n· Object-oriented/object function scripting languages: Python, PySpark Java, C++, R/RStudio/RSpark.\\n· CI/CD systems.\\n· Strong understanding across cloud and infrastructure components (server, storage, network, data, and applications) and ability to deliver end to end cloud infrastructure, architectures, and designs.\\n· Knowledge and implementation of enterprise scale cloud security platforms and tooling.\\n· Experience with enterprise applications, solutions, and data center infrastructures.\\n· Bachelor\\'s degree in computer science or similar field; master\\'s degree a plus.\\n· Exceptional product, project and client management skills.\\n· Azure, AWS or any other cloud/data engineering certifications are preferred.\\nJob Type: Full-time\\nSchedule:\\n8 hour shift\\nDay shift\\nMonday to Friday\\nExperience:\\nBig data: 5 years (Required)\\nAdvanced SQL: 5 years (Required)\\nCloud: 3 years (Required)\\nCI/CD: 3 years (Preferred)\\nNoSQL: 2 years (Required)\\nWork Location: Hybrid remote in Chicago, IL 60606',\n",
       " 'We hope this Job meets your skills and expectations. If you are available and interested, please contact me at your earliest convenience. You will be working with a highly skilled team of IT Professionals in a high pace corporate environment. This opportunity will move quickly and candidates will be interviewed in the order they apply. Job Description/ Required Skills: (i) Strong hands-on programming experience in Python (ii) Hands-on experience of API development (from application / software engineering perspective) (iii) AWS Lambda and data streaming ingestion (Kinesis) (iv) AWS tech stack from data engineering stand-point\\nJob Types: Full-time, Contract, Permanent\\nSalary: $39.76 - $86.23 per hour\\nExperience level:\\n8 years\\nSchedule:\\n8 hour shift\\nExperience:\\nInformatica: 1 year (Preferred)\\nSQL: 1 year (Preferred)\\nData warehouse: 1 year (Preferred)\\nWork Location: Remote',\n",
       " \"We're the premier, remote accounting, tax, and consulting firm built exclusively to serve the craft brewing industry.\\nOur mission is to help craft breweries grow profits and build deep successful relationships. And our team is filled with expert, autonomous, adaptable, technology-driven high performers.\\nAre you up for the challenge?\\nWe're looking for a full-time, remote Junior Data Engineer to join our specialized team. The main objective of this role is to design, develop, implement, and improve both internal and external applications to support our brewery clients and team in accordance with the SBS Core Values.\\nAbout The Role\\nThis role will report to our Technology/Product Manager and is accountable for fulfilling the following responsibilities:\\nBuilding our data pipeline and analysis applications. A key aspect of the consulting service we provide to clients involves the collection, aggregation, analysis, modeling, and usage of financial data and benchmarks. We use this data both internally to develop and inform strategy, as well as externally through our Benchmarks Assessment (https://sbstandard.com/assessment/) and Compass analysis product (https://sbstandard.com/levelup-compass/). You'll be responsible for working with our team to build out our data pipeline for these tools, and progressively increasing our ability to aggregate, analyze, query, and feed back this data into our reporting, analysis, and consulting work. Platforms we're building with include: SQL, Airflow, Excel Visual Basic for Applications (VBA), Google Apps Script, Intuit/QuickBooks Online.\\nNew process and technology R&D. We're always looking for new opportunities to provide both our team and our clients access to additional tools that give them leverage, automate and streamline processes, and overall make work more efficient. Part of your time will be dedicated to researching, testing, and prototyping new tech and application options.\\nParticipate and contribute to the overall success of our team. Each week the team meets to share wins, progress, and knowledge, as well as identify and solve issues at multiple levels (company, team, individual). Your full participation in this process is critical to ensure that we are operating as a cohesive, high-performance unit.\\nAbout You\\nWe're looking for an individual who:\\nIs a problem solver through-and-through. Everywhere you look, you both (a) see problems to solve, and (b) see solutions and new ways of doing things that just haven't been done yet. You know how to think outside of the box, are willing to “go there” with new ideas and solutions that haven't been done before, and have the confidence to start building, testing, iterating, and making sh*t work.\\nIs a systems thinker. You understand both the big picture and how the functional components fit together, and have the ability to take a specific analysis outcome and generalize it to fit a wide range of scenarios through structure and sound system design.\\nCan fail fast, iterate, and learn. You're an independent, self-directed, learner who isn't afraid to “move fast and break stuff” knowing that failure is a prerequisite to success, ESPECIALLY in product development. You may not have traditional credentials, but what you do have is the ability to rapidly learn, adopt, test, and understand new languages, platforms, tools, and solutions.\\nIs a manager of one. Unlike working within a traditional firm, in this role you'll be in the driver's seat, managing your workflow and workload in order to meet the standard set of deliverables required for each client.\\nAbout Our Culture\\nWe're fully remote, with team members and clients located all across the U.S. and have developed our own unique culture we call The SBS Way, within which we operate, evaluate performance, and make decisions using our core values as a guide:\\nBe Antifragile. Everything we do, good or bad, makes us better. And every experience is an opportunity for learning and continuous improvement.\\nPlay The Long Game. We make decisions, to the best of our ability, in the long-term interest of our firm, our team, our clients, and our broader industry and community.\\nEmbrace Technology. We welcome new technologies with open arms, and are always exploring, testing, and implementing them in the interest of enhancing both our internal capabilities and our client's outcomes.\\nBuild and Trust The Process. Each member of the team is committed to building, following, and improving the processes we use to deliver exceptional results for our clients.\\nAct as A Team of Expert Knowledge Workers. We openly and willingly collaborate, communicate, and provide rapid, direct feedback in the interest of learning, improving and developing ourselves.\\nWorking At SBS\\nWhat it's like working at our firm:\\nHigh flexibility. We believe in the ability of our team to determine the best way to complete their work. We measure outputs, not inputs. We don't have time sheets. We don't track hours. We don't pay attention to when and where our team works. Your schedule is yours to make.\\nHigh accountability. What we care about most is that we deliver on what we promise to our clients. In this respect, we measure and manage to our deliverable performance metrics and ensure each team member takes ownership over their accomplishment with a high level of quality that aligns with our core values\\nGreat pay for great work. We pay based on the characteristics that matter: position (and its market value), level of mastery, and longevity with the firm. All of which aim to ensure each member of the team feels they are compensated well and can focus on great work.\\nMerit-based career progression. We have clearly established career tracks, performance benchmarks, and mastery levels set for all of our core positions. How quickly you progress is entirely under your control, with a quarterly review and bi-annual promotion consideration cycle in place to evaluate your progress.\\nGenerous benefits. We offer a generous benefits package that includes medical, dental, and vision insurance enrollment; as well as an IRA match, tech stipend, 3 weeks of paid time off, and entry into our profit share bonus program after two years of service.\\nPersonal and and team development. In addition to our overall continuous learning focus, we also provide support for personal development in the form of expense coverage for continuing education (books, courses, training, certifications, etc.) as well as experiential learning (brewery visits, industry events and conferences, etc.). Each year we also meet in person for an all-expenses-paid annual retreat as a team. No work. Lots of fun. Lots of client beer.\\nJob Requirements\\nThe following basic requirements must be met:\\nPrevious experience in SQL development and database management.\\nPrevious experience building useful applications in scripting languages like VBA, Google Apps Script, Python, PHP, etc.\\nCan do effective cross-functional work in a remote environment.\\nHave crystal clear professional written and verbal communication skills.\\nHave exacting organizational standards and a calm and friendly attitude.\\nAvailable and responsive during normal business hours (9am-5pm Eastern Time, Monday-Friday).\\nHave a strong, consistent internet connection and a work environment conducive to video calls.\\nPreferred qualifications include:\\nDirect previous experience building data pipelines.\\nDirect previous experience building Airflow workflows and applications.\\nExperience building out and managing API connections.\\nExperience working with Quickbooks Online or similar accounting or finance platforms.\\nExperience using Podio or similar remote project management tools (e.g. Trello, Asana, etc.).\\nNext Steps\\nIf the position, culture, values, and mission at Small Batch Standard sound like they're the right fit for you, please apply here.\",\n",
       " \"What will you do:\\nIdentify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automate manual processes\\nOptimize data retrieval and develop dashboards, reports, and other visualizations for stakeholders\\nAlign architecture with business requirements, develop algorithms to transform data into useful, actionable information\\nDeploy sophisticated analytics programs, machine learning, and statistical methods\\nEnsure compliance with data governance and security policies\\nBuilding analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition\\nWorks closely with a team of frontend and backend engineers, product managers, and external clients.\\n\\nRequirements:\\nHave at least 5+ years of relevant experiences\\nBachelor or master degree in IT, CS, Engineering, Data Science or related fields\\nWorking knowledge of data tracking/analytics/visualization tools such as Google Analytics, Tag manager, Mixpanel, Tableau, etc. Familiar with CRM integration, A/B testing\\nKnowledge of SQL, noSQL, MongoDB, AWS, Azure, Python, Java\\nHave working experiences in e-commence, travel, marketing domain is a plus\\nKnowledge of best practices and IT operations in an always-up, always-available service\\nExperience with or knowledge of Agile Software Development methodologies\\nExcellent problem solving and troubleshooting skills\\nProcess oriented with great documentation skills\\nExcellent oral and written communication skills with a keen sense of customer service\\n\\n\\nAbout Globaleur:\\n\\nWe're a diverse team of thinkers and avid travelers, constantly reimagining how to better help worldwide travelers fully maximize their travel experience. The people who work for our company all share the same passion for innovation that goes into our platform and customer-centric products and services - revolutionizing how global travelers consume travel-related goods and services. Globaleur’s vision is to become the go-to platform that will help users in navigating worldwide destinations with ease, thereby building a more unified world where everyone feels comfortable visiting, communicating and exchanging cultural values with those who are from other parts of the world.\",\n",
       " \"What will you do:\\nIdentify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automate manual processes.\\nOptimize data retrieval and develop dashboards, reports, and other visualizations for stakeholders.\\nAlign architecture with business requirements, develop algorithms to transform data into useful, actionable information.\\nDeploy sophisticated analytics programs, machine learning, and statistical methods.\\nEnsure compliance with data governance and security policies.\\nBuilding analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition.\\nWorks closely with a team of frontend and backend engineers, product managers, and external clients.\\n\\nRequirements:\\nHave at least 5+ years of relevant experiences.\\nBachelor or master degree in IT, CS, Engineering, Data Science or related fields.\\nWorking knowledge of data tracking/analytics/visualization tools such as Google Analytics, Tag manager, Mixpanel, Tableau, etc. Familiar with CRM integration, A/B testing.\\nKnowledge of SQL, noSQL, MongoDB, AWS, Azure, Python, Java.\\nHave working experiences in e-commence, travel, marketing domain is a plus.\\nKnowledge of best practices and IT operations in an always-up, always-available service\\nExperience with or knowledge of Agile Software Development methodologies.\\nExcellent problem solving and troubleshooting skills.\\nProcess oriented with great documentation skills.\\nExcellent oral and written communication skills with a keen sense of customer service.\\n\\n\\nAbout Globaleur:\\n\\nWe're a diverse team of thinkers and avid travelers, constantly reimagining how to better help worldwide travelers fully maximize their travel experience. The people who work for our company all share the same passion for innovation that goes into our platform and customer-centric products and services - revolutionizing how global travelers consume travel-related goods and services. Globaleur’s vision is to become the go-to platform that will help users in navigating worldwide destinations with ease, thereby building a more unified world where everyone feels comfortable visiting, communicating and exchanging cultural values with those who are from other parts of the world.\",\n",
       " 'Who We Are\\nFor more than 20 years, we have been working with organizations large and small to help solve business challenges through technology. We bring a unique combination of engineering and strategy to Make Data Work for organizations.\\nOur clients range from the travel and leisure industry to publishing, retail and banking. The common thread between our clients is their commitment to making data work as seen through their investment in those efforts.\\nIn our quest to solve data challenges for our clients, we work with large enterprise, cloud based and marketing technology suites. We have a deep understanding of these solutions so we can help our clients make the most of their investment in an efficient way to have a data driven business.\\nWhy Work at Softcrylic?\\nSoftcrylic provides an engaging, team-focused, and rewarding work environment where people are excited about the work they do and passionate about delivering creative solutions to our clients.\\nWe are looking to add a Senior Data Engineer to our team! This is a 100% Remote role and preference will be given to candidates from Atlanta, NJ or Texas regions.\\nJob Description:\\nSoftcrylic is looking for a Senior Data Engineerwith strong design, development, and team leadership skills. The person should be working with Clients / Customer and with our internal (onshore and offshore) members to design, develop and rollout data projects. The person to be hands on and have strong leadership/communication and interpersonal skills.\\nRequirement:\\n· 5 to 7 years of experience in working as a Data Engineer.\\n· Strong experience in Python.\\n· Experience in working on GCP.\\n· Good experience in Airflow.\\n· Should have good experience in ETL pipeline design and development.\\n· Very good experience in SQL\\n· Experience in working on Redshift.\\n· Excellent designing and documentation (diagrams) and presentation skills.\\n· Data Quality Concepts are must have.\\n· Must know design and development of any of industry leading graph databases.\\n· Good communication skills.\\n· Independent thinker, good team player with Data Engineering Design skills.\\n· Work with minimum guidelines.\\nPlus:\\nGCP - Big Query\\nAgile background\\nMicrosoft Power BI\\nGraph Database\\nJob Types: Full-time, Contract\\nPay: $130,000.00 - $140,000.00 per year\\nBenefits:\\n401(k)\\nDental insurance\\nHealth insurance\\nLife insurance\\nPaid time off\\nCompensation package:\\nPerformance bonus\\nExperience level:\\n9 years\\nSchedule:\\nMonday to Friday\\nExperience:\\nData Engineer: 5 years (Preferred)\\nGCP: 3 years (Preferred)\\nPython: 4 years (Preferred)\\nWork Location: Remote\\nSpeak with the employer\\n+91 609.241.9641',\n",
       " \"Who we are:\\nWe (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.\\nWe're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.\\nJob Responsibilities\\nClient & Project Management:\\nScoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.\\nCommunicating with clients. Communicating technical details to clients, client progress reviews, etc.\\nData wrangling, data pipelining, and data modeling:\\nWorking with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium\\nWill also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)\\nSQL Database management and performance tuning\\nAutomation of data processing:\\nUsing AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.\\nUsing Docker containers to automate deployment.\\nSetting up error logging, tracking, and fault tolerance for deployed data pipelines.\\nMachine learning or predictive analytics\\nBe able to code, deploy, and monitor models for accuracy and business results.\\nWe work a lot with OpenAI, experience here is a plus\\nCommunicate verbally and written to internal team and clients\\nExcellent English skills are a must\\nMeeting with clients to gather abstract business requirements and turn them into a technical project plan\\nCreate, manage, and execute plans according to client timelines (some project mgmt. training will be provided)\\nManage and communicate client expectations, deadlines, and deliverables\\nAssist in developing detailed requirements for scoping of projects\\nReviewing code, Dashboards, and data to ensure quality work and valid data\\nPlease be prepared to answer the following questions before the interview:\\nPlease tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.\\nPlease tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.\\nDesired Skills:\\nStrong internal team and external client communication and management\\nStrong coding knowledge (Python)\\nStrong in SQL, Database Architecture, Database Optimization\\nStrong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers\\nExcellent communication abilities, both written and verbal\\nAbility to learn and adapt quickly to new technologies\\nExcellent organizational skills with strong attention to detail\\nExperience with Tableau or PowerBI (Nice to have)\\nGreat things about our company:\\nThis job is 100% remote.\\nHours are flexible, but you should generally be available during working hours to interface with the team and clients.\\nYou’ll receive world-class training to help you in your job, as well as ongoing mentorship.\\nYou’ll be part of a fun, collaborative team.\\nYou’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.\\nHealth Benefit options and 401k are available\\nJob Type: Full-time\\nJob Type: Full-time\\nPay: $50.00 - $70.00 per hour\\nBenefits:\\n401(k)\\nFlexible schedule\\nHealth insurance\\nHealth savings account\\nSchedule:\\nMonday to Friday\\nApplication Question(s):\\nPlease tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.\\nPlease tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.\\nWork Location: Remote\",\n",
       " '· 5+ Years’ experience in Data Engineering\\n· 1 year experience with Nexla or similar data ingestion & engineering tools\\n· 4+ years’ experience in SQL/SnowSQL\\n· 2+ Years’ thorough experience in Python coding and packages\\n· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.\\n· 2+ years’ experience in large volume File processing with different types and formats\\n· Experience in Orchestration Tools - Airflow\\n· Understanding of dimensional and relational data modeling.\\n· Understanding of BI/DW development methodologies.\\nJob Types: Full-time, Contract\\nSalary: $42.96 - $60.84 per hour\\nSchedule:\\n8 hour shift\\nDay shift\\nAbility to commute/relocate:\\nDallas, TX: Reliably commute or planning to relocate before starting work (Required)\\nExperience:\\nInformatica: 1 year (Preferred)\\nSQL: 1 year (Preferred)\\nData warehouse: 1 year (Preferred)\\nWork Location: One location',\n",
       " '· Databricks (s.Python, sql)\\n· Candidates should be really strong in Python and SQL\\n· Azure Data Factory\\n· Synapse Analytics\\n· Parque and Delta table\\nJob Type: Full-time\\nSalary: $84,050.65 - $189,023.89 per year\\nSchedule:\\n8 hour shift\\nWork Location: On the road']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(SalaryData['job_description'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bd0329",
   "metadata": {},
   "source": [
    "### List top skills by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02e63a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prog_languages = ['python', 'java', 'scala', 'go', 'r', 'c++', 'c#', 'sql', 'nosql', 'rust', 'shell']\n",
    "cloud_tools = ['aws', 'azure', 'google cloud', 'snowflake', 'databricks', 'redshift']\n",
    "viz_tools = ['power bi', 'tableau', 'excel', 'ssis', 'qlik', 'sap', 'looker']\n",
    "databases = ['sql server', 'postgresql', 'mongodb', 'mysql', 'oracle', 'casandra', 'elasticsearch', 'dynamodb', 'snowflake', 'redis', 'neo4j', 'hive', 'dbt']\n",
    "big_data = ['spark', 'hadoop', 'kafka', 'flink']\n",
    "devops = ['gitlab', 'terraform', 'docker', 'bash', 'ansible']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8301ce",
   "metadata": {},
   "source": [
    "### Match keywords in each skill list and create columns for each skill type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0b89699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(description, keywords):\n",
    "    pattern = r'\\b(?:{})\\b'.format('|'.join(map(re.escape, keywords)))\n",
    "    matches = set(re.findall(pattern, description.lower(), flags=re.IGNORECASE))\n",
    "    \n",
    "    return list(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f16db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SalaryData['job_languages'] = SalaryData['job_description'].apply(lambda x: extract_keywords(x, prog_languages))\n",
    "SalaryData['job_cloud'] = SalaryData['job_description'].apply(lambda x: extract_keywords(x, cloud_tools))\n",
    "SalaryData['job_viz'] = SalaryData['job_description'].apply(lambda x: extract_keywords(x, viz_tools))\n",
    "SalaryData['job_databases'] = SalaryData['job_description'].apply(lambda x: extract_keywords(x, databases))\n",
    "SalaryData['job_bigdata'] = SalaryData['job_description'].apply(lambda x: extract_keywords(x, big_data))\n",
    "SalaryData['job_devops'] = SalaryData['job_description'].apply(lambda x: extract_keywords(x, devops))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15b2050",
   "metadata": {},
   "source": [
    "## 5. Extract Education Level From Job Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a22d04b",
   "metadata": {},
   "source": [
    "### List education levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a9c14f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "education = ['associate', 'bachelor', 'master', 'phd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a14e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if matches multiple degree keywords, count only the lowest degree\n",
    "\n",
    "def extract_degree(description, degrees):\n",
    "    pattern = r'\\b(?:{})\\b'.format('|'.join(map(re.escape, degrees)))\n",
    "    matches = re.findall(pattern, description.lower(), flags=re.IGNORECASE)\n",
    "    \n",
    "    if 'bachelor' in matches:\n",
    "        return 'bachelor'\n",
    "    elif 'master' in matches:\n",
    "        return 'master'\n",
    "    elif 'phd' in matches:\n",
    "        return 'phd'\n",
    "    \n",
    "    return None   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ac6d3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bachelor    253\n",
       "master       34\n",
       "Name: job_education, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SalaryData['job_education'] = SalaryData['job_description'].apply(lambda x:extract_degree(x, education))\n",
    "SalaryData['job_education'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45c91b9",
   "metadata": {},
   "source": [
    "### Check if there are other ways specifying education level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "590496dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "· Degree in Computer Science, IT, or similar field (STEM)\n",
      "Bachelor’s Degree in Computer Science, IT, or related field\n",
      "Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree\n",
      "Bachelor's degree in Computer Science, Mathematics, Statistics or related experience\n",
      "Applicant must have a Bachelor’s degree in Computer Science, Information Systems, or Information Technology and 5 years of progressive post-baccalaureate experience in the job offered or a related occupation\n",
      "EDUCATION: Bachelors Degree in Mathematics/Statistics/Technology/Science/Engineering/Applied Mathematics or related field\n",
      "BS/MS degree in Computer Science or equivalent proven experience\n",
      "Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent\n",
      "(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e\n",
      "Bachelors Degree\n",
      "Bachelor's degree in computer science, Math, Analytics, Statistics, Informatics, Information Technology or equivalent quantitative field\n",
      "Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required\n",
      " Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred\n",
      "Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred\n",
      "Bachelor’s degree in computer science, information systems, math, engineering, or other technical field, or equivalent experience\n",
      "· Bachelor's degree in computer science or similar field; master's degree a plus\n",
      "Bachelor's degree in Computer Science, Electrical/Computer Engineering, Statistics, Mathematics, or other related fields with 5 years of experience OR Master's degree in Data Science or Data Engineering with 3 years of experience\n",
      "Bachelor’s Degree in a STEM filed or Master’s Degree in Operations Research, Industrial Engineering, Applied Mathematics, Statistics, Physics, Computer Science, or related fields\n",
      "Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience\n",
      "· Bachelor’s of Science degree from an accredited university or college in chemistry, physics, mathematics or computer science\n",
      "Bachelor's degree in Computer Science, Mathematics or related technical field\n",
      "Bachelor's degree in computer engineering or computer science\n",
      "Bachelor or master degree in IT, CS, Engineering, Data Science or related fields\n",
      "Bachelor's degree in Computer Science, Engineering, or a related field Master’s degree preferred\n",
      "Degree in Computer Science or related domains\n",
      "Bachelor’s degree in computer science or engineering or equivalent work experience\n",
      "Bachelor’s Degree in Computer Science or a related discipline\n",
      "Bachelor’s Degree\n",
      "High degree of organization and ability to manage multiple, competing projects and priorities simultaneously\n",
      "Bachelor’s degree in an Analytical field (Business, Marketing) required\n",
      "Minimum of a bachelor's degree in data science (DS), data engineering (DE), computer science (CS), software engineering (SE), computer engineering (CE), electrical engineering (EE), mathematics, information technology (IT), information systems (ISYS) or a related field\n",
      "Bachelor's Degree or higher in an Information Technology discipline or related field of study and minimum of two years of work experience designing, programming, and supporting software programs or applications\n",
      "Bachelor’s degree in Computer Science or Engineering related field\n",
      "Bachelor’s degree in a field of Engineering ME or EE\n",
      "Bachelors degree or equivalent experience required\n",
      "Bachelor's degree in Computer Science, Information Systems, Engineering or equivalent\n",
      "Education: Bachelor’s degree in Computer Science, Engineering, Software Engineering, or related field; Master’s degree preferred\n",
      "Bachelor’s degree in computer science, information technology, engineering, or related discipline\n",
      "Bachelor’s degree in Computer Science, Mathematics, Engineering, or in a related field\n",
      "BS or MS degree in Computer Science or a related technical field\n",
      "2+ years in data engineering with a Bachelor's degree in CS, Data Science, or similar technical field, or equivalent professional experience\n"
     ]
    }
   ],
   "source": [
    "# Function to split text into sentences and find the sentence containing \"degree\"\n",
    "def extract_degree_sentence(text):\n",
    "    sentences = re.split(r'[\\n.]+', text)  # Split text by \"\\n\" and \".\"\n",
    "    degree_sentence = next((sentence for sentence in sentences if re.search(r'\\bdegree\\b', sentence, re.IGNORECASE)), None)\n",
    "    \n",
    "    return degree_sentence\n",
    "\n",
    "# Apply the function to the 'job_description' column\n",
    "SalaryData['degree_sentence'] = SalaryData['job_description'].apply(extract_degree_sentence)\n",
    "\n",
    "# Print the unique sentence containing \"degree\"\n",
    "degree_sentence = SalaryData['degree_sentence'].dropna().unique()\n",
    "for sentence in degree_sentence:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba33afe",
   "metadata": {},
   "source": [
    "### To capture more education requirement, add BS/MS and \"degree in\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90724782",
   "metadata": {},
   "outputs": [],
   "source": [
    "education = ['associate', 'bs', 'bachelor', 'ms', 'master', 'phd', 'degree in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "192c482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if matches multiple degree keywords, count only the lowest degree\n",
    "\n",
    "def extract_degree_2(description, degrees):\n",
    "    pattern = r'\\b(?:{})\\b'.format('|'.join(map(re.escape, degrees)))\n",
    "    matches = re.findall(pattern, description.lower(), flags=re.IGNORECASE)\n",
    "    \n",
    "    # if degree is mentioned but level is not specified, default bachelor\n",
    "    if 'bs' in matches or 'bachelor' in matches or 'degree in' in matches:\n",
    "        return 'bachelor'\n",
    "    elif 'ms' in matches or 'master' in matches:\n",
    "        return 'master'\n",
    "    elif 'phd' in matches:\n",
    "        return 'phd'\n",
    "    \n",
    "    return None   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58c43d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bachelor    287\n",
       "master       35\n",
       "Name: job_education, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SalaryData['job_education'] = SalaryData['job_description'].apply(lambda x:extract_degree_2(x, education))\n",
    "SalaryData['job_education'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5965946f",
   "metadata": {},
   "source": [
    "## 6. Categorize Experience Level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686ed6b9",
   "metadata": {},
   "source": [
    "### Check descriptions with keyword \"year\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08984202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 per year\n",
      "MDM (Master Data Management): 1 year (Required)\n",
      " The base pay for this position ranges from $105,700/year in our lowest geographic market up to $205,600/year in our highest geographic market\n",
      "NoSQL: 1 year (Preferred)\n",
      "3+ year’s working with multiple Big Data file formats (Parquet, Avro, Delta Lake)\n",
      " Each year we also meet in person for an all-expenses-paid annual retreat as a team\n",
      "Paid-time off (PTO), accruing with each year of service, up to 20 days, plus 11 paid holidays\n",
      "Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth\n",
      "1+ year experience working with Azure analytical stack\n",
      "12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse\n",
      " This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal\n",
      "Informatica: 1 year (Preferred)\n",
      "Databricks: 1 year (Required)\n",
      "89 per year\n",
      "3+ year experience in Data Engineering or ETL Development\n",
      " Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace\n",
      "Data modeling: 1 year (Preferred)\n",
      "At least 1 year of experience in data technologies\n",
      "More than 2 years hands on experience on Azure Services like Databricks, Azure Data Factory, Azure Storage Accounts & Azure Data Lakes and 1 year of experience working with Cosmos DB or any other document based noSQL databases (MongoDB)\n",
      "At least one year experience with unified data governance solution using MS Purview\n",
      "· 1 year experience with Nexla or similar data ingestion & engineering tools\n",
      "10+ Year experience in BI and Analytics\n",
      "18 per year\n",
      "Microsoft Excel: 1 year (Preferred)\n",
      "SQL: 1 year (Preferred)\n",
      " One year after joining, you’ll have been able to pay off an extra $600!\n",
      "Within 1 year, you’ll…\n",
      "analyzing data to discover opportunities and address gaps: 1 year (Preferred)\n"
     ]
    }
   ],
   "source": [
    "# Function to split text into sentences and find the sentence containing \"year\"\n",
    "def extract_exp_sentence(text):\n",
    "    sentences = re.split(r'[\\n.]+', text)  # Split text by \"\\n\" and \".\"\n",
    "    exp_sentence = next((sentence for sentence in sentences if re.search(r'\\byear\\b', sentence, re.IGNORECASE)), None)\n",
    "    \n",
    "    return exp_sentence\n",
    "\n",
    "# Apply the function to the 'job_description' column\n",
    "SalaryData['exp_sentence'] = SalaryData['job_description'].apply(extract_exp_sentence)\n",
    "\n",
    "# Print the unique sentence containing \"degree\"\n",
    "exp_sentence = SalaryData['exp_sentence'].dropna().unique()\n",
    "for sentence in exp_sentence:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf9a02",
   "metadata": {},
   "source": [
    "### Extract number from year of experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a14144a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, '2', '8', '3', '10', '5', '6', '80', '7', '4', '1', '15'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_experience(description):\n",
    "    # match the number before \"years of experience\", \"year of experience\", \"year's working\" etc.\n",
    "    # excluding \"yearly\" because it would be salary instead of experience\n",
    "    pattern = r'\\b(\\d+)\\b\\s*(?:year\\'s\\s+working|years?\\s+(?:of\\s+)?experience)\\b(?!ly)'\n",
    "    matches = re.findall(pattern, description, flags=re.IGNORECASE)\n",
    "    \n",
    "    sentences = re.split(r'[\\n.]+', description)  # Split description into sentences\n",
    "    matched_sentences = [sentence.strip() for sentence in sentences if re.search(pattern, sentence, flags=re.IGNORECASE)]\n",
    "\n",
    "    return matches, matched_sentences\n",
    "\n",
    "SalaryData['job_description'].apply(lambda x: extract_experience(x)[0]).explode().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32ed98a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_experience(description):\n",
    "    pattern = r'(\\d+)\\+?(?:\\s*year(?:\\'s)?\\s*(?:of)?\\s*(?:working\\s*)?(?:experience)?)'\n",
    "    matches = re.findall(pattern, description, flags=re.IGNORECASE)\n",
    "    \n",
    "    if matches:\n",
    "        experience = matches[0]\n",
    "        if int(experience) < 2:\n",
    "            return \"0-2 years\"\n",
    "        elif int(experience) < 5:\n",
    "            return \"2-5 years\"\n",
    "        elif int(experience) < 10:\n",
    "            return \"5-10 years\"\n",
    "        else:\n",
    "            return \"10+ years\"\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca971bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-10 years    315\n",
       "2-5 years     118\n",
       "10+ years      93\n",
       "0-2 years      85\n",
       "Name: job_experience, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SalaryData['job_experience'] = SalaryData['job_description'].apply(lambda x: extract_experience(x))\n",
    "SalaryData['job_experience'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5e9f5b",
   "metadata": {},
   "source": [
    "# Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2b3c41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>location</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_description</th>\n",
       "      <th>salary_estimate</th>\n",
       "      <th>company_size</th>\n",
       "      <th>company_type</th>\n",
       "      <th>company_sector</th>\n",
       "      <th>company_industry</th>\n",
       "      <th>...</th>\n",
       "      <th>job_languages</th>\n",
       "      <th>job_cloud</th>\n",
       "      <th>job_viz</th>\n",
       "      <th>job_databases</th>\n",
       "      <th>job_bigdata</th>\n",
       "      <th>job_devops</th>\n",
       "      <th>job_education</th>\n",
       "      <th>degree_sentence</th>\n",
       "      <th>exp_sentence</th>\n",
       "      <th>job_experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PCS Global Tech\\n4.7</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Riverside, CA</td>\n",
       "      <td>Data Engineer | PAID BOOTCAMP</td>\n",
       "      <td>Responsibilities\\n· Analyze and organize raw d...</td>\n",
       "      <td>70000.0</td>\n",
       "      <td>501 to 1000 Employees</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Information Technology Support Services</td>\n",
       "      <td>...</td>\n",
       "      <td>[sql, python, java]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>bachelor</td>\n",
       "      <td>· Degree in Computer Science, IT, or similar f...</td>\n",
       "      <td>00 per year</td>\n",
       "      <td>0-2 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Futuretech Consultants LLC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Newton, MS</td>\n",
       "      <td>Snowflake Data Engineer</td>\n",
       "      <td>My name is Dileep and I am a recruiter at Futu...</td>\n",
       "      <td>76500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[sql]</td>\n",
       "      <td>[snowflake]</td>\n",
       "      <td>[ssis]</td>\n",
       "      <td>[snowflake]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>bachelor</td>\n",
       "      <td>Bachelor’s Degree in Computer Science, IT, or ...</td>\n",
       "      <td>None</td>\n",
       "      <td>5-10 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clairvoyant\\n4.4</td>\n",
       "      <td>4.4</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Data Engineer (MDM)</td>\n",
       "      <td>Required Skills:\\nMust have 5-8+ Years of expe...</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>51 to 200 Employees</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Pharmaceutical &amp; Biotechnology</td>\n",
       "      <td>Biotech &amp; Pharmaceuticals</td>\n",
       "      <td>...</td>\n",
       "      <td>[sql, python]</td>\n",
       "      <td>[databricks, aws]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[spark]</td>\n",
       "      <td>[]</td>\n",
       "      <td>master</td>\n",
       "      <td>None</td>\n",
       "      <td>MDM (Master Data Management): 1 year (Required)</td>\n",
       "      <td>5-10 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple\\n4.2</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Cupertino, CA</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Summary\\nPosted: Dec 22, 2021\\nWeekly Hours: 4...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000+ Employees</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Computer Hardware Development</td>\n",
       "      <td>...</td>\n",
       "      <td>[python]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tableau]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skytech Consultancy Services\\n5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Baltimore, MD</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Description of Work:\\nTechnical experience in ...</td>\n",
       "      <td>117000.0</td>\n",
       "      <td>1 to 50 Employees</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[sql]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tableau]</td>\n",
       "      <td>[oracle]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>bachelor</td>\n",
       "      <td>Bachelor's or master's degree in a training re...</td>\n",
       "      <td>None</td>\n",
       "      <td>5-10 years</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             company  company_rating       location  \\\n",
       "0               PCS Global Tech\\n4.7             4.7  Riverside, CA   \n",
       "1         Futuretech Consultants LLC             NaN     Newton, MS   \n",
       "2                   Clairvoyant\\n4.4             4.4         Remote   \n",
       "3                         Apple\\n4.2             4.2  Cupertino, CA   \n",
       "4  Skytech Consultancy Services\\n5.0             5.0  Baltimore, MD   \n",
       "\n",
       "                       job_title  \\\n",
       "0  Data Engineer | PAID BOOTCAMP   \n",
       "1        Snowflake Data Engineer   \n",
       "2            Data Engineer (MDM)   \n",
       "3                  Data Engineer   \n",
       "4                  Data Engineer   \n",
       "\n",
       "                                     job_description  salary_estimate  \\\n",
       "0  Responsibilities\\n· Analyze and organize raw d...          70000.0   \n",
       "1  My name is Dileep and I am a recruiter at Futu...          76500.0   \n",
       "2  Required Skills:\\nMust have 5-8+ Years of expe...         121500.0   \n",
       "3  Summary\\nPosted: Dec 22, 2021\\nWeekly Hours: 4...              NaN   \n",
       "4  Description of Work:\\nTechnical experience in ...         117000.0   \n",
       "\n",
       "            company_size       company_type                  company_sector  \\\n",
       "0  501 to 1000 Employees  Company - Private          Information Technology   \n",
       "1                    NaN                NaN                             NaN   \n",
       "2    51 to 200 Employees  Company - Private  Pharmaceutical & Biotechnology   \n",
       "3       10000+ Employees   Company - Public          Information Technology   \n",
       "4      1 to 50 Employees   Company - Public                             NaN   \n",
       "\n",
       "                          company_industry  ...        job_languages  \\\n",
       "0  Information Technology Support Services  ...  [sql, python, java]   \n",
       "1                                      NaN  ...                [sql]   \n",
       "2                Biotech & Pharmaceuticals  ...        [sql, python]   \n",
       "3            Computer Hardware Development  ...             [python]   \n",
       "4                                      NaN  ...                [sql]   \n",
       "\n",
       "           job_cloud    job_viz job_databases job_bigdata job_devops  \\\n",
       "0                 []         []            []          []         []   \n",
       "1        [snowflake]     [ssis]   [snowflake]          []         []   \n",
       "2  [databricks, aws]         []            []     [spark]         []   \n",
       "3                 []  [tableau]            []          []         []   \n",
       "4                 []  [tableau]      [oracle]          []         []   \n",
       "\n",
       "  job_education                                    degree_sentence  \\\n",
       "0      bachelor  · Degree in Computer Science, IT, or similar f...   \n",
       "1      bachelor  Bachelor’s Degree in Computer Science, IT, or ...   \n",
       "2        master                                               None   \n",
       "3          None                                               None   \n",
       "4      bachelor  Bachelor's or master's degree in a training re...   \n",
       "\n",
       "                                      exp_sentence job_experience  \n",
       "0                                      00 per year      0-2 years  \n",
       "1                                             None     5-10 years  \n",
       "2  MDM (Master Data Management): 1 year (Required)     5-10 years  \n",
       "3                                             None           None  \n",
       "4                                             None     5-10 years  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SalaryData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f9d10b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SalaryData.to_csv(\"glassdoor-data-engineer-kaggle_cleaned.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
